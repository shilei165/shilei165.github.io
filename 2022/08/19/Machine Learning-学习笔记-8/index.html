<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-Shi.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-Shi.png">
  <link rel="mask-icon" href="/images/logo-Shi.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="这篇文章跟大家分享一下Machine Learning的学习笔记: 08-exercise 2 summary。">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning-学习笔记-08-exercise 2 summary">
<meta property="og:url" content="http://example.com/2022/08/19/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8/index.html">
<meta property="og:site_name" content="Dr. Shi&#39;s Blog">
<meta property="og:description" content="这篇文章跟大家分享一下Machine Learning的学习笔记: 08-exercise 2 summary。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-08-20T02:40:23.000Z">
<meta property="article:modified_time" content="2023-07-17T14:50:11.440Z">
<meta property="article:author" content="Dr. Shi">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="计算机">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2022/08/19/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Machine Learning-学习笔记-08-exercise 2 summary | Dr. Shi's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Dr. Shi's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/19/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-1.gif">
      <meta itemprop="name" content="Dr. Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dr. Shi's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning-学习笔记-08-exercise 2 summary
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-19 22:40:23" itemprop="dateCreated datePublished" datetime="2022-08-19T22:40:23-04:00">2022-08-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">技术杂谈</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这篇文章跟大家分享一下Machine Learning的学习笔记: 08-exercise 2 summary。</p>
<span id="more"></span>

<p><strong>Programming Exercise 2: Logistic Regression</strong></p>
<p>In this exercise, you will implement logistic regression and apply it to two different datasets.</p>
<h1 id="ex2-m"><a href="#ex2-m" class="headerlink" title="ex2.m"></a>ex2.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise 2: Logistic Regression</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you get started on the logistic</span><br><span class="line">%  regression exercise. You will need to complete the following functions </span><br><span class="line">%  in this exericse:</span><br><span class="line">%</span><br><span class="line">%     sigmoid.m</span><br><span class="line">%     costFunction.m</span><br><span class="line">%     predict.m</span><br><span class="line">%     costFunctionReg.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Load Data</span><br><span class="line">%  The first two columns contains the exam scores and the third column</span><br><span class="line">%  contains the label.</span><br><span class="line"></span><br><span class="line">data = load(&#x27;ex2data1.txt&#x27;);</span><br><span class="line">X = data(:, [1, 2]); y = data(:, 3);</span><br><span class="line"></span><br><span class="line">%% ==================== Part 1: Plotting ====================</span><br><span class="line">%  We start the exercise by first plotting the data to understand the </span><br><span class="line">%  the problem we are working with.</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;Plotting data with + indicating (y = 1) examples and o &#x27; ...</span><br><span class="line">         &#x27;indicating (y = 0) examples.\n&#x27;]);</span><br><span class="line"></span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels </span><br><span class="line">hold on;</span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel(&#x27;Exam 1 score&#x27;)</span><br><span class="line">ylabel(&#x27;Exam 2 score&#x27;)</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend(&#x27;Admitted&#x27;, &#x27;Not admitted&#x27;)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============ Part 2: Compute Cost and Gradient ============</span><br><span class="line">%  In this part of the exercise, you will implement the cost and gradient</span><br><span class="line">%  for logistic regression. You neeed to complete the code in </span><br><span class="line">%  costFunction.m</span><br><span class="line"></span><br><span class="line">%  Setup the data matrix appropriately, and add ones for the intercept term</span><br><span class="line">[m, n] = size(X);</span><br><span class="line"></span><br><span class="line">% Add intercept term to x and X_test</span><br><span class="line">X = [ones(m, 1) X];</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(n + 1, 1);</span><br><span class="line"></span><br><span class="line">% Compute and display initial cost and gradient</span><br><span class="line">[cost, grad] = costFunction(initial_theta, X, y);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Cost at initial theta (zeros): %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 0.693\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradient at initial theta (zeros): \n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad);</span><br><span class="line">fprintf(&#x27;Expected gradients (approx):\n -0.1000\n -12.0092\n -11.2628\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Compute and display cost and gradient with non-zero theta</span><br><span class="line">test_theta = [-24; 0.2; 0.2];</span><br><span class="line">[cost, grad] = costFunction(test_theta, X, y);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nCost at test theta: %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 0.218\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradient at test theta: \n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad);</span><br><span class="line">fprintf(&#x27;Expected gradients (approx):\n 0.043\n 2.566\n 2.647\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============= Part 3: Optimizing using fminunc  =============</span><br><span class="line">%  In this exercise, you will use a built-in function (fminunc) to find the</span><br><span class="line">%  optimal parameters theta.</span><br><span class="line"></span><br><span class="line">%  Set options for fminunc</span><br><span class="line">options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, 400);</span><br><span class="line"></span><br><span class="line">%  Run fminunc to obtain the optimal theta</span><br><span class="line">%  This function will return theta and the cost </span><br><span class="line">[theta, cost] = ...</span><br><span class="line">  fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);</span><br><span class="line"></span><br><span class="line">% Print theta to screen</span><br><span class="line">fprintf(&#x27;Cost at theta found by fminunc: %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 0.203\n&#x27;);</span><br><span class="line">fprintf(&#x27;theta: \n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, theta);</span><br><span class="line">fprintf(&#x27;Expected theta (approx):\n&#x27;);</span><br><span class="line">fprintf(&#x27; -25.161\n 0.206\n 0.201\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Plot Boundary</span><br><span class="line">plotDecisionBoundary(theta, X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels </span><br><span class="line">hold on;</span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel(&#x27;Exam 1 score&#x27;)</span><br><span class="line">ylabel(&#x27;Exam 2 score&#x27;)</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend(&#x27;Admitted&#x27;, &#x27;Not admitted&#x27;)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============== Part 4: Predict and Accuracies ==============</span><br><span class="line">%  After learning the parameters, you&#x27;ll like to use it to predict the outcomes</span><br><span class="line">%  on unseen data. In this part, you will use the logistic regression model</span><br><span class="line">%  to predict the probability that a student with score 45 on exam 1 and </span><br><span class="line">%  score 85 on exam 2 will be admitted.</span><br><span class="line">%</span><br><span class="line">%  Furthermore, you will compute the training and test set accuracies of </span><br><span class="line">%  our model.</span><br><span class="line">%</span><br><span class="line">%  Your task is to complete the code in predict.m</span><br><span class="line"></span><br><span class="line">%  Predict probability for a student with score 45 on exam 1 </span><br><span class="line">%  and score 85 on exam 2 </span><br><span class="line"></span><br><span class="line">prob = sigmoid([1 45 85] * theta);</span><br><span class="line">fprintf([&#x27;For a student with scores 45 and 85, we predict an admission &#x27; ...</span><br><span class="line">         &#x27;probability of %f\n&#x27;], prob);</span><br><span class="line">fprintf(&#x27;Expected value: 0.775 +/- 0.002\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Compute accuracy on our training set</span><br><span class="line">p = predict(theta, X);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Train Accuracy: %f\n&#x27;, mean(double(p == y)) * 100);</span><br><span class="line">fprintf(&#x27;Expected accuracy (approx): 89.0\n&#x27;);</span><br><span class="line">fprintf(&#x27;\n&#x27;);</span><br></pre></td></tr></table></figure>
<h1 id="plotData-m"><a href="#plotData-m" class="headerlink" title="plotData.m"></a>plotData.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">function plotData(X, y)</span><br><span class="line">%PLOTDATA Plots the data points X and y into a new figure </span><br><span class="line">%   PLOTDATA(x,y) plots the data points with + for the positive examples</span><br><span class="line">%   and o for the negative examples. X is assumed to be a Mx2 matrix.</span><br><span class="line"></span><br><span class="line">% Create New Figure</span><br><span class="line">figure; hold on;</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Plot the positive and negative examples on a</span><br><span class="line">%               2D plot, using the option &#x27;k+&#x27; for the positive</span><br><span class="line">%               examples and &#x27;ko&#x27; for the negative examples.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">pos = find(y==1); neg = find(y==0);</span><br><span class="line">plot(X(pos,1),X(pos,2), &#x27;k+&#x27;,&#x27;LineWidth&#x27;,2,&#x27;MarkerSize&#x27;,5);</span><br><span class="line"></span><br><span class="line">plot(X(neg,1),X(neg,2), &#x27;ko&#x27;,&#x27;LineWidth&#x27;,2,&#x27;MarkerFaceColor&#x27;,&#x27;y&#x27;,&#x27;MarkerSize&#x27;,5);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h1 id="costFunction-m"><a href="#costFunction-m" class="headerlink" title="costFunction.m"></a>costFunction.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = costFunction(theta, X, y)</span><br><span class="line">%COSTFUNCTION Compute cost and gradient for logistic regression</span><br><span class="line">%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the</span><br><span class="line">%   parameter for logistic regression and the gradient of the cost</span><br><span class="line">%   w.r.t. to the parameters.</span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">J = 0;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost of a particular choice of theta.</span><br><span class="line">%               You should set J to the cost.</span><br><span class="line">%               Compute the partial derivatives and set grad to the partial</span><br><span class="line">%               derivatives of the cost w.r.t. each parameter in theta</span><br><span class="line">%</span><br><span class="line">% Note: grad should have the same dimensions as theta</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">h = sigmoid(X*theta);</span><br><span class="line">J = (-y&#x27;*log(h)-(1-y&#x27;)*log(1-h))/m;</span><br><span class="line"></span><br><span class="line">grad = ((h-y)&#x27;*X)&#x27;/m;</span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h1 id="plotDecisionBoundary-m"><a href="#plotDecisionBoundary-m" class="headerlink" title="plotDecisionBoundary.m"></a>plotDecisionBoundary.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">function plotDecisionBoundary(theta, X, y)</span><br><span class="line">%PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with</span><br><span class="line">%the decision boundary defined by theta</span><br><span class="line">%   PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the </span><br><span class="line">%   positive examples and o for the negative examples. X is assumed to be </span><br><span class="line">%   a either </span><br><span class="line">%   1) Mx3 matrix, where the first column is an all-ones column for the </span><br><span class="line">%      intercept.</span><br><span class="line">%   2) MxN, N&gt;3 matrix, where the first column is all-ones</span><br><span class="line"></span><br><span class="line">% Plot Data</span><br><span class="line">plotData(X(:,2:3), y);</span><br><span class="line">hold on</span><br><span class="line"></span><br><span class="line">if size(X, 2) &lt;= 3</span><br><span class="line">    % Only need 2 points to define a line, so choose two endpoints</span><br><span class="line">    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];</span><br><span class="line"></span><br><span class="line">    % Calculate the decision boundary line</span><br><span class="line">    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));</span><br><span class="line"></span><br><span class="line">    % Plot, and adjust axes for better viewing</span><br><span class="line">    plot(plot_x, plot_y)</span><br><span class="line">    </span><br><span class="line">    % Legend, specific for the exercise</span><br><span class="line">    legend(&#x27;Admitted&#x27;, &#x27;Not admitted&#x27;, &#x27;Decision Boundary&#x27;)</span><br><span class="line">    axis([30, 100, 30, 100])</span><br><span class="line">else</span><br><span class="line">    % Here is the grid range</span><br><span class="line">    u = linspace(-1, 1.5, 50);</span><br><span class="line">    v = linspace(-1, 1.5, 50);</span><br><span class="line"></span><br><span class="line">    z = zeros(length(u), length(v));</span><br><span class="line">    % Evaluate z = theta*x over the grid</span><br><span class="line">    for i = 1:length(u)</span><br><span class="line">        for j = 1:length(v)</span><br><span class="line">            z(i,j) = mapFeature(u(i), v(j))*theta;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    z = z&#x27;; % important to transpose z before calling contour</span><br><span class="line"></span><br><span class="line">    % Plot z = 0</span><br><span class="line">    % Notice you need to specify the range [0, 0]</span><br><span class="line">    contour(u, v, z, [0, 0], &#x27;LineWidth&#x27;, 2)</span><br><span class="line">end</span><br><span class="line">hold off</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h1 id="sigmoid-m"><a href="#sigmoid-m" class="headerlink" title="sigmoid.m"></a>sigmoid.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">function g = sigmoid(z)</span><br><span class="line">%SIGMOID Compute sigmoid function</span><br><span class="line">%   g = SIGMOID(z) computes the sigmoid of z.</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">g = zeros(size(z));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the sigmoid of each value of z (z can be a matrix,</span><br><span class="line">%               vector or scalar).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">g = 1./(1+e.^(-z));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h1 id="predict-m"><a href="#predict-m" class="headerlink" title="predict.m"></a>predict.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">function p = predict(theta, X)</span><br><span class="line">%PREDICT Predict whether the label is 0 or 1 using learned logistic </span><br><span class="line">%regression parameters theta</span><br><span class="line">%   p = PREDICT(theta, X) computes the predictions for X using a </span><br><span class="line">%   threshold at 0.5 (i.e., if sigmoid(theta&#x27;*x) &gt;= 0.5, predict 1)</span><br><span class="line"></span><br><span class="line">m = size(X, 1); % Number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly</span><br><span class="line">p = zeros(m, 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Complete the following code to make predictions using</span><br><span class="line">%               your learned logistic regression parameters. </span><br><span class="line">%               You should set p to a vector of 0&#x27;s and 1&#x27;s</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p = round(sigmoid(X*theta));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h1 id="ex2-reg-m"><a href="#ex2-reg-m" class="headerlink" title="ex2_reg.m"></a>ex2_reg.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise 2: Logistic Regression</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you get started on the second part</span><br><span class="line">%  of the exercise which covers regularization with logistic regression.</span><br><span class="line">%</span><br><span class="line">%  You will need to complete the following functions in this exericse:</span><br><span class="line">%</span><br><span class="line">%     sigmoid.m</span><br><span class="line">%     costFunction.m</span><br><span class="line">%     predict.m</span><br><span class="line">%     costFunctionReg.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Load Data</span><br><span class="line">%  The first two columns contains the X values and the third column</span><br><span class="line">%  contains the label (y).</span><br><span class="line"></span><br><span class="line">data = load(&#x27;ex2data2.txt&#x27;);</span><br><span class="line">X = data(:, [1, 2]); y = data(:, 3);</span><br><span class="line"></span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels</span><br><span class="line">hold on;</span><br><span class="line"></span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel(&#x27;Microchip Test 1&#x27;)</span><br><span class="line">ylabel(&#x27;Microchip Test 2&#x27;)</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend(&#x27;y = 1&#x27;, &#x27;y = 0&#x27;)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Regularized Logistic Regression ============</span><br><span class="line">%  In this part, you are given a dataset with data points that are not</span><br><span class="line">%  linearly separable. However, you would still like to use logistic</span><br><span class="line">%  regression to classify the data points.</span><br><span class="line">%</span><br><span class="line">%  To do so, you introduce more features to use -- in particular, you add</span><br><span class="line">%  polynomial features to our data matrix (similar to polynomial</span><br><span class="line">%  regression).</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Add Polynomial Features</span><br><span class="line"></span><br><span class="line">% Note that mapFeature also adds a column of ones for us, so the intercept</span><br><span class="line">% term is handled</span><br><span class="line">X = mapFeature(X(:,1), X(:,2));</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(size(X, 2), 1);</span><br><span class="line"></span><br><span class="line">% Set regularization parameter lambda to 1</span><br><span class="line">lambda = 1;</span><br><span class="line"></span><br><span class="line">% Compute and display initial cost and gradient for regularized logistic</span><br><span class="line">% regression</span><br><span class="line">[cost, grad] = costFunctionReg(initial_theta, X, y, lambda);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Cost at initial theta (zeros): %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 0.693\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradient at initial theta (zeros) - first five values only:\n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad(1:5));</span><br><span class="line">fprintf(&#x27;Expected gradients (approx) - first five values only:\n&#x27;);</span><br><span class="line">fprintf(&#x27; 0.0085\n 0.0188\n 0.0001\n 0.0503\n 0.0115\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">% Compute and display cost and gradient</span><br><span class="line">% with all-ones theta and lambda = 10</span><br><span class="line">test_theta = ones(size(X,2),1);</span><br><span class="line">[cost, grad] = costFunctionReg(test_theta, X, y, 10);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nCost at test theta (with lambda = 10): %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 3.16\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradient at test theta - first five values only:\n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad(1:5));</span><br><span class="line">fprintf(&#x27;Expected gradients (approx) - first five values only:\n&#x27;);</span><br><span class="line">fprintf(&#x27; 0.3460\n 0.1614\n 0.1948\n 0.2269\n 0.0922\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============= Part 2: Regularization and Accuracies =============</span><br><span class="line">%  Optional Exercise:</span><br><span class="line">%  In this part, you will get to try different values of lambda and</span><br><span class="line">%  see how regularization affects the decision coundart</span><br><span class="line">%</span><br><span class="line">%  Try the following values of lambda (0, 1, 10, 100).</span><br><span class="line">%</span><br><span class="line">%  How does the decision boundary change when you vary lambda? How does</span><br><span class="line">%  the training set accuracy vary?</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(size(X, 2), 1);</span><br><span class="line"></span><br><span class="line">% Set regularization parameter lambda to 1 (you should vary this)</span><br><span class="line">lambda = 1;</span><br><span class="line"></span><br><span class="line">% Set Options</span><br><span class="line">options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, 400);</span><br><span class="line"></span><br><span class="line">% Optimize</span><br><span class="line">[theta, J, exit_flag] = ...</span><br><span class="line">  fminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);</span><br><span class="line"></span><br><span class="line">% Plot Boundary</span><br><span class="line">plotDecisionBoundary(theta, X, y);</span><br><span class="line">hold on;</span><br><span class="line">title(sprintf(&#x27;lambda = %g&#x27;, lambda))</span><br><span class="line"></span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel(&#x27;Microchip Test 1&#x27;)</span><br><span class="line">ylabel(&#x27;Microchip Test 2&#x27;)</span><br><span class="line"></span><br><span class="line">legend(&#x27;y = 1&#x27;, &#x27;y = 0&#x27;, &#x27;Decision boundary&#x27;)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">% Compute accuracy on our training set</span><br><span class="line">p = predict(theta, X);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Train Accuracy: %f\n&#x27;, mean(double(p == y)) * 100);</span><br><span class="line">fprintf(&#x27;Expected accuracy (with lambda = 1): 83.1 (approx)\n&#x27;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="mapFeature-m"><a href="#mapFeature-m" class="headerlink" title="mapFeature.m"></a>mapFeature.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">function out = mapFeature(X1, X2)</span><br><span class="line">% MAPFEATURE Feature mapping function to polynomial features</span><br><span class="line">%</span><br><span class="line">%   MAPFEATURE(X1, X2) maps the two input features</span><br><span class="line">%   to quadratic features used in the regularization exercise.</span><br><span class="line">%</span><br><span class="line">%   Returns a new feature array with more features, comprising of </span><br><span class="line">%   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..</span><br><span class="line">%</span><br><span class="line">%   Inputs X1, X2 must be the same size</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">degree = 6;</span><br><span class="line">out = ones(size(X1(:,1)));</span><br><span class="line">for i = 1:degree</span><br><span class="line">    for j = 0:i</span><br><span class="line">        out(:, end+1) = (X1.^(i-j)).*(X2.^j);</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h1 id="costFunctionReg-m"><a href="#costFunctionReg-m" class="headerlink" title="costFunctionReg.m"></a>costFunctionReg.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = costFunctionReg(theta, X, y, lambda)</span><br><span class="line">%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization</span><br><span class="line">%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using</span><br><span class="line">%   theta as the parameter for regularized logistic regression and the</span><br><span class="line">%   gradient of the cost w.r.t. to the parameters. </span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">J = 0;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost of a particular choice of theta.</span><br><span class="line">%               You should set J to the cost.</span><br><span class="line">%               Compute the partial derivatives and set grad to the partial</span><br><span class="line">%               derivatives of the cost w.r.t. each parameter in theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">h = sigmoid(X*theta);</span><br><span class="line">reg = lambda/(2*m)*(theta&#x27;*theta-theta(1)^2);</span><br><span class="line">J = 1/m*(-y&#x27;*log(h)-(1-y&#x27;)*log(1-h))+reg;</span><br><span class="line"></span><br><span class="line">mask = ones(size(theta));</span><br><span class="line">mask(1)=0;</span><br><span class="line"></span><br><span class="line">grad = 1/m*X&#x27;*(h-y) + lambda/m*(theta.*mask);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        <div class="reward-container">
  <div>您的支持将鼓励我继续创作</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Dr. Shi 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Dr. Shi 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/" rel="tag"># 计算机</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/08/18/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-7/" rel="prev" title="Machine Learning-学习笔记-07-Regularization">
      <i class="fa fa-chevron-left"></i> Machine Learning-学习笔记-07-Regularization
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/08/20/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-9/" rel="next" title="Machine Learning-学习笔记-09-Neural Networks:Representation">
      Machine Learning-学习笔记-09-Neural Networks:Representation <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ex2-m"><span class="nav-number">1.</span> <span class="nav-text">ex2.m</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#plotData-m"><span class="nav-number">2.</span> <span class="nav-text">plotData.m</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#costFunction-m"><span class="nav-number">3.</span> <span class="nav-text">costFunction.m</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#plotDecisionBoundary-m"><span class="nav-number">4.</span> <span class="nav-text">plotDecisionBoundary.m</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sigmoid-m"><span class="nav-number">5.</span> <span class="nav-text">sigmoid.m</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#predict-m"><span class="nav-number">6.</span> <span class="nav-text">predict.m</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ex2-reg-m"><span class="nav-number">7.</span> <span class="nav-text">ex2_reg.m</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mapFeature-m"><span class="nav-number">8.</span> <span class="nav-text">mapFeature.m</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#costFunctionReg-m"><span class="nav-number">9.</span> <span class="nav-text">costFunctionReg.m</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Dr. Shi"
      src="/images/avatar-1.gif">
  <p class="site-author-name" itemprop="name">Dr. Shi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">248</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">110</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dr. Shi</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
