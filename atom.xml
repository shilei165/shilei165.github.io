<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>石磊磊的个人站点</title>
  
  
  <link href="https://shilei165.github.io/atom.xml" rel="self"/>
  
  <link href="https://shilei165.github.io/"/>
  <updated>2023-01-07T19:55:15.848Z</updated>
  <id>https://shilei165.github.io/</id>
  
  <author>
    <name>石磊磊</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Information Systems 3- Evaluating Informaiton</title>
    <link href="https://shilei165.github.io/2023/01/07/Information_Systems_Chapter_3/"/>
    <id>https://shilei165.github.io/2023/01/07/Information_Systems_Chapter_3/</id>
    <published>2023-01-07T17:52:00.000Z</published>
    <updated>2023-01-07T19:55:15.848Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a><strong>Learning Objectives</strong></h1><ul><li>Discuss why it is important both personally and professionally to be an informed information consumer</li><li>Describe information overload, its consequences, and approaches for dealing with information overload</li><li>Discuss the relationship between information overload and information evaluation</li><li>List and describe the dimesions of information quality</li><li>List and describe the elements of an information evaluation framework</li><li>Given an information-related task, evaluate information for its usefulness and believability</li></ul><span id="more"></span><hr><h1 id="Being-a-Smart-Information-Consumer"><a href="#Being-a-Smart-Information-Consumer" class="headerlink" title="Being a Smart Information Consumer"></a>Being a Smart Information Consumer</h1><ul><li>Online available information has both advantages and disadvantages<ul><li>Advantages: easy access to information</li><li>Disadvantages: no quality control.</li></ul></li><li><strong>Information evaluation</strong> is the systematic determination of the merit and worth of information. Information evaluation skills will be important to you both personally and in your business career.<ul><li><strong>Personally</strong>: sift through and evaluate many kinds of information.</li><li><strong>Business Career</strong>: your reputation &amp; career success depends on the outcomes of the decisions you make, which based on better information.</li></ul></li></ul><h1 id="Information-Overload-and-the-Need-to-Evaluate-Information"><a href="#Information-Overload-and-the-Need-to-Evaluate-Information" class="headerlink" title="Information Overload and the Need to Evaluate Information"></a>Information Overload and the Need to Evaluate Information</h1><ul><li><strong>Information overload</strong> is being faced with more information than one can effectively process.</li><li>Information overload reduces productivity, increases stress, and can acutally lead to physical health problems.</li><li>Two major strategies for dealing with information overload are <strong>filtering and withdrawal</strong>.<ul><li><strong>Withdrawal</strong> involes disconnecting from sources of information. Example: not checking email, no TV, no Internet, etc.</li><li><strong>Filtering</strong> involves knowing what information we need and what information merits attention and use. Example: choose which message to open and which to delete or ignore.</li></ul></li></ul><h1 id="Information-Quality"><a href="#Information-Quality" class="headerlink" title="Information Quality"></a>Information Quality</h1><p><strong>Information Quality</strong> The degree to which information is suitable for a particular purpose. In other words, the information with high quality is useful toward the achievement of whatever task is at hand.</p><p>Wang and Strong put quality dimensions into four categories:</p><ol><li>Intrinsic quality  –  important regardless of the context or how it is repsrsented.</li><li>Contextual quality  –  viewed differently depending on the task at hand.</li><li>Representational quality  –  concerns how the information is provided to the user.</li><li>Accessibility quality  –  whether authorized users can easily access the information.</li></ol><p>A few points regarding the classification of quality dimension</p><ol><li>There is considerable disagreement regarding the dimensions of information quality.</li><li>It is important to consider the context when thinking about the information quality.</li><li>Information quality has a cost. <strong>Sufficient quality</strong> is usually we want.<ul><li>For more important, higher impact decisions, it is worthwhile to pay much more attention to information quality than for lower consequence decision. Example: buying a house vs. notebook paper.</li></ul></li></ol><h1 id="Evaluating-Information"><a href="#Evaluating-Information" class="headerlink" title="Evaluating Information"></a>Evaluating Information</h1><p>There are two questions you need to answer:</p><ul><li>(1) Is the information useful?</li><li>(2) Is the information believable?</li></ul><h2 id="Evaluating-Usefulness"><a href="#Evaluating-Usefulness" class="headerlink" title="Evaluating Usefulness"></a>Evaluating Usefulness</h2><ul><li><strong>Relevant</strong><ul><li>Will this information help me accomplish my task?</li></ul></li><li><strong>Appropriate</strong><ul><li>Is the information suitable for your purpose?</li></ul></li><li><strong>Current</strong><ul><li>How current you need the information to be?</li></ul></li><li>Consistent</li><li>Understandable</li></ul><h2 id="Evaluating-Believability"><a href="#Evaluating-Believability" class="headerlink" title="Evaluating Believability"></a>Evaluating Believability</h2><ul><li><strong>Credible</strong><ul><li>Author’s or the information source’s reputation, influence, and experience</li></ul></li><li><strong>Objective</strong><ul><li>Whether the source of the information is relevent to or has interest conflict with the reported product or service</li><li>Whether it use the persuasive or emotional lanuage</li></ul></li><li><strong>Supported</strong><ul><li>Claims without support should not be trusted</li><li>When support is offered, you should evaluate the quality of the support</li><li>Consider the reasonableness of the claim</li><li>Think about whether the claim is testable</li></ul></li><li><strong>Comprehensive</strong><ul><li>Assessing comprehensiveness requires assessing the <strong>depth and breadth</strong> of the informaiton</li><li>Breadth concern whether all aspects of a topic are covered</li><li>Depth concerns the level of detail provided</li></ul></li></ul><p><strong>The more information you evaluate, the more some of this will become second nature.</strong></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Learning-Objectives&quot;&gt;&lt;a href=&quot;#Learning-Objectives&quot; class=&quot;headerlink&quot; title=&quot;Learning Objectives&quot;&gt;&lt;/a&gt;&lt;strong&gt;Learning Objectives&lt;/strong&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Discuss why it is important both personally and professionally to be an informed information consumer&lt;/li&gt;
&lt;li&gt;Describe information overload, its consequences, and approaches for dealing with information overload&lt;/li&gt;
&lt;li&gt;Discuss the relationship between information overload and information evaluation&lt;/li&gt;
&lt;li&gt;List and describe the dimesions of information quality&lt;/li&gt;
&lt;li&gt;List and describe the elements of an information evaluation framework&lt;/li&gt;
&lt;li&gt;Given an information-related task, evaluate information for its usefulness and believability&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Information Systems" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Information-Systems/"/>
    
    
    <category term="Information Systems" scheme="https://shilei165.github.io/tags/Information-Systems/"/>
    
  </entry>
  
  <entry>
    <title>Information Systems 2- Introduction to Information Systems</title>
    <link href="https://shilei165.github.io/2023/01/05/Information_Systems_Chapter_2/"/>
    <id>https://shilei165.github.io/2023/01/05/Information_Systems_Chapter_2/</id>
    <published>2023-01-05T18:24:00.000Z</published>
    <updated>2023-01-05T20:49:17.484Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a><strong>Learning Objectives</strong></h1><ul><li>Describe the major functions of an information system</li><li>Explain why it is important for business professionals to understand information systems</li><li>Explain key concepts related to systems</li><li>Describe the information processing cycle</li><li>Describe the critical elements of an information system</li><li>Explain how information systems help managers deal with information</li><li>Give examples of business rules</li><li>Descuss how information systems facilitate organizational change</li><li>Compare and contrast common information systems</li></ul><span id="more"></span><hr><h1 id="Why-All-Business-Professionals-Need-to-Be-Information-Systems-Managers"><a href="#Why-All-Business-Professionals-Need-to-Be-Information-Systems-Managers" class="headerlink" title="Why All Business Professionals Need to Be Information Systems Managers"></a>Why All Business Professionals Need to Be Information Systems Managers</h1><ul><li>IS are increasingly ingrained in our business and professional lives.</li><li>Everyone will use information system regardless of your major or your career.</li><li>Learning how to effectively use IS can help you be more effective and successful in your career.</li></ul><h1 id="Overview-of-Systems"><a href="#Overview-of-Systems" class="headerlink" title="Overview of Systems"></a>Overview of Systems</h1><p>A <strong>System</strong> is a set of interacting components working together to form a complex, integrated whole to achieve some goal by taking inputs and processing them to produce outputs.</p><ul><li>A system is made up of different pieces, called components. It can take many different forms, ranging from human organs to computer softwares.</li><li>These components work together; they are interrelated.</li><li>A system has some purpose or goals.</li><li>The goal is achieved by taking inputs and processing them to produce outputs.</li></ul><p>A few systems-related concepts that are not apparent from the definition.</p><ul><li>A system is separated from its environment by the system’s boundary.</li><li>Most systems are <strong>open system</strong> – interact with their environments.</li><li>Systems are often made up of <strong>subsystems</strong> – a part of larger system.</li><li><strong>Equifinality</strong> – there are many different potential paths to the final outcome.</li><li><strong>Feedback and control</strong> - a set of functions intended to ensure the proper operation of a system.</li></ul><h1 id="Foundations-of-Information-Systems"><a href="#Foundations-of-Information-Systems" class="headerlink" title="Foundations of Information Systems"></a>Foundations of Information Systems</h1><ul><li>An information system does not require a computer. However, in this class, we are primarily concerned with computerized information system.</li><li>Information systems include the following operations (also called information processing cycle)<ol><li><em>Input</em> – Collection of data and their conversion into a form that allows processing.</li><li><em>Processing</em> – Manipulation and transformation of data.</li><li><em>Storage</em> – Holding place for data so that they can be retrieved at a larger time.</li><li><em>Output</em> – Transformation of processed data into a form that can be understood.</li><li><em>Control</em> – Enforcement of correct processing procedures.</li></ol></li><li>Information systems have six critical elements.<ol><li><em>Data</em> – Raw facts, text, numbers, images…</li><li><em>Hardware</em> – Physical devices (computers&#x2F;hard disks, printers keyboards, etc.)</li><li><em>Software</em> – Set of instructions that gorven the operation of IS (system software &amp; application software)</li><li><em>Communication media</em> – Set of devices and protocols (rules) that enable computers to communicate with each other (network cabling, routers, etc.)</li><li><em>People</em> – Individuals who use the IS (most important component – use&#x2F;interpret&#x2F;monitor&#x2F;build&#x2F;maintain)</li></ol></li></ul><h1 id="How-Information-Systems-Help-Us-Deal-with-Information"><a href="#How-Information-Systems-Help-Us-Deal-with-Information" class="headerlink" title="How Information Systems Help Us Deal with Information"></a>How Information Systems Help Us Deal with Information</h1><ul><li>First, information systems let us gather large amounts of data quickly, easily, and reliably.<ul><li>Examples include the check out system in a grocery store.</li></ul></li><li>Second, information systems allow businesses to store and organize very large amounts of data.<ul><li>IS allow businesses to store volumes of data is an organized manner that allows for rapid retrieval.</li></ul></li><li>Third, information systems perform their data manipulations quicky, accurately, and consistently.<ul><li>As long as the hardware is operating correctly and the software is designed and implemented correctly, an information system is very consistent in its manipulations.</li></ul></li><li>Finally, information systems let us retrieve and output information in a variety of forms, depending on what is useful to the user.<ul><li>The same information can be displayed on a screen, printed, or graphed.</li></ul></li></ul><p>One important function of many business information systems is to enforce <strong>business rules</strong>.</p><ul><li>A business rule is a statement that defines or constrains an aspect of a business with the intent of controlling behaviors within the business.</li><li>All businesses have rules that govern their operations. (Hotel reservation&#x2F; course registration)</li><li>Information systems enforce business rules by not allowing violations to occur.</li></ul><h1 id="How-Information-Systems-Facilitate-Organizational-Change"><a href="#How-Information-Systems-Facilitate-Organizational-Change" class="headerlink" title="How Information Systems Facilitate Organizational Change"></a>How Information Systems Facilitate Organizational Change</h1><ul><li><p><strong>Process Improvements</strong></p><ul><li>IS can help organizations improve both the efficiency and effectiveness of processes.</li><li>Example: customer self-service&#x2F; ATM.</li></ul></li><li><p><strong>Automation</strong></p><ul><li>Some processes have been totally automated.</li><li>Example: online ordering.</li></ul></li><li><p><strong>Control</strong></p><ul><li>When properly designed and implemented, an information system can ensure that business rules are followed throughout a process.</li><li>Example: check credit card before accepting payment.</li></ul></li><li><p><strong>Information Flow</strong></p><ul><li>Workflow systems facilitate information flow throughout a work task.</li><li>Example: grade change –&gt; auto email notification.</li></ul></li></ul><h1 id="Common-Information-Systems"><a href="#Common-Information-Systems" class="headerlink" title="Common Information Systems"></a>Common Information Systems</h1><p><strong>Classify information systems according to the impact or “reach” of the system</strong>:</p><ul><li><p><strong>Personal applications</strong></p><ul><li>Help make individuals’ daily work more efficient and effective</li><li>Example: Microsoft Office, Evernote or Onenote</li></ul></li><li><p><strong>Transaction processing systems (TPSs)</strong></p><ul><li>Collect, monitor, process, report, and store large volumes of data that are created by business processes.</li><li>Example: grocery store point-of-sale system</li></ul></li><li><p><strong>Functional and management information systems</strong></p><ul><li>Monitor, control, and analyze the operation of functional areas</li><li>Example: financial management systems, sales force automation systems</li></ul></li><li><p><strong>Integrated enterprise systems</strong></p><ul><li>Multiple applications in cohesive, interrelated system</li><li>Example: enterprise resource planning systems provide an integrated set of modules that carry out the information processing and reporting systems for the entire organization</li></ul></li><li><p><strong>Interorganizational systems</strong></p><ul><li>Span organizational boundaries to connect companies to suppliers and customers.</li><li>Electronic data interchange (EDI) is at the heart of many of these systems. It allows the systems interact with partner organization.</li></ul></li><li><p><strong>Global systems</strong></p><ul><li>Interorganizational systems that cross national boundaries</li><li>More complex than other systems</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Learning-Objectives&quot;&gt;&lt;a href=&quot;#Learning-Objectives&quot; class=&quot;headerlink&quot; title=&quot;Learning Objectives&quot;&gt;&lt;/a&gt;&lt;strong&gt;Learning Objectives&lt;/strong&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Describe the major functions of an information system&lt;/li&gt;
&lt;li&gt;Explain why it is important for business professionals to understand information systems&lt;/li&gt;
&lt;li&gt;Explain key concepts related to systems&lt;/li&gt;
&lt;li&gt;Describe the information processing cycle&lt;/li&gt;
&lt;li&gt;Describe the critical elements of an information system&lt;/li&gt;
&lt;li&gt;Explain how information systems help managers deal with information&lt;/li&gt;
&lt;li&gt;Give examples of business rules&lt;/li&gt;
&lt;li&gt;Descuss how information systems facilitate organizational change&lt;/li&gt;
&lt;li&gt;Compare and contrast common information systems&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Information Systems" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Information-Systems/"/>
    
    
    <category term="Information Systems" scheme="https://shilei165.github.io/tags/Information-Systems/"/>
    
  </entry>
  
  <entry>
    <title>Information Systems 1- The Value of Information</title>
    <link href="https://shilei165.github.io/2023/01/04/Information_Systems_Chapter_1/"/>
    <id>https://shilei165.github.io/2023/01/04/Information_Systems_Chapter_1/</id>
    <published>2023-01-05T00:24:14.000Z</published>
    <updated>2023-01-05T17:54:50.746Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a><strong>Learning Objectives</strong></h1><ul><li>Describe several important emerging technologies, including 5G, IoT, and AI</li><li>Compare and contrast data, information, and knowledge</li><li>Explain the concepts of connectedness and usefulness as they relate to information</li><li>Name the skills requird for information literacy</li><li>Discuss the importance of information literacy in a businesses</li><li>Compare and contrast information systems careers and information analysis careers</li></ul><span id="more"></span><hr><h1 id="What-is-information-and-why-is-important"><a href="#What-is-information-and-why-is-important" class="headerlink" title="What is information and why is important?"></a>What is information and why is important?</h1><p>Information is important, so knowing about the <strong>technologies and systems</strong> that help one effectively deal with informaiton are also important.</p><h2 id="Information-technology-vs-Information-Systems"><a href="#Information-technology-vs-Information-Systems" class="headerlink" title="Information technology vs. Information Systems"></a>Information technology vs. Information Systems</h2><ul><li><strong>Information technology</strong>: hardware, software and media used to store, organize, retrieve and communicate information.</li><li><strong>Information system</strong>: organized combination of hardware, software, infrastructure, data and people used to accomplish a specified organizational or personal bojective.</li></ul><h1 id="Information-and-Emerging-Technologies"><a href="#Information-and-Emerging-Technologies" class="headerlink" title="Information and Emerging Technologies"></a>Information and Emerging Technologies</h1><p>The world of technology changes rapidly, bring equally rapid changes in how we live and work. Examples include email, social media, mobile apps for ordering food, GPS, music etc.</p><h2 id="Emering-technologies-that-are-likely-to-change-our-lives-even-more"><a href="#Emering-technologies-that-are-likely-to-change-our-lives-even-more" class="headerlink" title="Emering technologies that are likely to change our lives even more."></a>Emering technologies that are likely to change our lives even more.</h2><ul><li><p><strong>Fifth-generation cellular networking (5G):</strong></p><ol><li>It offers much faster data transmission speed, wider coverage areas, and better response times than 4G. </li><li>5G’s enhanced data-handing capabilities offer many possibilities for new technology applications that depend on the ability to send and receive information rapidly. </li><li>These capabilities are especially useful when combined with IoT</li></ol></li><li><p><strong>Internet of Things (IoT):</strong></p><ol><li>IoT refers to an almost limitless network of connected sensors that are embedded in physical objects ranging from refrigerators to airplanes.</li><li>Sensors collect data from physical objects and environment, process the data in limited ways, and transmit the data to computer through the Internet.</li><li>Computers process data further and send data back to sensors, which then can send commands to other processors embeded in the physical object.</li><li>Examples include: traffic sensors; smart homes (smart climate control and lighting systems, surveilance systems, video doorbells, smart refrigerators, and smart locks, among many others)</li></ol></li><li><p><strong>Artificial intelligence (AI):</strong></p><ol><li>AI is used to describe a family of technologies that approximate human cognitive abilities, such as resoning, planning, learning, problem-solving, perception, and language processing.</li><li>Broad applications: expert systems, machining learning, natural language processing, and intelligent agents, among others.</li></ol></li></ul><h2 id="Combine-thses-Technologies-Together"><a href="#Combine-thses-Technologies-Together" class="headerlink" title="Combine thses Technologies Together"></a>Combine thses Technologies Together</h2><ul><li><p>Information technology applications are built around the ability to <strong>gather, communicate, and process</strong> information.</p></li><li><p>These three capabilities align nicely with the main functions of <strong>IoT (gather), 5G (communicate), and AI (process)</strong>.</p></li><li><p><strong>Information</strong> has been, and continues to be, the driving element behind advances in how we apply information technologies.</p></li><li><p>The purpose of IoT is to gather information, the point of 5G is to communicate information faster and more effectively, and the point of AI is to use information more effectively.</p></li></ul><h2 id="Interesting-Applications-of-the-Fusion"><a href="#Interesting-Applications-of-the-Fusion" class="headerlink" title="Interesting Applications of the Fusion"></a>Interesting Applications of the Fusion</h2><ul><li><p><strong>Smart cities</strong></p><ol><li>The idea behind smart cities is to use IoT to collect data, which is then used to better manage resources and to more effectively provide services.</li><li>Smart traffic control system: <strong>IoT-enabled sensors</strong> to conrol traffic by sensing when areas become congested and adjusting traffic lights to ease the traffic jams androute traffic to less congested routes. <strong>AI</strong> can be used to determine the optimal timing of traffic signals. <strong>5G</strong> and <strong>autonomous vehicles</strong> can further improve the efficiency and safety.</li><li>Other smart city applications: environmental monitoring, energy management, intelligent transportation systems, management of public spaces, and refuse collection.</li></ol></li><li><p><strong>Autonomous Vehicles</strong></p><ol><li>Automated systems alter the driver to potential hazards, such as stopped vehicle ahead. In some cases, the automated system will take over certain systems (e.g. braking system).</li><li>There are 6 levels of automated driving systems. Level 0 - no automation, level 5 - full automation. Level 3 are becoming more common.</li><li>Sensors – cameras, radar system, &amp; position and proximity sensors. 5G&#x2F;other network – send and receive data, communicate with traffic lights system or other vehicles. AI – use the information from IoT sensors and other information sources to take action to avoid potential collisions.</li></ol></li><li><p><strong>VR, AR and Mixted Reality</strong></p><ol><li><strong>VR</strong>: Virtual reality. <strong>AR</strong>: Augmented reality. </li><li>They are all similar in that they involve using digital technologies to reprsent or add to physical reality.</li><li>VR is a fully immersive experience in which the technology makes you sense that you are in a different environment.</li><li>AR systems overlay information on top of elements of the real world. Pokemon Go and Real Strike are examples.</li><li>Mixed reality lies somewhere between VR and AR. It allows users to interact with both physical and virtual objects, which opens the possibilities for applications in many areas, such as surgery, training, retailing, real estate, and entertainment, etc.</li></ol></li></ul><h1 id="Data-Information-Knowledge-and-Wisdom"><a href="#Data-Information-Knowledge-and-Wisdom" class="headerlink" title="Data, Information, Knowledge, and Wisdom"></a>Data, Information, Knowledge, and Wisdom</h1><ul><li><strong>Data</strong> are raw symbols. Data itself is meaningless.</li><li><strong>Information</strong> is data that have been processed so that they are useful. In other words, <strong>Information</strong> is the original data combined with other related data and it helps us interpret the data.</li><li><strong>Knowledge</strong> is when information is applied to some decision or action.</li><li><strong>Wisdon</strong> involves using knowledge for the greater good. Wisdom is deeper and more uniquely human.</li></ul><h2 id="Two-reasons-bring-up-this-hierarchy"><a href="#Two-reasons-bring-up-this-hierarchy" class="headerlink" title="Two reasons bring up this hierarchy"></a>Two reasons bring up this hierarchy</h2><ol><li><p>It gives us a way to think about different kinds of systems and how they have evolved. The hierarchy parallels how businesses have applied information technology.</p><pre><code> Manage data --&gt;mamage information --&gt; manage knowledge --&gt; mamage wisdom (debatable).</code></pre></li><li><p>It helps frame two important concepts: <strong>connectedness and usefulness</strong>.</p><pre><code> Data to information requires connecting data elements, and which gives the data meaning (may not be useful).  To be useful, information is interpreted and applied leading to knowledge.</code></pre></li></ol><h1 id="Information-Literacy"><a href="#Information-Literacy" class="headerlink" title="Information Literacy"></a>Information Literacy</h1><p><strong>Information Literacy</strong> is the ability to efficiently and effectively determine what information is needed and then access, evaluate, use, and manage that information in an ethical manner.</p><p>Understanding how to deal with information may well have serious benefits as you move through your business career.</p><h1 id="Use-Information"><a href="#Use-Information" class="headerlink" title="Use Information"></a>Use Information</h1><p>All businesses use information for three main purposes:</p><ol><li><p><strong>communication</strong> – Organizations much exchange information for many reasons, including sharing ideas, corrdinating actions, and transmitting information.</p></li><li><p><strong>process support</strong> – Many business processes are quite comples and involve various parts of business.</p></li><li><p><strong>decision-making</strong> – Making decision requires information about the alternatives.</p></li><li><p>Some businesses also use information for <strong>product</strong>. (like Google)</p></li></ol><h1 id="Information-and-Your-Career"><a href="#Information-and-Your-Career" class="headerlink" title="Information and Your Career"></a>Information and Your Career</h1><ul><li><p><strong>Information system careers</strong> – focus on the design, building, support or management of information systems.</p><ul><li>Examples include computer systems analyst, information systems manager, computer support specialists, etc.</li></ul></li><li><p><strong>Information analysis careers</strong> – use systems to retrieve, report on, and analyze the information contained in the systems. </p><ul><li>Examples include data analyst, data scientist, business intelligence professional, etc.</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Learning-Objectives&quot;&gt;&lt;a href=&quot;#Learning-Objectives&quot; class=&quot;headerlink&quot; title=&quot;Learning Objectives&quot;&gt;&lt;/a&gt;&lt;strong&gt;Learning Objectives&lt;/strong&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Describe several important emerging technologies, including 5G, IoT, and AI&lt;/li&gt;
&lt;li&gt;Compare and contrast data, information, and knowledge&lt;/li&gt;
&lt;li&gt;Explain the concepts of connectedness and usefulness as they relate to information&lt;/li&gt;
&lt;li&gt;Name the skills requird for information literacy&lt;/li&gt;
&lt;li&gt;Discuss the importance of information literacy in a businesses&lt;/li&gt;
&lt;li&gt;Compare and contrast information systems careers and information analysis careers&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Information Systems" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Information-Systems/"/>
    
    
    <category term="Information Systems" scheme="https://shilei165.github.io/tags/Information-Systems/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-23-Application example:Photo OCR</title>
    <link href="https://shilei165.github.io/2022/09/13/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-23/"/>
    <id>https://shilei165.github.io/2022/09/13/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-23/</id>
    <published>2022-09-13T19:32:23.000Z</published>
    <updated>2022-09-14T15:31:55.299Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 22-应用实例：图片文字识别(Application example:Photo OCR)。</p><span id="more"></span><h1 id="问题描述和流程图-Problem-description-and-pipeline"><a href="#问题描述和流程图-Problem-description-and-pipeline" class="headerlink" title="问题描述和流程图(Problem description and pipeline)"></a>问题描述和流程图(Problem description and pipeline)</h1><p>图像文字识别应用所作的事是，从一张给定的图片中识别文字。这比从一份扫描文档中识别文字要复杂的多。</p><p>为了完成这样的工作，需要采取如下步骤：</p><ul><li>文字侦测（Text detection）——将图片上的文字与其他环境对象分离开来</li><li>字符切分（Character segmentation）——将文字分割成一个个单一的字符</li><li>字符分类（Character classification）——确定每一个字符是什么</li></ul><p>我们可以用任务流程图来表达这个问题，每一项任务可以由一个单独的小队来负责解决。</p><p><img src="/../images/OCR_1.png" alt="OCR_1"></p><h1 id="滑动窗口-Sliding-windows"><a href="#滑动窗口-Sliding-windows" class="headerlink" title="滑动窗口(Sliding windows)"></a>滑动窗口(Sliding windows)</h1><p>滑动窗口是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。</p><p>一旦完成后，我们按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。</p><p><img src="/../images/OCR_2.png" alt="OCR_2"></p><p>滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。下图中绿色的区域是经过这些步骤后被认为是文字的区域，而红色的区域是被忽略的。</p><p><img src="/../images/OCR_3.png" alt="OCR_3"></p><p>以上便是文字侦测阶段。下一步是训练一个模型来完成将文字分割成一个个字符的任务，需要的训练集由单个字符的图片和两个相连字符之间的图片来训练模型。</p><p><img src="/../images/OCR_4.png" alt="OCR_4"></p><p>模型训练完后，我们仍然是使用滑动窗口技术来进行字符识别。</p><p>以上便是字符切分阶段。 最后一个阶段是字符分类阶段，利用神经网络、支持向量机或者逻辑回归算法训练一个分类器即可。</p><h1 id="获取大量数据和人工数据-Getting-lots-of-data-Artificial-data-synthesis"><a href="#获取大量数据和人工数据-Getting-lots-of-data-Artificial-data-synthesis" class="headerlink" title="获取大量数据和人工数据(Getting lots of data: Artificial data synthesis)"></a>获取大量数据和人工数据(Getting lots of data: Artificial data synthesis)</h1><p>如果我们的模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的。问题在于，我们怎样获得数据，数据不总是可以直接获得的，我们有可能需要人工地创造一些数据。</p><p>以我们的文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。</p><p>另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。</p><p>有关获得更多数据的几种方法：</p><ul><li>人工数据合成</li><li>手动收集、标记数据</li><li>众包 (E.g. Amazon Mechanical Turk)</li></ul><h1 id="上限分析：哪部分管道值得去进一步提升-Ceiling-analysis-What-part-of-the-pipeline-to-work-on-next"><a href="#上限分析：哪部分管道值得去进一步提升-Ceiling-analysis-What-part-of-the-pipeline-to-work-on-next" class="headerlink" title="上限分析：哪部分管道值得去进一步提升(Ceiling analysis: What part of the pipeline to work on next)"></a>上限分析：哪部分管道值得去进一步提升(Ceiling analysis: What part of the pipeline to work on next)</h1><p>在机器学习的应用中，我们通常需要通过几个步骤才能进行最终的预测，我们如何能够知道哪一部分最值得我们花时间和精力去改善呢？这个问题可以通过上限分析来回答。</p><p>回到我们的文字识别应用中，我们的流程图如下：</p><p><img src="/../images/OCR_1.png" alt="OCR_1"></p><p>流程图中每一部分的输出都是下一部分的输入，上限分析中，我们选取一部分，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。假使我们的例子中总体效果为72%的正确率。</p><p>如果我们令文字侦测部分输出的结果100%正确，发现系统的总体效果从72%提高到了89%。这意味着我们很可能会希望投入时间精力来提高我们的文字侦测部分。</p><p>接着我们手动选择数据，让字符切分输出的结果100%正确，发现系统的总体效果只提升了1%，这意味着，我们的字符切分部分可能已经足够好了。</p><p>最后我们手工选择数据，让字符分类输出的结果100%正确，系统的总体效果又提升了10%，这意味着我们可能也会应该投入更多的时间和精力来提高应用的总体表现。</p><p><img src="/../images/OCR_5.png" alt="OCR_5"></p><h1 id="个人课程总结"><a href="#个人课程总结" class="headerlink" title="个人课程总结"></a>个人课程总结</h1><p>终于学习完了这门课程的所有内容，最后简单总结一下。总体来说，这门课质量真的不错，无论是课程安排还是编程作业，都值得我花更多的时间来继续挖掘和研究。上完这门课之后，个人感觉收获也非常大。</p><p>其实，在好几年前就对Machine Learning比较感兴趣，也听朋友说起它在我们生活中的种种应用。但由于读博期间科研工作比较繁忙，一直没有下定决心要好好研究一下。</p><p>直到2019年，终于决定要挤出时间系统的学习下Machine Learning。刚好听说了Andrew Ng在Course上面开设的这门课，而且评价非常高。就花钱在Course上面报了名，但只坚持了大概五个星期，就无法再继续学下去了。首先，是当时计算机基础太薄弱，对于里面的编程部分非常吃力。每次都要花上非常久的时间才能写出个大概。其次，还是上面提到的科研工作比较繁忙，压力也比较大，很难抽出大块时间花在这门课程上。还有就是，当时是断断续续的学习，把战线拉得比较长。往往在还没有完全吃透前面章节的前提下，就开始了后面的课程，导致后面课程理解难度越来越大。就这样，最终不得不把学习计划暂时搁浅。</p><p>这一放就是三年，三年期间忙着结婚、生子、博士毕业、找工作等等这些事情。虽然没有继续学习Machine Learning，但这期间还是学习了很多计算机相关知识，提升了编程能力。工作之后，由于是大学老师的工作，时间比较自由，刚好这学期课业压力也不算特别大。有了一些时间之后，就想着继续把之前放弃的课程学完，所以从八月份开始就重新把这门课从头学起。在学习的同时，每一个章节都写了博客，来帮助梳理和记忆。发现这种方法非常好，极大的提升了学习效率。大概花了一个月左右时间，终于学完了所有的课程。</p><p>个人感觉，学完课程只算是基本入门，如果想要应用在生活和工作中，还是有很多地方需要学习和思考的。希望与各位共勉，活到老学到老。</p><p>最后，分享一下我的结业证书：）</p><p><img src="/../images/Certificate.PNG" alt="Certificate"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 22-应用实例：图片文字识别(Application example:Photo OCR)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-22-Large Scale Machine Learning</title>
    <link href="https://shilei165.github.io/2022/09/13/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-22/"/>
    <id>https://shilei165.github.io/2022/09/13/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-22/</id>
    <published>2022-09-13T16:08:23.000Z</published>
    <updated>2022-09-13T17:26:17.777Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 22-大规模机器学习(Large Scale Machine Learning)。</p><span id="more"></span><h1 id="大型数据集的学习"><a href="#大型数据集的学习" class="headerlink" title="大型数据集的学习"></a>大型数据集的学习</h1><p>如果我们有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。我们应该怎样应对一个有100万条记录的训练集？</p><p>以线性回归模型为例，每一次梯度下降迭代，我们都需要计算这100万条记录的误差的平方和，如果我们的学习算法需要有20次迭代，这便已经是非常大的计算代价。</p><p>所以，我们首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。</p><p><img src="/../images/largeScaleML_1.png" alt="largeScaleML_1"></p><h1 id="随机梯度下降法-Stochastic-gradient-descent"><a href="#随机梯度下降法-Stochastic-gradient-descent" class="headerlink" title="随机梯度下降法(Stochastic gradient descent)"></a>随机梯度下降法(Stochastic gradient descent)</h1><p>如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法(Stochastic gradient descent)来代替批量梯度下降法。</p><p>在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：</p><p>$$<br>cost(\theta,(x^{(i)},y^{(i)})) &#x3D; \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2<br>$$<br>随机梯度下降算法为：</p><ul><li>对训练集随机“洗牌”</li><li>重复以下循环1到10次：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Repeat&#123;</span><br><span class="line">  for i=1:m&#123; </span><br><span class="line">    theta := theta_j - alpha(h_theta(x_i)-y_i)</span><br><span class="line">    (for j = 0:n)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>随机梯度下降算法在每一次计算之后便更新参数\(\theta\)，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。</li></ul><p><img src="/../images/largeScaleML_2.png" alt="largeScaleML_2"></p><h1 id="小批量梯度下降-Mini-batch-gradient-descent"><a href="#小批量梯度下降-Mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降(Mini-batch gradient descent)"></a>小批量梯度下降(Mini-batch gradient descent)</h1><p>小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数b次训练实例，便更新一次参数\(\theta\)。</p><p>Repeat{</p><p><img src="/../images/largeScaleML_3.png" alt="largeScaleML_3"></p><p>通常我们会令b在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环b个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。</p><h1 id="随机梯度下降收敛-Stochastic-gradient-descent-convergence"><a href="#随机梯度下降收敛-Stochastic-gradient-descent-convergence" class="headerlink" title="随机梯度下降收敛(Stochastic gradient descent convergence)"></a>随机梯度下降收敛(Stochastic gradient descent convergence)</h1><p>在批量梯度下降中，我们可以令代价函数J为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。</p><p>在随机梯度下降中，我们在每一次更新\(\theta\)之前都计算一次代价，然后每x次迭代后，求出这x次对训练实例计算代价的平均值，然后绘制这些平均值与x次迭代的次数之间的函数图表。</p><p><img src="/../images/largeScaleML_4.png" alt="largeScaleML_4"></p><p>当我们绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加\(\alpha\)来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。</p><p>如果我们得到的曲线如上面右下方所示，不断地上升，那么说明我们选择的\(\alpha\)太大，我们可能会需要选择一个较小的学习率\(\alpha\)。</p><p>我们也可以令学习率随着迭代次数的增加而减小，例如令：</p><p>$$<br>\alpha&#x3D;\frac{const1}{iterationNumber + const2}<br>$$</p><p>随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。 但是通常我们不需要这样做便能有非常好的效果了，对\(alpha\)进行调整所耗费的计算通常不值得。</p><p><img src="/../images/largeScaleML_5.png" alt="largeScaleML_5"></p><h1 id="在线学习-Online-learning"><a href="#在线学习-Online-learning" class="headerlink" title="在线学习(Online learning)"></a>在线学习(Online learning)</h1><p>如果你有一个由连续的用户流引发的连续的数据流，进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。</p><p>许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。</p><p>假使我们正在经营一家物流公司，每当一个用户询问从地点A至地点B的快递费用时，我们给用户一个报价，该用户可能选择接受（y&#x3D;1）或不接受（y&#x3D;0）。</p><p>现在，我们希望构建一个模型，来预测用户接受报价使用我们的物流服务的可能性。因此报价 是我们的一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是:\(p(y&#x3D;1)\)。</p><p>在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Repeat forever (as long as the website is running) &#123;</span><br><span class="line"></span><br><span class="line">  Get (x,y) corresponding to the current user</span><br><span class="line"></span><br><span class="line">  theta := theta_j- alpha(h_theta(x)-y)</span><br><span class="line">  (for j=0:n)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。</p><p>每次交互事件并不只产生一个数据集，例如，我们一次给用户提供3个物流选项，用户选择2项，我们实际上可以获得3个新的训练实例，因而我们的算法可以一次从3个实例中学习并更新模型。</p><p>我们所使用的这个算法与随机梯度下降算法非常类似，唯一的区别的是，我们不会使用一个固定的数据集，我们会做的是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去。尤其是，如果你对某一种应用有一个连续的数据流，这样的算法会非常值得认真考虑。在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化 （就像你的用户的品味在缓慢变化），在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。</p><h1 id="映射化简和数据并行-Map-reduce-and-data-parallelism"><a href="#映射化简和数据并行-Map-reduce-and-data-parallelism" class="headerlink" title="映射化简和数据并行(Map-reduce and data parallelism)"></a>映射化简和数据并行(Map-reduce and data parallelism)</h1><p>映射化简和数据并行对于大规模机器学习问题而言是非常重要的概念。之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。</p><p>具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同CPU 核心），以达到加速处理的目的。</p><p>例如，我们有400个训练实例，我们可以将批量梯度下降的求和任务分配给4台计算机进行处理：</p><p><img src="/../images/largeScaleML_6.png" alt="largeScaleML_6"></p><p>很多高级的线性代数函数库已经能够利用多核CPU的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 22-大规模机器学习(Large Scale Machine Learning)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-21-execrise 8 summary</title>
    <link href="https://shilei165.github.io/2022/09/12/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-21/"/>
    <id>https://shilei165.github.io/2022/09/12/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-21/</id>
    <published>2022-09-12T20:35:23.000Z</published>
    <updated>2022-09-14T15:31:47.574Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 21-execrise 8 summary。</p><span id="more"></span><p><strong>Programming Exercise 8: Anomaly Detection and Recommender Systems</strong></p><p>In this exercise, you will implement the anomaly detection algorithm and<br>apply it to detect failing servers on a network. In the second part, you will<br>use collaborative filtering to build a recommender system for movies.</p><h1 id="exe8-m"><a href="#exe8-m" class="headerlink" title="exe8.m"></a>exe8.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class</span><br><span class="line">%  Exercise 8 | Anomaly Detection and Collaborative Filtering</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  exercise. You will need to complete the following functions:</span><br><span class="line">%</span><br><span class="line">%     estimateGaussian.m</span><br><span class="line">%     selectThreshold.m</span><br><span class="line">%     cofiCostFunc.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% ================== Part 1: Load Example Dataset  ===================</span><br><span class="line">%  We start this exercise by using a small dataset that is easy to</span><br><span class="line">%  visualize.</span><br><span class="line">%</span><br><span class="line">%  Our example case consists of 2 network server statistics across</span><br><span class="line">%  several machines: the latency and throughput of each machine.</span><br><span class="line">%  This exercise will help us find possibly faulty (or very fast) machines.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Visualizing example dataset for outlier detection.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  The following command loads the dataset. You should now have the</span><br><span class="line">%  variables X, Xval, yval in your environment</span><br><span class="line">load(&#x27;ex8data1.mat&#x27;);</span><br><span class="line"></span><br><span class="line">%  Visualize the example dataset</span><br><span class="line">plot(X(:, 1), X(:, 2), &#x27;bx&#x27;);</span><br><span class="line">axis([0 30 0 30]);</span><br><span class="line">xlabel(&#x27;Latency (ms)&#x27;);</span><br><span class="line">ylabel(&#x27;Throughput (mb/s)&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================== Part 2: Estimate the dataset statistics ===================</span><br><span class="line">%  For this exercise, we assume a Gaussian distribution for the dataset.</span><br><span class="line">%</span><br><span class="line">%  We first estimate the parameters of our assumed Gaussian distribution, </span><br><span class="line">%  then compute the probabilities for each of the points and then visualize </span><br><span class="line">%  both the overall distribution and where each of the points falls in </span><br><span class="line">%  terms of that distribution.</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;Visualizing Gaussian fit.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Estimate my and sigma2</span><br><span class="line">[mu sigma2] = estimateGaussian(X);</span><br><span class="line"></span><br><span class="line">%  Returns the density of the multivariate normal at each data point (row) </span><br><span class="line">%  of X</span><br><span class="line">p = multivariateGaussian(X, mu, sigma2);</span><br><span class="line"></span><br><span class="line">%  Visualize the fit</span><br><span class="line">visualizeFit(X,  mu, sigma2);</span><br><span class="line">xlabel(&#x27;Latency (ms)&#x27;);</span><br><span class="line">ylabel(&#x27;Throughput (mb/s)&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ================== Part 3: Find Outliers ===================</span><br><span class="line">%  Now you will find a good epsilon threshold using a cross-validation set</span><br><span class="line">%  probabilities given the estimated Gaussian distribution</span><br><span class="line">% </span><br><span class="line"></span><br><span class="line">pval = multivariateGaussian(Xval, mu, sigma2);</span><br><span class="line"></span><br><span class="line">[epsilon F1] = selectThreshold(yval, pval);</span><br><span class="line">fprintf(&#x27;Best epsilon found using cross-validation: %e\n&#x27;, epsilon);</span><br><span class="line">fprintf(&#x27;Best F1 on Cross Validation Set:  %f\n&#x27;, F1);</span><br><span class="line">fprintf(&#x27;   (you should see a value epsilon of about 8.99e-05)\n&#x27;);</span><br><span class="line">fprintf(&#x27;   (you should see a Best F1 value of  0.875000)\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Find the outliers in the training set and plot the</span><br><span class="line">outliers = find(p &lt; epsilon);</span><br><span class="line"></span><br><span class="line">%  Draw a red circle around those outliers</span><br><span class="line">hold on</span><br><span class="line">plot(X(outliers, 1), X(outliers, 2), &#x27;ro&#x27;, &#x27;LineWidth&#x27;, 2, &#x27;MarkerSize&#x27;, 10);</span><br><span class="line">hold off</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ================== Part 4: Multidimensional Outliers ===================</span><br><span class="line">%  We will now use the code from the previous part and apply it to a </span><br><span class="line">%  harder problem in which more features describe each datapoint and only </span><br><span class="line">%  some features indicate whether a point is an outlier.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%  Loads the second dataset. You should now have the</span><br><span class="line">%  variables X, Xval, yval in your environment</span><br><span class="line">load(&#x27;ex8data2.mat&#x27;);</span><br><span class="line"></span><br><span class="line">%  Apply the same steps to the larger dataset</span><br><span class="line">[mu sigma2] = estimateGaussian(X);</span><br><span class="line"></span><br><span class="line">%  Training set </span><br><span class="line">p = multivariateGaussian(X, mu, sigma2);</span><br><span class="line"></span><br><span class="line">%  Cross-validation set</span><br><span class="line">pval = multivariateGaussian(Xval, mu, sigma2);</span><br><span class="line"></span><br><span class="line">%  Find the best threshold</span><br><span class="line">[epsilon F1] = selectThreshold(yval, pval);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Best epsilon found using cross-validation: %e\n&#x27;, epsilon);</span><br><span class="line">fprintf(&#x27;Best F1 on Cross Validation Set:  %f\n&#x27;, F1);</span><br><span class="line">fprintf(&#x27;   (you should see a value epsilon of about 1.38e-18)\n&#x27;);</span><br><span class="line">fprintf(&#x27;   (you should see a Best F1 value of 0.615385)\n&#x27;);</span><br><span class="line">fprintf(&#x27;# Outliers found: %d\n\n&#x27;, sum(p &lt; epsilon));</span><br></pre></td></tr></table></figure><h1 id="estimateGaussian-m"><a href="#estimateGaussian-m" class="headerlink" title="estimateGaussian.m"></a>estimateGaussian.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">function [mu sigma2] = estimateGaussian(X)</span><br><span class="line">%ESTIMATEGAUSSIAN This function estimates the parameters of a </span><br><span class="line">%Gaussian distribution using the data in X</span><br><span class="line">%   [mu sigma2] = estimateGaussian(X), </span><br><span class="line">%   The input X is the dataset with each n-dimensional data point in one row</span><br><span class="line">%   The output is an n-dimensional vector mu, the mean of the data set</span><br><span class="line">%   and the variances sigma^2, an n x 1 vector</span><br><span class="line">% </span><br><span class="line"></span><br><span class="line">% Useful variables</span><br><span class="line">[m, n] = size(X);</span><br><span class="line"></span><br><span class="line">% You should return these values correctly</span><br><span class="line">mu = zeros(n, 1);</span><br><span class="line">sigma2 = zeros(n, 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the mean of the data and the variances</span><br><span class="line">%               In particular, mu(i) should contain the mean of</span><br><span class="line">%               the data for the i-th feature and sigma2(i)</span><br><span class="line">%               should contain variance of the i-th feature.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">mu = sum(X,1)/m;</span><br><span class="line"></span><br><span class="line">sigma2 = sum((X-mu).^2,1)/m;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="multivariateGaussian-m"><a href="#multivariateGaussian-m" class="headerlink" title="multivariateGaussian.m"></a>multivariateGaussian.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">function p = multivariateGaussian(X, mu, Sigma2)</span><br><span class="line">%MULTIVARIATEGAUSSIAN Computes the probability density function of the</span><br><span class="line">%multivariate gaussian distribution.</span><br><span class="line">%    p = MULTIVARIATEGAUSSIAN(X, mu, Sigma2) Computes the probability </span><br><span class="line">%    density function of the examples X under the multivariate gaussian </span><br><span class="line">%    distribution with parameters mu and Sigma2. If Sigma2 is a matrix, it is</span><br><span class="line">%    treated as the covariance matrix. If Sigma2 is a vector, it is treated</span><br><span class="line">%    as the \sigma^2 values of the variances in each dimension (a diagonal</span><br><span class="line">%    covariance matrix)</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">k = length(mu);</span><br><span class="line"></span><br><span class="line">if (size(Sigma2, 2) == 1) || (size(Sigma2, 1) == 1)</span><br><span class="line">    Sigma2 = diag(Sigma2);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">X = bsxfun(@minus, X, mu(:)&#x27;);</span><br><span class="line">p = (2 * pi) ^ (- k / 2) * det(Sigma2) ^ (-0.5) * ...</span><br><span class="line">    exp(-0.5 * sum(bsxfun(@times, X * pinv(Sigma2), X), 2));</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="selectThreshold-m"><a href="#selectThreshold-m" class="headerlink" title="selectThreshold.m"></a>selectThreshold.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">function [bestEpsilon bestF1] = selectThreshold(yval, pval)</span><br><span class="line">%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting</span><br><span class="line">%outliers</span><br><span class="line">%   [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best</span><br><span class="line">%   threshold to use for selecting outliers based on the results from a</span><br><span class="line">%   validation set (pval) and the ground truth (yval).</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">bestEpsilon = 0;</span><br><span class="line">bestF1 = 0;</span><br><span class="line">F1 = 0;</span><br><span class="line"></span><br><span class="line">stepsize = (max(pval) - min(pval)) / 1000;</span><br><span class="line">for epsilon = min(pval):stepsize:max(pval)</span><br><span class="line">    </span><br><span class="line">    % ====================== YOUR CODE HERE ======================</span><br><span class="line">    % Instructions: Compute the F1 score of choosing epsilon as the</span><br><span class="line">    %               threshold and place the value in F1. The code at the</span><br><span class="line">    %               end of the loop will compare the F1 score for this</span><br><span class="line">    %               choice of epsilon and set it to be the best epsilon if</span><br><span class="line">    %               it is better than the current choice of epsilon.</span><br><span class="line">    %               </span><br><span class="line">    % Note: You can use predictions = (pval &lt; epsilon) to get a binary vector</span><br><span class="line">    %       of 0&#x27;s and 1&#x27;s of the outlier predictions</span><br><span class="line"></span><br><span class="line">    predictions = (pval &lt; epsilon);</span><br><span class="line"></span><br><span class="line">    fp = sum((predictions ==1) &amp; (yval == 0));</span><br><span class="line">    tp = sum((predictions ==1) &amp; (yval == 1));</span><br><span class="line">    fn = sum((predictions ==0) &amp; (yval == 1));</span><br><span class="line"></span><br><span class="line">    prec = tp/(tp+fp);</span><br><span class="line">    rec = tp/(tp+fn);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    F1 = 2*(prec*rec)/(prec+rec);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    % =============================================================</span><br><span class="line"></span><br><span class="line">    if F1 &gt; bestF1</span><br><span class="line">       bestF1 = F1;</span><br><span class="line">       bestEpsilon = epsilon;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="visualizeFit-m"><a href="#visualizeFit-m" class="headerlink" title="visualizeFit.m"></a>visualizeFit.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">function visualizeFit(X, mu, sigma2)</span><br><span class="line">%VISUALIZEFIT Visualize the dataset and its estimated distribution.</span><br><span class="line">%   VISUALIZEFIT(X, p, mu, sigma2) This visualization shows you the </span><br><span class="line">%   probability density function of the Gaussian distribution. Each example</span><br><span class="line">%   has a location (x1, x2) that depends on its feature values.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">[X1,X2] = meshgrid(0:.5:35); </span><br><span class="line">Z = multivariateGaussian([X1(:) X2(:)],mu,sigma2);</span><br><span class="line">Z = reshape(Z,size(X1));</span><br><span class="line"></span><br><span class="line">plot(X(:, 1), X(:, 2),&#x27;bx&#x27;);</span><br><span class="line">hold on;</span><br><span class="line">% Do not plot if there are infinities</span><br><span class="line">if (sum(isinf(Z)) == 0)</span><br><span class="line">    contour(X1, X2, Z, 10.^(-20:3:0)&#x27;);</span><br><span class="line">end</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="ex8-cofi-m"><a href="#ex8-cofi-m" class="headerlink" title="ex8_cofi.m"></a>ex8_cofi.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class</span><br><span class="line">%  Exercise 8 | Anomaly Detection and Collaborative Filtering</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  exercise. You will need to complete the following functions:</span><br><span class="line">%</span><br><span class="line">%     estimateGaussian.m</span><br><span class="line">%     selectThreshold.m</span><br><span class="line">%     cofiCostFunc.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% =============== Part 1: Loading movie ratings dataset ================</span><br><span class="line">%  You will start by loading the movie ratings dataset to understand the</span><br><span class="line">%  structure of the data.</span><br><span class="line">%  </span><br><span class="line">fprintf(&#x27;Loading movie ratings dataset.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Load data</span><br><span class="line">load (&#x27;ex8_movies.mat&#x27;);</span><br><span class="line"></span><br><span class="line">%  Y is a 1682x943 matrix, containing ratings (1-5) of 1682 movies on </span><br><span class="line">%  943 users</span><br><span class="line">%</span><br><span class="line">%  R is a 1682x943 matrix, where R(i,j) = 1 if and only if user j gave a</span><br><span class="line">%  rating to movie i</span><br><span class="line"></span><br><span class="line">%  From the matrix, we can compute statistics like average rating.</span><br><span class="line">fprintf(&#x27;Average rating for movie 1 (Toy Story): %f / 5\n\n&#x27;, ...</span><br><span class="line">        mean(Y(1, R(1, :))));</span><br><span class="line"></span><br><span class="line">%  We can &quot;visualize&quot; the ratings matrix by plotting it with imagesc</span><br><span class="line">imagesc(Y);</span><br><span class="line">ylabel(&#x27;Movies&#x27;);</span><br><span class="line">xlabel(&#x27;Users&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============ Part 2: Collaborative Filtering Cost Function ===========</span><br><span class="line">%  You will now implement the cost function for collaborative filtering.</span><br><span class="line">%  To help you debug your cost function, we have included set of weights</span><br><span class="line">%  that we trained on that. Specifically, you should complete the code in </span><br><span class="line">%  cofiCostFunc.m to return J.</span><br><span class="line"></span><br><span class="line">%  Load pre-trained weights (X, Theta, num_users, num_movies, num_features)</span><br><span class="line">load (&#x27;ex8_movieParams.mat&#x27;);</span><br><span class="line"></span><br><span class="line">%  Reduce the data set size so that this runs faster</span><br><span class="line">num_users = 4; num_movies = 5; num_features = 3;</span><br><span class="line">X = X(1:num_movies, 1:num_features);</span><br><span class="line">Theta = Theta(1:num_users, 1:num_features);</span><br><span class="line">Y = Y(1:num_movies, 1:num_users);</span><br><span class="line">R = R(1:num_movies, 1:num_users);</span><br><span class="line"></span><br><span class="line">%  Evaluate cost function</span><br><span class="line">J = cofiCostFunc([X(:) ; Theta(:)], Y, R, num_users, num_movies, ...</span><br><span class="line">               num_features, 0);</span><br><span class="line">           </span><br><span class="line">fprintf([&#x27;Cost at loaded parameters: %f &#x27;...</span><br><span class="line">         &#x27;\n(this value should be about 22.22)\n&#x27;], J);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============== Part 3: Collaborative Filtering Gradient ==============</span><br><span class="line">%  Once your cost function matches up with ours, you should now implement </span><br><span class="line">%  the collaborative filtering gradient function. Specifically, you should </span><br><span class="line">%  complete the code in cofiCostFunc.m to return the grad argument.</span><br><span class="line">%  </span><br><span class="line">fprintf(&#x27;\nChecking Gradients (without regularization) ... \n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Check gradients by running checkNNGradients</span><br><span class="line">checkCostFunction;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ========= Part 4: Collaborative Filtering Cost Regularization ========</span><br><span class="line">%  Now, you should implement regularization for the cost function for </span><br><span class="line">%  collaborative filtering. You can implement it by adding the cost of</span><br><span class="line">%  regularization to the original cost computation.</span><br><span class="line">%  </span><br><span class="line"></span><br><span class="line">%  Evaluate cost function</span><br><span class="line">J = cofiCostFunc([X(:) ; Theta(:)], Y, R, num_users, num_movies, ...</span><br><span class="line">               num_features, 1.5);</span><br><span class="line">           </span><br><span class="line">fprintf([&#x27;Cost at loaded parameters (lambda = 1.5): %f &#x27;...</span><br><span class="line">         &#x27;\n(this value should be about 31.34)\n&#x27;], J);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ======= Part 5: Collaborative Filtering Gradient Regularization ======</span><br><span class="line">%  Once your cost matches up with ours, you should proceed to implement </span><br><span class="line">%  regularization for the gradient. </span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%  </span><br><span class="line">fprintf(&#x27;\nChecking Gradients (with regularization) ... \n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Check gradients by running checkNNGradients</span><br><span class="line">checkCostFunction(1.5);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============== Part 6: Entering ratings for a new user ===============</span><br><span class="line">%  Before we will train the collaborative filtering model, we will first</span><br><span class="line">%  add ratings that correspond to a new user that we just observed. This</span><br><span class="line">%  part of the code will also allow you to put in your own ratings for the</span><br><span class="line">%  movies in our dataset!</span><br><span class="line">%</span><br><span class="line">movieList = loadMovieList();</span><br><span class="line"></span><br><span class="line">%  Initialize my ratings</span><br><span class="line">my_ratings = zeros(1682, 1);</span><br><span class="line"></span><br><span class="line">% Check the file movie_idx.txt for id of each movie in our dataset</span><br><span class="line">% For example, Toy Story (1995) has ID 1, so to rate it &quot;4&quot;, you can set</span><br><span class="line">my_ratings(1) = 4;</span><br><span class="line"></span><br><span class="line">% Or suppose did not enjoy Silence of the Lambs (1991), you can set</span><br><span class="line">my_ratings(98) = 2;</span><br><span class="line"></span><br><span class="line">% We have selected a few movies we liked / did not like and the ratings we</span><br><span class="line">% gave are as follows:</span><br><span class="line">my_ratings(7) = 3;</span><br><span class="line">my_ratings(12)= 5;</span><br><span class="line">my_ratings(54) = 4;</span><br><span class="line">my_ratings(64)= 5;</span><br><span class="line">my_ratings(66)= 3;</span><br><span class="line">my_ratings(69) = 5;</span><br><span class="line">my_ratings(183) = 4;</span><br><span class="line">my_ratings(226) = 5;</span><br><span class="line">my_ratings(355)= 5;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\n\nNew user ratings:\n&#x27;);</span><br><span class="line">for i = 1:length(my_ratings)</span><br><span class="line">    if my_ratings(i) &gt; 0 </span><br><span class="line">        fprintf(&#x27;Rated %d for %s\n&#x27;, my_ratings(i), ...</span><br><span class="line">                 movieList&#123;i&#125;);</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================== Part 7: Learning Movie Ratings ====================</span><br><span class="line">%  Now, you will train the collaborative filtering model on a movie rating </span><br><span class="line">%  dataset of 1682 movies and 943 users</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nTraining collaborative filtering...\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Load data</span><br><span class="line">load(&#x27;ex8_movies.mat&#x27;);</span><br><span class="line"></span><br><span class="line">%  Y is a 1682x943 matrix, containing ratings (1-5) of 1682 movies by </span><br><span class="line">%  943 users</span><br><span class="line">%</span><br><span class="line">%  R is a 1682x943 matrix, where R(i,j) = 1 if and only if user j gave a</span><br><span class="line">%  rating to movie i</span><br><span class="line"></span><br><span class="line">%  Add our own ratings to the data matrix</span><br><span class="line">Y = [my_ratings Y];</span><br><span class="line">R = [(my_ratings ~= 0) R];</span><br><span class="line"></span><br><span class="line">%  Normalize Ratings</span><br><span class="line">[Ynorm, Ymean] = normalizeRatings(Y, R);</span><br><span class="line"></span><br><span class="line">%  Useful Values</span><br><span class="line">num_users = size(Y, 2);</span><br><span class="line">num_movies = size(Y, 1);</span><br><span class="line">num_features = 10;</span><br><span class="line"></span><br><span class="line">% Set Initial Parameters (Theta, X)</span><br><span class="line">X = randn(num_movies, num_features);</span><br><span class="line">Theta = randn(num_users, num_features);</span><br><span class="line"></span><br><span class="line">initial_parameters = [X(:); Theta(:)];</span><br><span class="line"></span><br><span class="line">% Set options for fmincg</span><br><span class="line">options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, 100);</span><br><span class="line"></span><br><span class="line">% Set Regularization</span><br><span class="line">lambda = 10;</span><br><span class="line">theta = fmincg (@(t)(cofiCostFunc(t, Ynorm, R, num_users, num_movies, ...</span><br><span class="line">                                num_features, lambda)), ...</span><br><span class="line">                initial_parameters, options);</span><br><span class="line"></span><br><span class="line">% Unfold the returned theta back into U and W</span><br><span class="line">X = reshape(theta(1:num_movies*num_features), num_movies, num_features);</span><br><span class="line">Theta = reshape(theta(num_movies*num_features+1:end), ...</span><br><span class="line">                num_users, num_features);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Recommender system learning completed.\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ================== Part 8: Recommendation for you ====================</span><br><span class="line">%  After training the model, you can now make recommendations by computing</span><br><span class="line">%  the predictions matrix.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">p = X * Theta&#x27;;</span><br><span class="line">my_predictions = p(:,1) + Ymean;</span><br><span class="line"></span><br><span class="line">movieList = loadMovieList();</span><br><span class="line"></span><br><span class="line">[r, ix] = sort(my_predictions, &#x27;descend&#x27;);</span><br><span class="line">fprintf(&#x27;\nTop recommendations for you:\n&#x27;);</span><br><span class="line">for i=1:10</span><br><span class="line">    j = ix(i);</span><br><span class="line">    fprintf(&#x27;Predicting rating %.1f for movie %s\n&#x27;, my_predictions(j), ...</span><br><span class="line">            movieList&#123;j&#125;);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\n\nOriginal ratings provided:\n&#x27;);</span><br><span class="line">for i = 1:length(my_ratings)</span><br><span class="line">    if my_ratings(i) &gt; 0 </span><br><span class="line">        fprintf(&#x27;Rated %d for %s\n&#x27;, my_ratings(i), ...</span><br><span class="line">                 movieList&#123;i&#125;);</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="cofiCostFunc-m"><a href="#cofiCostFunc-m" class="headerlink" title="cofiCostFunc.m"></a>cofiCostFunc.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, ...</span><br><span class="line">                                  num_features, lambda)</span><br><span class="line">%COFICOSTFUNC Collaborative filtering cost function</span><br><span class="line">%   [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...</span><br><span class="line">%   num_features, lambda) returns the cost and gradient for the</span><br><span class="line">%   collaborative filtering problem.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Unfold the U and W matrices from params</span><br><span class="line">X = reshape(params(1:num_movies*num_features), num_movies, num_features);</span><br><span class="line">Theta = reshape(params(num_movies*num_features+1:end), ...</span><br><span class="line">                num_users, num_features);</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">% You need to return the following values correctly</span><br><span class="line">J = 0;</span><br><span class="line">X_grad = zeros(size(X));</span><br><span class="line">Theta_grad = zeros(size(Theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost function and gradient for collaborative</span><br><span class="line">%               filtering. Concretely, you should first implement the cost</span><br><span class="line">%               function (without regularization) and make sure it is</span><br><span class="line">%               matches our costs. After that, you should implement the </span><br><span class="line">%               gradient and use the checkCostFunction routine to check</span><br><span class="line">%               that the gradient is correct. Finally, you should implement</span><br><span class="line">%               regularization.</span><br><span class="line">%</span><br><span class="line">% Notes: X - num_movies  x num_features matrix of movie features</span><br><span class="line">%        Theta - num_users  x num_features matrix of user features</span><br><span class="line">%        Y - num_movies x num_users matrix of user ratings of movies</span><br><span class="line">%        R - num_movies x num_users matrix, where R(i, j) = 1 if the </span><br><span class="line">%            i-th movie was rated by the j-th user</span><br><span class="line">%</span><br><span class="line">% You should set the following variables correctly:</span><br><span class="line">%</span><br><span class="line">%        X_grad - num_movies x num_features matrix, containing the </span><br><span class="line">%                 partial derivatives w.r.t. to each element of X</span><br><span class="line">%        Theta_grad - num_users x num_features matrix, containing the </span><br><span class="line">%                     partial derivatives w.r.t. to each element of Theta</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">J = 1/2*sum(sum((X*Theta&#x27;-Y).^2.*R));</span><br><span class="line"></span><br><span class="line">c_regular = 0;</span><br><span class="line">c_regular = (lambda/2)*sum(sum(Theta.^2)) + (lambda/2)*sum(sum(X.^2));</span><br><span class="line"></span><br><span class="line">J = J + c_regular;</span><br><span class="line"></span><br><span class="line">X_grad = ((X*Theta&#x27;-Y).*R)*Theta;</span><br><span class="line">Theta_grad = ((X*Theta&#x27;-Y).*R)&#x27;*X;</span><br><span class="line"></span><br><span class="line">X_grad_regular = lambda*X;</span><br><span class="line">Theta_grad_regular = lambda*Theta;</span><br><span class="line"></span><br><span class="line">X_grad = X_grad + X_grad_regular;</span><br><span class="line">Theta_grad = Theta_grad + Theta_grad_regular;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">grad = [X_grad(:); Theta_grad(:)];</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="checkCostFunction-m"><a href="#checkCostFunction-m" class="headerlink" title="checkCostFunction.m"></a>checkCostFunction.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">function checkCostFunction(lambda)</span><br><span class="line">%CHECKCOSTFUNCTION Creates a collaborative filering problem </span><br><span class="line">%to check your cost function and gradients</span><br><span class="line">%   CHECKCOSTFUNCTION(lambda) Creates a collaborative filering problem </span><br><span class="line">%   to check your cost function and gradients, it will output the </span><br><span class="line">%   analytical gradients produced by your code and the numerical gradients </span><br><span class="line">%   (computed using computeNumericalGradient). These two gradient </span><br><span class="line">%   computations should result in very similar values.</span><br><span class="line"></span><br><span class="line">% Set lambda</span><br><span class="line">if ~exist(&#x27;lambda&#x27;, &#x27;var&#x27;) || isempty(lambda)</span><br><span class="line">    lambda = 0;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">%% Create small problem</span><br><span class="line">X_t = rand(4, 3);</span><br><span class="line">Theta_t = rand(5, 3);</span><br><span class="line"></span><br><span class="line">% Zap out most entries</span><br><span class="line">Y = X_t * Theta_t&#x27;;</span><br><span class="line">Y(rand(size(Y)) &gt; 0.5) = 0;</span><br><span class="line">R = zeros(size(Y));</span><br><span class="line">R(Y ~= 0) = 1;</span><br><span class="line"></span><br><span class="line">%% Run Gradient Checking</span><br><span class="line">X = randn(size(X_t));</span><br><span class="line">Theta = randn(size(Theta_t));</span><br><span class="line">num_users = size(Y, 2);</span><br><span class="line">num_movies = size(Y, 1);</span><br><span class="line">num_features = size(Theta_t, 2);</span><br><span class="line"></span><br><span class="line">numgrad = computeNumericalGradient( ...</span><br><span class="line">                @(t) cofiCostFunc(t, Y, R, num_users, num_movies, ...</span><br><span class="line">                                num_features, lambda), [X(:); Theta(:)]);</span><br><span class="line"></span><br><span class="line">[cost, grad] = cofiCostFunc([X(:); Theta(:)],  Y, R, num_users, ...</span><br><span class="line">                          num_movies, num_features, lambda);</span><br><span class="line"></span><br><span class="line">disp([numgrad grad]);</span><br><span class="line">fprintf([&#x27;The above two columns you get should be very similar.\n&#x27; ...</span><br><span class="line">         &#x27;(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\n&#x27;]);</span><br><span class="line"></span><br><span class="line">diff = norm(numgrad-grad)/norm(numgrad+grad);</span><br><span class="line">fprintf([&#x27;If your cost function implementation is correct, then \n&#x27; ...</span><br><span class="line">         &#x27;the relative difference will be small (less than 1e-9). \n&#x27; ...</span><br><span class="line">         &#x27;\nRelative Difference: %g\n&#x27;], diff);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="computeNumericalGradient-m"><a href="#computeNumericalGradient-m" class="headerlink" title="computeNumericalGradient.m"></a>computeNumericalGradient.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">function numgrad = computeNumericalGradient(J, theta)</span><br><span class="line">%COMPUTENUMERICALGRADIENT Computes the gradient using &quot;finite differences&quot;</span><br><span class="line">%and gives us a numerical estimate of the gradient.</span><br><span class="line">%   numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical</span><br><span class="line">%   gradient of the function J around theta. Calling y = J(theta) should</span><br><span class="line">%   return the function value at theta.</span><br><span class="line"></span><br><span class="line">% Notes: The following code implements numerical gradient checking, and </span><br><span class="line">%        returns the numerical gradient.It sets numgrad(i) to (a numerical </span><br><span class="line">%        approximation of) the partial derivative of J with respect to the </span><br><span class="line">%        i-th input argument, evaluated at theta. (i.e., numgrad(i) should </span><br><span class="line">%        be the (approximately) the partial derivative of J with respect </span><br><span class="line">%        to theta(i).)</span><br><span class="line">%                </span><br><span class="line"></span><br><span class="line">numgrad = zeros(size(theta));</span><br><span class="line">perturb = zeros(size(theta));</span><br><span class="line">e = 1e-4;</span><br><span class="line">for p = 1:numel(theta)</span><br><span class="line">    % Set perturbation vector</span><br><span class="line">    perturb(p) = e;</span><br><span class="line">    loss1 = J(theta - perturb);</span><br><span class="line">    loss2 = J(theta + perturb);</span><br><span class="line">    % Compute Numerical Gradient</span><br><span class="line">    numgrad(p) = (loss2 - loss1) / (2*e);</span><br><span class="line">    perturb(p) = 0;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="loadMovieList-m"><a href="#loadMovieList-m" class="headerlink" title="loadMovieList.m"></a>loadMovieList.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">function movieList = loadMovieList()</span><br><span class="line">%GETMOVIELIST reads the fixed movie list in movie.txt and returns a</span><br><span class="line">%cell array of the words</span><br><span class="line">%   movieList = GETMOVIELIST() reads the fixed movie list in movie.txt </span><br><span class="line">%   and returns a cell array of the words in movieList.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% Read the fixed movieulary list</span><br><span class="line">fid = fopen(&#x27;movie_ids.txt&#x27;);</span><br><span class="line"></span><br><span class="line">% Store all movies in cell array movie&#123;&#125;</span><br><span class="line">n = 1682;  % Total number of movies </span><br><span class="line"></span><br><span class="line">movieList = cell(n, 1);</span><br><span class="line">for i = 1:n</span><br><span class="line">    % Read line</span><br><span class="line">    line = fgets(fid);</span><br><span class="line">    % Word Index (can ignore since it will be = i)</span><br><span class="line">    [idx, movieName] = strtok(line, &#x27; &#x27;);</span><br><span class="line">    % Actual Word</span><br><span class="line">    movieList&#123;i&#125; = strtrim(movieName);</span><br><span class="line">end</span><br><span class="line">fclose(fid);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="normalizeRatings-m"><a href="#normalizeRatings-m" class="headerlink" title="normalizeRatings.m"></a>normalizeRatings.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">function [Ynorm, Ymean] = normalizeRatings(Y, R)</span><br><span class="line">%NORMALIZERATINGS Preprocess data by subtracting mean rating for every </span><br><span class="line">%movie (every row)</span><br><span class="line">%   [Ynorm, Ymean] = NORMALIZERATINGS(Y, R) normalized Y so that each movie</span><br><span class="line">%   has a rating of 0 on average, and returns the mean rating in Ymean.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">[m, n] = size(Y);</span><br><span class="line">Ymean = zeros(m, 1);</span><br><span class="line">Ynorm = zeros(size(Y));</span><br><span class="line">for i = 1:m</span><br><span class="line">    idx = find(R(i, :) == 1);</span><br><span class="line">    Ymean(i) = mean(Y(i, idx));</span><br><span class="line">    Ynorm(i, idx) = Y(i, idx) - Ymean(i);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="fmincg-m"><a href="#fmincg-m" class="headerlink" title="fmincg.m"></a>fmincg.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line">function [X, fX, i] = fmincg(f, X, options, P1, P2, P3, P4, P5)</span><br><span class="line">% Minimize a continuous differentialble multivariate function. Starting point</span><br><span class="line">% is given by &quot;X&quot; (D by 1), and the function named in the string &quot;f&quot;, must</span><br><span class="line">% return a function value and a vector of partial derivatives. The Polack-</span><br><span class="line">% Ribiere flavour of conjugate gradients is used to compute search directions,</span><br><span class="line">% and a line search using quadratic and cubic polynomial approximations and the</span><br><span class="line">% Wolfe-Powell stopping criteria is used together with the slope ratio method</span><br><span class="line">% for guessing initial step sizes. Additionally a bunch of checks are made to</span><br><span class="line">% make sure that exploration is taking place and that extrapolation will not</span><br><span class="line">% be unboundedly large. The &quot;length&quot; gives the length of the run: if it is</span><br><span class="line">% positive, it gives the maximum number of line searches, if negative its</span><br><span class="line">% absolute gives the maximum allowed number of function evaluations. You can</span><br><span class="line">% (optionally) give &quot;length&quot; a second component, which will indicate the</span><br><span class="line">% reduction in function value to be expected in the first line-search (defaults</span><br><span class="line">% to 1.0). The function returns when either its length is up, or if no further</span><br><span class="line">% progress can be made (ie, we are at a minimum, or so close that due to</span><br><span class="line">% numerical problems, we cannot get any closer). If the function terminates</span><br><span class="line">% within a few iterations, it could be an indication that the function value</span><br><span class="line">% and derivatives are not consistent (ie, there may be a bug in the</span><br><span class="line">% implementation of your &quot;f&quot; function). The function returns the found</span><br><span class="line">% solution &quot;X&quot;, a vector of function values &quot;fX&quot; indicating the progress made</span><br><span class="line">% and &quot;i&quot; the number of iterations (line searches or function evaluations,</span><br><span class="line">% depending on the sign of &quot;length&quot;) used.</span><br><span class="line">%</span><br><span class="line">% Usage: [X, fX, i] = fmincg(f, X, options, P1, P2, P3, P4, P5)</span><br><span class="line">%</span><br><span class="line">% See also: checkgrad </span><br><span class="line">%</span><br><span class="line">% Copyright (C) 2001 and 2002 by Carl Edward Rasmussen. Date 2002-02-13</span><br><span class="line">%</span><br><span class="line">%</span><br><span class="line">% (C) Copyright 1999, 2000 &amp; 2001, Carl Edward Rasmussen</span><br><span class="line">% </span><br><span class="line">% Permission is granted for anyone to copy, use, or modify these</span><br><span class="line">% programs and accompanying documents for purposes of research or</span><br><span class="line">% education, provided this copyright notice is retained, and note is</span><br><span class="line">% made of any changes that have been made.</span><br><span class="line">% </span><br><span class="line">% These programs and documents are distributed without any warranty,</span><br><span class="line">% express or implied.  As the programs were written for research</span><br><span class="line">% purposes only, they have not been tested to the degree that would be</span><br><span class="line">% advisable in any important application.  All use of these programs is</span><br><span class="line">% entirely at the user&#x27;s own risk.</span><br><span class="line">%</span><br><span class="line">% [ml-class] Changes Made:</span><br><span class="line">% 1) Function name and argument specifications</span><br><span class="line">% 2) Output display</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Read options</span><br><span class="line">if exist(&#x27;options&#x27;, &#x27;var&#x27;) &amp;&amp; ~isempty(options) &amp;&amp; isfield(options, &#x27;MaxIter&#x27;)</span><br><span class="line">    length = options.MaxIter;</span><br><span class="line">else</span><br><span class="line">    length = 100;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RHO = 0.01;                            % a bunch of constants for line searches</span><br><span class="line">SIG = 0.5;       % RHO and SIG are the constants in the Wolfe-Powell conditions</span><br><span class="line">INT = 0.1;    % don&#x27;t reevaluate within 0.1 of the limit of the current bracket</span><br><span class="line">EXT = 3.0;                    % extrapolate maximum 3 times the current bracket</span><br><span class="line">MAX = 20;                         % max 20 function evaluations per line search</span><br><span class="line">RATIO = 100;                                      % maximum allowed slope ratio</span><br><span class="line"></span><br><span class="line">argstr = [&#x27;feval(f, X&#x27;];                      % compose string used to call function</span><br><span class="line">for i = 1:(nargin - 3)</span><br><span class="line">  argstr = [argstr, &#x27;,P&#x27;, int2str(i)];</span><br><span class="line">end</span><br><span class="line">argstr = [argstr, &#x27;)&#x27;];</span><br><span class="line"></span><br><span class="line">if max(size(length)) == 2, red=length(2); length=length(1); else red=1; end</span><br><span class="line">S=[&#x27;Iteration &#x27;];</span><br><span class="line"></span><br><span class="line">i = 0;                                            % zero the run length counter</span><br><span class="line">ls_failed = 0;                             % no previous line search has failed</span><br><span class="line">fX = [];</span><br><span class="line">[f1 df1] = eval(argstr);                      % get function value and gradient</span><br><span class="line">i = i + (length&lt;0);                                            % count epochs?!</span><br><span class="line">s = -df1;                                        % search direction is steepest</span><br><span class="line">d1 = -s&#x27;*s;                                                 % this is the slope</span><br><span class="line">z1 = red/(1-d1);                                  % initial step is red/(|s|+1)</span><br><span class="line"></span><br><span class="line">while i &lt; abs(length)                                      % while not finished</span><br><span class="line">  i = i + (length&gt;0);                                      % count iterations?!</span><br><span class="line"></span><br><span class="line">  X0 = X; f0 = f1; df0 = df1;                   % make a copy of current values</span><br><span class="line">  X = X + z1*s;                                             % begin line search</span><br><span class="line">  [f2 df2] = eval(argstr);</span><br><span class="line">  i = i + (length&lt;0);                                          % count epochs?!</span><br><span class="line">  d2 = df2&#x27;*s;</span><br><span class="line">  f3 = f1; d3 = d1; z3 = -z1;             % initialize point 3 equal to point 1</span><br><span class="line">  if length&gt;0, M = MAX; else M = min(MAX, -length-i); end</span><br><span class="line">  success = 0; limit = -1;                     % initialize quanteties</span><br><span class="line">  while 1</span><br><span class="line">    while ((f2 &gt; f1+z1*RHO*d1) || (d2 &gt; -SIG*d1)) &amp;&amp; (M &gt; 0) </span><br><span class="line">      limit = z1;                                         % tighten the bracket</span><br><span class="line">      if f2 &gt; f1</span><br><span class="line">        z2 = z3 - (0.5*d3*z3*z3)/(d3*z3+f2-f3);                 % quadratic fit</span><br><span class="line">      else</span><br><span class="line">        A = 6*(f2-f3)/z3+3*(d2+d3);                                 % cubic fit</span><br><span class="line">        B = 3*(f3-f2)-z3*(d3+2*d2);</span><br><span class="line">        z2 = (sqrt(B*B-A*d2*z3*z3)-B)/A;       % numerical error possible - ok!</span><br><span class="line">      end</span><br><span class="line">      if isnan(z2) || isinf(z2)</span><br><span class="line">        z2 = z3/2;                  % if we had a numerical problem then bisect</span><br><span class="line">      end</span><br><span class="line">      z2 = max(min(z2, INT*z3),(1-INT)*z3);  % don&#x27;t accept too close to limits</span><br><span class="line">      z1 = z1 + z2;                                           % update the step</span><br><span class="line">      X = X + z2*s;</span><br><span class="line">      [f2 df2] = eval(argstr);</span><br><span class="line">      M = M - 1; i = i + (length&lt;0);                           % count epochs?!</span><br><span class="line">      d2 = df2&#x27;*s;</span><br><span class="line">      z3 = z3-z2;                    % z3 is now relative to the location of z2</span><br><span class="line">    end</span><br><span class="line">    if f2 &gt; f1+z1*RHO*d1 || d2 &gt; -SIG*d1</span><br><span class="line">      break;                                                % this is a failure</span><br><span class="line">    elseif d2 &gt; SIG*d1</span><br><span class="line">      success = 1; break;                                             % success</span><br><span class="line">    elseif M == 0</span><br><span class="line">      break;                                                          % failure</span><br><span class="line">    end</span><br><span class="line">    A = 6*(f2-f3)/z3+3*(d2+d3);                      % make cubic extrapolation</span><br><span class="line">    B = 3*(f3-f2)-z3*(d3+2*d2);</span><br><span class="line">    z2 = -d2*z3*z3/(B+sqrt(B*B-A*d2*z3*z3));        % num. error possible - ok!</span><br><span class="line">    if ~isreal(z2) || isnan(z2) || isinf(z2) || z2 &lt; 0 % num prob or wrong sign?</span><br><span class="line">      if limit &lt; -0.5                               % if we have no upper limit</span><br><span class="line">        z2 = z1 * (EXT-1);                 % the extrapolate the maximum amount</span><br><span class="line">      else</span><br><span class="line">        z2 = (limit-z1)/2;                                   % otherwise bisect</span><br><span class="line">      end</span><br><span class="line">    elseif (limit &gt; -0.5) &amp;&amp; (z2+z1 &gt; limit)         % extraplation beyond max?</span><br><span class="line">      z2 = (limit-z1)/2;                                               % bisect</span><br><span class="line">    elseif (limit &lt; -0.5) &amp;&amp; (z2+z1 &gt; z1*EXT)       % extrapolation beyond limit</span><br><span class="line">      z2 = z1*(EXT-1.0);                           % set to extrapolation limit</span><br><span class="line">    elseif z2 &lt; -z3*INT</span><br><span class="line">      z2 = -z3*INT;</span><br><span class="line">    elseif (limit &gt; -0.5) &amp;&amp; (z2 &lt; (limit-z1)*(1.0-INT))  % too close to limit?</span><br><span class="line">      z2 = (limit-z1)*(1.0-INT);</span><br><span class="line">    end</span><br><span class="line">    f3 = f2; d3 = d2; z3 = -z2;                  % set point 3 equal to point 2</span><br><span class="line">    z1 = z1 + z2; X = X + z2*s;                      % update current estimates</span><br><span class="line">    [f2 df2] = eval(argstr);</span><br><span class="line">    M = M - 1; i = i + (length&lt;0);                             % count epochs?!</span><br><span class="line">    d2 = df2&#x27;*s;</span><br><span class="line">  end                                                      % end of line search</span><br><span class="line"></span><br><span class="line">  if success                                         % if line search succeeded</span><br><span class="line">    f1 = f2; fX = [fX&#x27; f1]&#x27;;</span><br><span class="line">    fprintf(&#x27;%s %4i | Cost: %4.6e\r&#x27;, S, i, f1);</span><br><span class="line">    s = (df2&#x27;*df2-df1&#x27;*df2)/(df1&#x27;*df1)*s - df2;      % Polack-Ribiere direction</span><br><span class="line">    tmp = df1; df1 = df2; df2 = tmp;                         % swap derivatives</span><br><span class="line">    d2 = df1&#x27;*s;</span><br><span class="line">    if d2 &gt; 0                                      % new slope must be negative</span><br><span class="line">      s = -df1;                              % otherwise use steepest direction</span><br><span class="line">      d2 = -s&#x27;*s;    </span><br><span class="line">    end</span><br><span class="line">    z1 = z1 * min(RATIO, d1/(d2-realmin));          % slope ratio but max RATIO</span><br><span class="line">    d1 = d2;</span><br><span class="line">    ls_failed = 0;                              % this line search did not fail</span><br><span class="line">  else</span><br><span class="line">    X = X0; f1 = f0; df1 = df0;  % restore point from before failed line search</span><br><span class="line">    if ls_failed || i &gt; abs(length)          % line search failed twice in a row</span><br><span class="line">      break;                             % or we ran out of time, so we give up</span><br><span class="line">    end</span><br><span class="line">    tmp = df1; df1 = df2; df2 = tmp;                         % swap derivatives</span><br><span class="line">    s = -df1;                                                    % try steepest</span><br><span class="line">    d1 = -s&#x27;*s;</span><br><span class="line">    z1 = 1/(1-d1);                     </span><br><span class="line">    ls_failed = 1;                                    % this line search failed</span><br><span class="line">  end</span><br><span class="line">  if exist(&#x27;OCTAVE_VERSION&#x27;)</span><br><span class="line">    fflush(stdout);</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line">fprintf(&#x27;\n&#x27;);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 21-execrise 8 summary。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-20-Recommender Systems</title>
    <link href="https://shilei165.github.io/2022/09/11/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-20/"/>
    <id>https://shilei165.github.io/2022/09/11/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-20/</id>
    <published>2022-09-11T18:33:23.000Z</published>
    <updated>2022-09-13T17:26:49.286Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 20-推荐系统(Recommender Systems)。</p><span id="more"></span><h1 id="为什么学习推荐系统"><a href="#为什么学习推荐系统" class="headerlink" title="为什么学习推荐系统"></a>为什么学习推荐系统</h1><ol><li>推荐系统是机器学习中的一个重要的应用。如果你考虑网站像亚马逊，或网飞公司或易趣，或iTunes Genius，有很多的网站或系统试图推荐新产品给用户。如，亚马逊推荐新书给你，网飞公司试图推荐新电影给你，等等。这些推荐系统，根据浏览你过去买过什么书，或过去评价过什么电影来判断。这些系统会带来很大一部分收入，比如为亚马逊和像网飞这样的公司。因此，对推荐系统性能的改善，将对这些企业的有实质性和直接的影响。</li><li>通过推荐系统，我们将领略一小部分特征学习的思想。对机器学习来说，特征是很重要的，你所选择的特征，将对你学习算法的性能有很大的影响。因此，在机器学习中有一种大思想，它针对一些问题(可能并不是所有的问题)，有些现有的算法可以为你自动学习一套好的特征。</li></ol><h1 id="问题形式化"><a href="#问题形式化" class="headerlink" title="问题形式化"></a>问题形式化</h1><p>我们从一个例子开始定义推荐系统的问题。</p><p>假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。</p><p><img src="/../images/recommenderSys_1.png" alt="recommenderSys_1"></p><p>前三部电影是爱情片，后两部则是动作片，我们可以看出Alice和Bob似乎更倾向与爱情片， 而 Carol 和 Dave 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。</p><p>下面引入一些标记：</p><ul><li>\(n_u\)代表用户的数量</li><li>\(n_m\)代表电影的数量</li><li>\(r(i,j)\) 如果用户\(j\)给电影\(i\)评过分则\(r(i,j)&#x3D;1\)</li><li>\(y^{(i,j)}\)代表用户\(j\)给电影\(i\)的评分</li><li>\(m_j\)代表用户\(j\)评过分的电影的总数</li></ul><h1 id="基于内容的推荐系统"><a href="#基于内容的推荐系统" class="headerlink" title="基于内容的推荐系统"></a>基于内容的推荐系统</h1><p>在我们的例子中，我们可以假设每部电影都有两个特征，如\(x_1\)代表电影的浪漫程度，\(x_2\)代表电影的动作程度。</p><p><img src="/../images/recommenderSys_2.png" alt="recommenderSys_2"></p><p>则每部电影都有一个特征向量，如\(x^{(1)}\)是第一部电影的特征向量为[0.9 0]。</p><p>下面我们要基于这些特征来构建一个推荐系统算法。 假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如\(\theta^{(1)}\)是第一个用户的模型的参数。 于是我们有：</p><ul><li>\(\theta^{(j)}\)：用户\(j\)的参数向量</li><li>\(x^{(i)}\)：电影\(i\)的特征向量</li><li>对于用户\(j\)和电影\(i\)，我们的预测评分为\((\theta^{(j)})^Tx^{(i)}\)</li></ul><p><strong>代价函数</strong><br>$$<br>J(\theta^{(j)})&#x3D;\frac{1}{2}\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{k&#x3D;1}^n(\theta_k^{(j)})^2<br>$$<br>其中\(i:r(i,j)&#x3D;1\)表示我们只计算那些用户\(j\)评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以\(\frac{1}{2m}\)，在这里我们将m去掉。并且我们不对方差项\(\theta_0\)进行正则化处理。</p><p>上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：<br>$$<br>J(\theta^{(1)},…,\theta^{(n_u)})&#x3D;\frac{1}{2}\sum_{j&#x3D;1}^{n_u}\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j&#x3D;1}^{n_u}\sum_{k&#x3D;1}^n(\theta_k^{(j)})^2<br>$$</p><p><strong>梯度下降求解公式</strong></p><p>如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式如下：</p><p>for \(k&#x3D;0\):<br>$$<br>\theta_k^{(j)}:&#x3D;\theta_k^{(j)}-\alpha\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}<br>$$</p><p>for \(k \neq 0\):<br>$$<br>\theta_k^{(j)}:&#x3D;\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} + \lambda\theta_k^{(j)})<br>$$</p><h1 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h1><p>在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。</p><p><img src="/../images/recommenderSys_3.png" alt="recommenderSys_3"></p><p>但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。我们的优化目标便改为同时针对\(x\)和\(\theta\)进行。<br>$$<br>J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)}) &#x3D; \frac{1}{2}\sum_{j&#x3D;1}^{n_u}\sum_{(i,j):r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i&#x3D;1}^{n_m}\sum_{k&#x3D;1}^n(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j&#x3D;1}^{n_u}\sum_{k&#x3D;1}^n(\theta_k^{(j)})^2<br>$$<br>对代价函数求偏导数的结果如下：<br>$$<br>x_k^{(i)}:&#x3D;x_k^{(i)}-\alpha(\sum_{j:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(i)} + \lambda x_k^{(j)})<br>$$</p><p>$$<br>\theta_k^{(j)}:&#x3D;\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} + \lambda\theta_k^{(j)})<br>$$</p><p>协同过滤算法使用步骤如下：</p><ul><li>初始\(x^{(1)},…,x^{(n_m),\theta^{(1)},…,\theta^{(n_u)}\)为一些小的随机值</li><li>使用梯度下降算法最小化代价函数</li><li>在训练完算法后，我们预测\((\theta^{(j)})^Tx^{(i)}\)为用户\(j\)给电影\(i\)的评分</li></ul><h1 id="向量化：低秩矩阵分解"><a href="#向量化：低秩矩阵分解" class="headerlink" title="向量化：低秩矩阵分解"></a>向量化：低秩矩阵分解</h1><p>这里将会讲到有关协同过滤算法的向量化实现，以及说说有关该算法你可以做的其他事情，例如：</p><ol><li>当给出一件产品时，你能否找到与之相关的其它产品。</li><li>一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。</li></ol><h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>我们有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。</p><p>我们有五部电影，以及四位用户，那么 这个矩阵Y就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里：</p><p><img src="/../images/recommenderSys_4.png" alt="recommenderSys_4"></p><p>使用协同过滤算法，我们就可以找到特征向量X，以及参数\(\Theta\)。从而，推导出预测的评分\(X\Theta^T\)</p><p><img src="/../images/recommenderSys_5.png" alt="recommenderSys_5"></p><h2 id="找到相似产品"><a href="#找到相似产品" class="headerlink" title="找到相似产品"></a>找到相似产品</h2><p>当用户在看某部电影  的时候，如果你想找5部与电影非常相似的电影，为了能给用户推荐5部新电影，你需要做的是找出电影\(j\)与我们要找的电影\(i\)的距离最小，这样你就能给你的用户推荐几部不同的电影了。</p><p><img src="/../images/recommenderSys_6.png" alt="recommenderSys_6"></p><h1 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h1><p>让我们来看下面的用户评分数据：</p><p><img src="/../images/recommenderSys_7.png" alt="recommenderSys_7"></p><p>如果我们新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么我们以什么为依据为Eve推荐电影呢？</p><p>我们首先需要对结果 矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值：</p><p><img src="/../images/recommenderSys_8.png" alt="recommenderSys_8"></p><p>然后我们利用这个新的Y矩阵来训练算法。 如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测\((\theta^{(j)})^Tx^{(i)}+\mu_i\)，对于Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 20-推荐系统(Recommender Systems)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-19-Anomaly Detection</title>
    <link href="https://shilei165.github.io/2022/09/11/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-19/"/>
    <id>https://shilei165.github.io/2022/09/11/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-19/</id>
    <published>2022-09-11T15:39:23.000Z</published>
    <updated>2022-09-13T17:26:20.769Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 19-异常检测(Anomaly Detection)。</p><span id="more"></span><h1 id="问题的动机"><a href="#问题的动机" class="headerlink" title="问题的动机"></a>问题的动机</h1><h2 id="什么是异常检测？"><a href="#什么是异常检测？" class="headerlink" title="什么是异常检测？"></a>什么是异常检测？</h2><p>异常检测(Anomaly Detection)问题是机器学习算法的一个常见的应用。这种算法的一个有趣之处在于：它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。</p><p>我们来举例说明：</p><p>假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行QA(质量控制测试)，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎运转时产生的热量，或者引擎的振动等等。这样一来，你就有了一个数据集，从\(x^{(1)}\)到\(x^{(m)}\)，你将这些数据绘制成图表，看起来就是这个样子：</p><p><img src="/../images/anomalyDetection_1.png" alt="anomalyDetection_1"></p><p>这里的每个数据点都是无标签的数据。异常检测问题可以定义如下：我们假设后来有一天，你有一个新的飞机引擎从生产线上流出，而你的新飞机引擎有特征变量\(x_{test}\)。我们希望通过对现有数据的分析知道这个新的飞机引擎是否有某种异常，而不需要进一步的测试。</p><p>假使给定的数据集\(x^{(1)},x^{(2)},…x^{(m)}\)是正常的，我们需要知道新的数据\(x_{test}\)是不是正常的。我们可以把这个问题转化为测试该数据不属于给定数据集的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的概率\(p(x)\)。</p><p><img src="/../images/anomalyDetection_2.png" alt="anomalyDetection_2"></p><p>上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。这种方法称为密度估计，表达如下：</p><p>$$<br>if\ \ p(x)<br>\begin{cases}<br>&lt; &amp; \epsilon &amp; anomaly \\<br>\geq &amp; \epsilon &amp; normal<br>\end{cases}<br>$$</p><h2 id="异常检测例子"><a href="#异常检测例子" class="headerlink" title="异常检测例子"></a>异常检测例子</h2><h3 id="欺诈检测"><a href="#欺诈检测" class="headerlink" title="欺诈检测"></a>欺诈检测</h3><p>异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。</p><ul><li>通过建模来计算\(p(x)\)</li><li>检测\(p(x)&lt;\epsilon\)来确定非正常用户。</li></ul><h3 id="生产制造残次品检测"><a href="#生产制造残次品检测" class="headerlink" title="生产制造残次品检测"></a>生产制造残次品检测</h3><p>例如我们上面提到的飞机引擎的例子</p><h3 id="数据中心监测"><a href="#数据中心监测" class="headerlink" title="数据中心监测"></a>数据中心监测</h3><p>再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量，CPU的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。</p><h1 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h1><p>高斯分布，也称为正态分布。先回顾以下高斯分布的基本知识：</p><p>通常如果我们认为变量\(x\)符合高斯分布\(x~N(\mu, \sigma^2)\)则其概率密度函数为：<br>$$<br>p(x;\mu,\sigma^2)&#x3D;\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})<br>$$</p><p>我们可以利用已有的数据来预测总体中的\(\mu\)和\(\sigma^2\),计算方法如下： </p><p>$$<br>\mu &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}x^{(i)}<br>$$</p><p>$$<br>\sigma^2 &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}(x^{(i)}-\mu)^2<br>$$</p><p><img src="/../images/Gaussian_1.png" alt="Gaussian_1"></p><p>注：机器学习中对于方差我们通常只除以m而非统计学中的(m-1)。这两个版本的公式在理论特性和数学特性上稍有不同，但是在实际使用中，他们的区别甚小，几乎可以忽略不计。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><h2 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h2><p>对于给定的数据集\(x^{(1)},x^{(2)},…x^{(m)}\)，我们要针对每一个特征计算\(\mu\)和\(\sigma^2\)的估计值。<br>$$<br>\mu &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}x_j^{(i)}<br>$$</p><p>$$<br>\sigma^2 &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}(x_j^{(i)}-\mu_j)^2<br>$$</p><p>一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算\(p(x)\)：<br>$$<br>p(x) &#x3D; \Pi_{j&#x3D;1}^{n}p(x_j;\mu_j,\sigma^2)&#x3D;\Pi_{j&#x3D;1}^{1}\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})<br>$$</p><p>当\(p(x) &lt; \epsilon\)时为异常。</p><p>例如下图是一个由两个特征的训练集，以及特征的分布情况：</p><p><img src="/../images/anomalyDetection_3.png" alt="anomalyDetection_3"></p><p>下面的三维图表表示的是密度估计函数，z轴为根据两个特征的值所估计的\(p(x)\)值：</p><p><img src="/../images/anomalyDetection_4.png" alt="anomalyDetection_4"></p><p>我们需要选择一个\(\epsilon\)，将\(p(x)&#x3D;\epsilon\)作为我们的判定边界，当时预测数据\(p(x)&gt;\epsilon\)为正常数据，否则为异常。</p><h2 id="开发和评价一个异常检测系统"><a href="#开发和评价一个异常检测系统" class="headerlink" title="开发和评价一个异常检测系统"></a>开发和评价一个异常检测系统</h2><p>异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量y的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。</p><p>当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。</p><p>例如：我们有10000台正常引擎的数据，有20台异常引擎的数据。 我们这样分配数据：</p><ul><li><p>6000台正常引擎的数据作为训练集</p></li><li><p>2000台正常引擎和10台异常引擎的数据作为交叉检验集</p></li><li><p>2000台正常引擎和10台异常引擎的数据作为测试集</p></li></ul><p>具体的评价方法如下：</p><ul><li>根据测试集数据，我们估计特征的平均值和方差并构建\(p(x)\)函数</li><li>对交叉检验集，我们尝试使用不同的\(\epsilon\)值作为阀值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择 \(\epsilon\)</li><li>选出\(\epsilon\)后，针对测试集进行预测，计算异常检验系统的F1值，或者查准率与查全率之比</li></ul><h1 id="异常检测与监督学习对比"><a href="#异常检测与监督学习对比" class="headerlink" title="异常检测与监督学习对比"></a>异常检测与监督学习对比</h1><p>之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对比有助于选择采用监督学习还是异常检测。</p><table><thead><tr><th>异常检测</th><th>监督学习</th></tr></thead><tbody><tr><td>非常少量的正向类（异常数据y&#x3D;1）, 大量的负向类（y&#x3D;0）</td><td>同时有大量的正向类和负向类</td></tr><tr><td>许多不同种类的异常。对于任何算法来说，根据非常少量的正向类数据来进行训练什么是异常都是非常困难的</td><td>有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似</td></tr><tr><td>未来遇到的异常可能与已掌握的异常、非常的不同</td><td></td></tr><tr><td>例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况</td><td>例如：邮件过滤器 天气预报 肿瘤分类</td></tr></tbody></table><h1 id="选择特征"><a href="#选择特征" class="headerlink" title="选择特征"></a>选择特征</h1><p>对于异常检测算法，我们使用的特征是至关重要的，下面谈谈如何选择特征：</p><h2 id="高斯分布转换"><a href="#高斯分布转换" class="headerlink" title="高斯分布转换"></a>高斯分布转换</h2><p>异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布。</p><p>例如使用对数函数：\(x&#x3D;log(x+c)\)，其中\(c\)为非负常数； 或者\(x&#x3D;x^c\)，\(c\)为 0-1 之间的一个分数，等方法。</p><p><img src="/../images/anomalyDetection_5.png" alt="anomalyDetection_5"></p><h2 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h2><p>我们异常检测系统希望得到的结果:</p><ul><li>对于正常样品，我们希望获得较大的p(x)值</li><li>对于异常样品，我们希望获得较小的p(x)值</li></ul><p>但在异常检测问题中一个常见的问题就是，对于正常和异常样品，我们获得的p(x)值比较接近，从而无法真正把异常样品挑选出来。</p><p><img src="/../images/anomalyDetection_6.png" alt="anomalyDetection_6"></p><p>我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。</p><h1 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h1><p>假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。</p><p>下图中是两个相关特征，洋红色的线（根据ε的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的X所代表的数据点很可能是异常值，但是其p(x)值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。</p><p><img src="/../images/anomalyDetection_7.png" alt="anomalyDetection_7"></p><p>在一般的高斯分布模型中，我们计算p(x)的方法是： 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算p(x)。</p><p>$$<br>\mu&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^{m}x^{(i)}<br>$$<br>$$<br>\Sigma &#x3D; \frac{1}{m}(x^{(i)}-\mu)(x^{(i)}-\mu)^T&#x3D;\frac{1}{m}(X-\mu)^T(X-\mu)<br>$$</p><p>其中，\(\mu\)是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。</p><p>最后我们计算多元高斯分布的p(x):<br>$$<br>p(x;\mu,\Sigma)&#x3D;\frac{1}{(2\pi)^{n&#x2F;2}|\Sigma|^{1&#x2F;2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))<br>$$</p><p>其中\(|\Sigma|\)是定矩阵，在Octave中用det(sigma)计算。</p><p>下面我们来看看协方差矩阵是如何影响模型的：</p><p><img src="/../images/anomalyDetection_8.png" alt="anomalyDetection_8"></p><p>上图是5个不同的模型，从左往右依次分析：</p><ol><li>是一个一般的高斯分布模型</li><li>通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差</li><li>通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差</li><li>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性</li><li>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性</li></ol><h2 id="原高斯分布模型和多元高斯分布模型的比较："><a href="#原高斯分布模型和多元高斯分布模型的比较：" class="headerlink" title="原高斯分布模型和多元高斯分布模型的比较："></a>原高斯分布模型和多元高斯分布模型的比较：</h2><table><thead><tr><th>原高斯分布模型</th><th>多元高斯分布模型</th></tr></thead><tbody><tr><td>不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决</td><td>自动捕捉特征之间的相关性</td></tr><tr><td>计算代价低，能适应大规模的特征</td><td>计算代价较高；训练集较小时也同样适用</td></tr><tr><td>m较小的情况也可以使用</td><td>必须要有m&gt;n，不然的话协方差矩阵不可逆，通常需要m&gt;10n另外特征冗余也会导致协方差矩阵不可逆</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 19-异常检测(Anomaly Detection)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-18-exercise 7 summuary</title>
    <link href="https://shilei165.github.io/2022/09/06/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-18/"/>
    <id>https://shilei165.github.io/2022/09/06/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-18/</id>
    <published>2022-09-06T19:35:23.000Z</published>
    <updated>2022-09-14T15:31:16.350Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 18-exercise 7 summuary。</p><span id="more"></span><p><strong>Programming Exercise 7: K-means Clustering and Principal Component Analysis</strong></p><p>In this exercise, you will implement the K-means clustering algorithm and apply it to compress an image. </p><p>In the second part, you will use principal component analysis to find a low-dimensional representation of face images.</p><h1 id="ex7-m"><a href="#ex7-m" class="headerlink" title="ex7.m"></a>ex7.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class</span><br><span class="line">%  Exercise 7 | Principle Component Analysis and K-Means Clustering</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  exercise. You will need to complete the following functions:</span><br><span class="line">%</span><br><span class="line">%     pca.m</span><br><span class="line">%     projectData.m</span><br><span class="line">%     recoverData.m</span><br><span class="line">%     computeCentroids.m</span><br><span class="line">%     findClosestCentroids.m</span><br><span class="line">%     kMeansInitCentroids.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% ================= Part 1: Find Closest Centroids ====================</span><br><span class="line">%  To help you implement K-Means, we have divided the learning algorithm </span><br><span class="line">%  into two functions -- findClosestCentroids and computeCentroids. In this</span><br><span class="line">%  part, you should complete the code in the findClosestCentroids function. </span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;Finding closest centroids.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Load an example dataset that we will be using</span><br><span class="line">load(&#x27;ex7data2.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% Select an initial set of centroids</span><br><span class="line">K = 3; % 3 Centroids</span><br><span class="line">initial_centroids = [3 3; 6 2; 8 5];</span><br><span class="line"></span><br><span class="line">% Find the closest centroids for the examples using the</span><br><span class="line">% initial_centroids</span><br><span class="line">idx = findClosestCentroids(X, initial_centroids);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Closest centroids for the first 3 examples: \n&#x27;)</span><br><span class="line">fprintf(&#x27; %d&#x27;, idx(1:3));</span><br><span class="line">fprintf(&#x27;\n(the closest centroids should be 1, 3, 2 respectively)\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ===================== Part 2: Compute Means =========================</span><br><span class="line">%  After implementing the closest centroids function, you should now</span><br><span class="line">%  complete the computeCentroids function.</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nComputing centroids means.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Compute means based on the closest centroids found in the previous part.</span><br><span class="line">centroids = computeCentroids(X, idx, K);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Centroids computed after initial finding of closest centroids: \n&#x27;)</span><br><span class="line">fprintf(&#x27; %f %f \n&#x27; , centroids&#x27;);</span><br><span class="line">fprintf(&#x27;\n(the centroids should be\n&#x27;);</span><br><span class="line">fprintf(&#x27;   [ 2.428301 3.157924 ]\n&#x27;);</span><br><span class="line">fprintf(&#x27;   [ 5.813503 2.633656 ]\n&#x27;);</span><br><span class="line">fprintf(&#x27;   [ 7.119387 3.616684 ]\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =================== Part 3: K-Means Clustering ======================</span><br><span class="line">%  After you have completed the two functions computeCentroids and</span><br><span class="line">%  findClosestCentroids, you have all the necessary pieces to run the</span><br><span class="line">%  kMeans algorithm. In this part, you will run the K-Means algorithm on</span><br><span class="line">%  the example dataset we have provided. </span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nRunning K-Means clustering on example dataset.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Load an example dataset</span><br><span class="line">load(&#x27;ex7data2.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% Settings for running K-Means</span><br><span class="line">K = 3;</span><br><span class="line">max_iters = 10;</span><br><span class="line"></span><br><span class="line">% For consistency, here we set centroids to specific values</span><br><span class="line">% but in practice you want to generate them automatically, such as by</span><br><span class="line">% settings them to be random examples (as can be seen in</span><br><span class="line">% kMeansInitCentroids).</span><br><span class="line">initial_centroids = [3 3; 6 2; 8 5];</span><br><span class="line"></span><br><span class="line">% Run K-Means algorithm. The &#x27;true&#x27; at the end tells our function to plot</span><br><span class="line">% the progress of K-Means</span><br><span class="line">[centroids, idx] = runkMeans(X, initial_centroids, max_iters, true);</span><br><span class="line">fprintf(&#x27;\nK-Means Done.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============= Part 4: K-Means Clustering on Pixels ===============</span><br><span class="line">%  In this exercise, you will use K-Means to compress an image. To do this,</span><br><span class="line">%  you will first run K-Means on the colors of the pixels in the image and</span><br><span class="line">%  then you will map each pixel onto its closest centroid.</span><br><span class="line">%  </span><br><span class="line">%  You should now complete the code in kMeansInitCentroids.m</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nRunning K-Means clustering on pixels from an image.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Load an image of a bird</span><br><span class="line">A = double(imread(&#x27;bird_small.png&#x27;));</span><br><span class="line"></span><br><span class="line">% If imread does not work for you, you can try instead</span><br><span class="line">%   load (&#x27;bird_small.mat&#x27;);</span><br><span class="line"></span><br><span class="line">A = A / 255; % Divide by 255 so that all values are in the range 0 - 1</span><br><span class="line"></span><br><span class="line">% Size of the image</span><br><span class="line">img_size = size(A);</span><br><span class="line"></span><br><span class="line">% Reshape the image into an Nx3 matrix where N = number of pixels.</span><br><span class="line">% Each row will contain the Red, Green and Blue pixel values</span><br><span class="line">% This gives us our dataset matrix X that we will use K-Means on.</span><br><span class="line">X = reshape(A, img_size(1) * img_size(2), 3);</span><br><span class="line"></span><br><span class="line">% Run your K-Means algorithm on this data</span><br><span class="line">% You should try different values of K and max_iters here</span><br><span class="line">K = 16; </span><br><span class="line">max_iters = 10;</span><br><span class="line"></span><br><span class="line">% When using K-Means, it is important the initialize the centroids</span><br><span class="line">% randomly. </span><br><span class="line">% You should complete the code in kMeansInitCentroids.m before proceeding</span><br><span class="line">initial_centroids = kMeansInitCentroids(X, K);</span><br><span class="line"></span><br><span class="line">% Run K-Means</span><br><span class="line">[centroids, idx] = runkMeans(X, initial_centroids, max_iters);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================= Part 5: Image Compression ======================</span><br><span class="line">%  In this part of the exercise, you will use the clusters of K-Means to</span><br><span class="line">%  compress an image. To do this, we first find the closest clusters for</span><br><span class="line">%  each example. After that, we </span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nApplying K-Means to compress an image.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Find closest cluster members</span><br><span class="line">idx = findClosestCentroids(X, centroids);</span><br><span class="line"></span><br><span class="line">% Essentially, now we have represented the image X as in terms of the</span><br><span class="line">% indices in idx. </span><br><span class="line"></span><br><span class="line">% We can now recover the image from the indices (idx) by mapping each pixel</span><br><span class="line">% (specified by its index in idx) to the centroid value</span><br><span class="line">X_recovered = centroids(idx,:);</span><br><span class="line"></span><br><span class="line">% Reshape the recovered image into proper dimensions</span><br><span class="line">X_recovered = reshape(X_recovered, img_size(1), img_size(2), 3);</span><br><span class="line"></span><br><span class="line">% Display the original image </span><br><span class="line">subplot(1, 2, 1);</span><br><span class="line">imagesc(A); </span><br><span class="line">title(&#x27;Original&#x27;);</span><br><span class="line"></span><br><span class="line">% Display compressed image side by side</span><br><span class="line">subplot(1, 2, 2);</span><br><span class="line">imagesc(X_recovered)</span><br><span class="line">title(sprintf(&#x27;Compressed, with %d colors.&#x27;, K));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="findClosestCentroids-m"><a href="#findClosestCentroids-m" class="headerlink" title="findClosestCentroids.m"></a>findClosestCentroids.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">function idx = findClosestCentroids(X, centroids)</span><br><span class="line">%FINDCLOSESTCENTROIDS computes the centroid memberships for every example</span><br><span class="line">%   idx = FINDCLOSESTCENTROIDS (X, centroids) returns the closest centroids</span><br><span class="line">%   in idx for a dataset X where each row is a single example. idx = m x 1 </span><br><span class="line">%   vector of centroid assignments (i.e. each entry in range [1..K])</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Set K</span><br><span class="line">K = size(centroids, 1);</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">idx = zeros(size(X,1), 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Go over every example, find its closest centroid, and store</span><br><span class="line">%               the index inside idx at the appropriate location.</span><br><span class="line">%               Concretely, idx(i) should contain the index of the centroid</span><br><span class="line">%               closest to example i. Hence, it should be a value in the </span><br><span class="line">%               range 1..K</span><br><span class="line">%</span><br><span class="line">% Note: You can use a for-loop over the examples to compute this.</span><br><span class="line">%</span><br><span class="line">m = size(X,1);</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line">  for j = 1:K</span><br><span class="line">    dist(j) = sum((X(i,:)-centroids(j,:)).^2);</span><br><span class="line">  end</span><br><span class="line">  [d(i),idx(i)] = min(dist);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="computeCentroids-m"><a href="#computeCentroids-m" class="headerlink" title="computeCentroids.m"></a>computeCentroids.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">function centroids = computeCentroids(X, idx, K)</span><br><span class="line">%COMPUTECENTROIDS returns the new centroids by computing the means of the </span><br><span class="line">%data points assigned to each centroid.</span><br><span class="line">%   centroids = COMPUTECENTROIDS(X, idx, K) returns the new centroids by </span><br><span class="line">%   computing the means of the data points assigned to each centroid. It is</span><br><span class="line">%   given a dataset X where each row is a single data point, a vector</span><br><span class="line">%   idx of centroid assignments (i.e. each entry in range [1..K]) for each</span><br><span class="line">%   example, and K, the number of centroids. You should return a matrix</span><br><span class="line">%   centroids, where each row of centroids is the mean of the data points</span><br><span class="line">%   assigned to it.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Useful variables</span><br><span class="line">[m n] = size(X);</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">centroids = zeros(K, n);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Go over every centroid and compute mean of all points that</span><br><span class="line">%               belong to it. Concretely, the row vector centroids(i, :)</span><br><span class="line">%               should contain the mean of the data points assigned to</span><br><span class="line">%               centroid i.</span><br><span class="line">%</span><br><span class="line">% Note: You can use a for-loop over the centroids to compute this.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">Y = zeros(K,n);</span><br><span class="line">a = zeros(K,1);</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line">  for j = 1:K</span><br><span class="line">    if idx(i) == j</span><br><span class="line">      Y(j,:) = Y(j,:) + X(i,:);</span><br><span class="line">      a(j) = a(j)+1;</span><br><span class="line">    centroids(j,:) = Y(j,:)./a(j);</span><br><span class="line">    end</span><br><span class="line">  end</span><br><span class="line">end </span><br><span class="line"></span><br><span class="line">% Alternative solution</span><br><span class="line"></span><br><span class="line">% for i = 1:K</span><br><span class="line">%    X_i = X(idx == i, :);</span><br><span class="line">%    centroids(i,:) = mean(X_i, 1);</span><br><span class="line">% end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="runkMeans-m"><a href="#runkMeans-m" class="headerlink" title="runkMeans.m"></a>runkMeans.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">function [centroids, idx] = runkMeans(X, initial_centroids, ...</span><br><span class="line">                                      max_iters, plot_progress)</span><br><span class="line">%RUNKMEANS runs the K-Means algorithm on data matrix X, where each row of X</span><br><span class="line">%is a single example</span><br><span class="line">%   [centroids, idx] = RUNKMEANS(X, initial_centroids, max_iters, ...</span><br><span class="line">%   plot_progress) runs the K-Means algorithm on data matrix X, where each </span><br><span class="line">%   row of X is a single example. It uses initial_centroids used as the</span><br><span class="line">%   initial centroids. max_iters specifies the total number of interactions </span><br><span class="line">%   of K-Means to execute. plot_progress is a true/false flag that </span><br><span class="line">%   indicates if the function should also plot its progress as the </span><br><span class="line">%   learning happens. This is set to false by default. runkMeans returns </span><br><span class="line">%   centroids, a Kxn matrix of the computed centroids and idx, a m x 1 </span><br><span class="line">%   vector of centroid assignments (i.e. each entry in range [1..K])</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Set default value for plot progress</span><br><span class="line">if ~exist(&#x27;plot_progress&#x27;, &#x27;var&#x27;) || isempty(plot_progress)</span><br><span class="line">    plot_progress = false;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Plot the data if we are plotting progress</span><br><span class="line">if plot_progress</span><br><span class="line">    figure;</span><br><span class="line">    hold on;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Initialize values</span><br><span class="line">[m n] = size(X);</span><br><span class="line">K = size(initial_centroids, 1);</span><br><span class="line">centroids = initial_centroids;</span><br><span class="line">previous_centroids = centroids;</span><br><span class="line">idx = zeros(m, 1);</span><br><span class="line"></span><br><span class="line">% Run K-Means</span><br><span class="line">for i=1:max_iters</span><br><span class="line">    </span><br><span class="line">    % Output progress</span><br><span class="line">    fprintf(&#x27;K-Means iteration %d/%d...\n&#x27;, i, max_iters);</span><br><span class="line">    if exist(&#x27;OCTAVE_VERSION&#x27;)</span><br><span class="line">        fflush(stdout);</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    % For each example in X, assign it to the closest centroid</span><br><span class="line">    idx = findClosestCentroids(X, centroids);</span><br><span class="line">    </span><br><span class="line">    % Optionally, plot progress here</span><br><span class="line">    if plot_progress</span><br><span class="line">        plotProgresskMeans(X, centroids, previous_centroids, idx, K, i);</span><br><span class="line">        previous_centroids = centroids;</span><br><span class="line">        fprintf(&#x27;Press enter to continue.\n&#x27;);</span><br><span class="line">        pause;</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    % Given the memberships, compute new centroids</span><br><span class="line">    centroids = computeCentroids(X, idx, K);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Hold off if we are plotting progress</span><br><span class="line">if plot_progress</span><br><span class="line">    hold off;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="featureNormalize-m"><a href="#featureNormalize-m" class="headerlink" title="featureNormalize.m"></a>featureNormalize.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function [X_norm, mu, sigma] = featureNormalize(X)</span><br><span class="line">%FEATURENORMALIZE Normalizes the features in X </span><br><span class="line">%   FEATURENORMALIZE(X) returns a normalized version of X where</span><br><span class="line">%   the mean value of each feature is 0 and the standard deviation</span><br><span class="line">%   is 1. This is often a good preprocessing step to do when</span><br><span class="line">%   working with learning algorithms.</span><br><span class="line"></span><br><span class="line">mu = mean(X);</span><br><span class="line">X_norm = bsxfun(@minus, X, mu);</span><br><span class="line"></span><br><span class="line">sigma = std(X_norm);</span><br><span class="line">X_norm = bsxfun(@rdivide, X_norm, sigma);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% ============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="kMeansInitCentroids-m"><a href="#kMeansInitCentroids-m" class="headerlink" title="kMeansInitCentroids.m"></a>kMeansInitCentroids.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">function centroids = kMeansInitCentroids(X, K)</span><br><span class="line">%KMEANSINITCENTROIDS This function initializes K centroids that are to be </span><br><span class="line">%used in K-Means on the dataset X</span><br><span class="line">%   centroids = KMEANSINITCENTROIDS(X, K) returns K initial centroids to be</span><br><span class="line">%   used with the K-Means on the dataset X</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% You should return this values correctly</span><br><span class="line">centroids = zeros(K, size(X, 2));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: You should set centroids to randomly chosen examples from</span><br><span class="line">%               the dataset X</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% Initialize the centroids to be random examples</span><br><span class="line">% Randomly reorder the indices of examples</span><br><span class="line"></span><br><span class="line">randidx = randperm(size(X, 1));</span><br><span class="line"></span><br><span class="line">% Take the first K examples as centroids</span><br><span class="line"></span><br><span class="line">centroids = X(randidx(1:K), :);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="ex7-pca-m"><a href="#ex7-pca-m" class="headerlink" title="ex7_pca.m"></a>ex7_pca.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class</span><br><span class="line">%  Exercise 7 | Principle Component Analysis and K-Means Clustering</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  exercise. You will need to complete the following functions:</span><br><span class="line">%</span><br><span class="line">%     pca.m</span><br><span class="line">%     projectData.m</span><br><span class="line">%     recoverData.m</span><br><span class="line">%     computeCentroids.m</span><br><span class="line">%     findClosestCentroids.m</span><br><span class="line">%     kMeansInitCentroids.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% ================== Part 1: Load Example Dataset  ===================</span><br><span class="line">%  We start this exercise by using a small dataset that is easily to</span><br><span class="line">%  visualize</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;Visualizing example dataset for PCA.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  The following command loads the dataset. You should now have the </span><br><span class="line">%  variable X in your environment</span><br><span class="line">load (&#x27;ex7data1.mat&#x27;);</span><br><span class="line"></span><br><span class="line">%  Visualize the example dataset</span><br><span class="line">plot(X(:, 1), X(:, 2), &#x27;bo&#x27;);</span><br><span class="line">axis([0.5 6.5 2 8]); axis square;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =============== Part 2: Principal Component Analysis ===============</span><br><span class="line">%  You should now implement PCA, a dimension reduction technique. You</span><br><span class="line">%  should complete the code in pca.m</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nRunning PCA on example dataset.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Before running PCA, it is important to first normalize X</span><br><span class="line">[X_norm, mu, sigma] = featureNormalize(X);</span><br><span class="line"></span><br><span class="line">%  Run PCA</span><br><span class="line">[U, S] = pca(X_norm);</span><br><span class="line"></span><br><span class="line">%  Compute mu, the mean of the each feature</span><br><span class="line"></span><br><span class="line">%  Draw the eigenvectors centered at mean of data. These lines show the</span><br><span class="line">%  directions of maximum variations in the dataset.</span><br><span class="line">hold on;</span><br><span class="line">drawLine(mu, mu + 1.5 * S(1,1) * U(:,1)&#x27;, &#x27;-k&#x27;, &#x27;LineWidth&#x27;, 2);</span><br><span class="line">drawLine(mu, mu + 1.5 * S(2,2) * U(:,2)&#x27;, &#x27;-k&#x27;, &#x27;LineWidth&#x27;, 2);</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Top eigenvector: \n&#x27;);</span><br><span class="line">fprintf(&#x27; U(:,1) = %f %f \n&#x27;, U(1,1), U(2,1));</span><br><span class="line">fprintf(&#x27;\n(you should expect to see -0.707107 -0.707107)\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =================== Part 3: Dimension Reduction ===================</span><br><span class="line">%  You should now implement the projection step to map the data onto the </span><br><span class="line">%  first k eigenvectors. The code will then plot the data in this reduced </span><br><span class="line">%  dimensional space.  This will show you what the data looks like when </span><br><span class="line">%  using only the corresponding eigenvectors to reconstruct it.</span><br><span class="line">%</span><br><span class="line">%  You should complete the code in projectData.m</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nDimension reduction on example dataset.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Plot the normalized dataset (returned from pca)</span><br><span class="line">plot(X_norm(:, 1), X_norm(:, 2), &#x27;bo&#x27;);</span><br><span class="line">axis([-4 3 -4 3]); axis square</span><br><span class="line"></span><br><span class="line">%  Project the data onto K = 1 dimension</span><br><span class="line">K = 1;</span><br><span class="line">Z = projectData(X_norm, U, K);</span><br><span class="line">fprintf(&#x27;Projection of the first example: %f\n&#x27;, Z(1));</span><br><span class="line">fprintf(&#x27;\n(this value should be about 1.481274)\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">X_rec  = recoverData(Z, U, K);</span><br><span class="line">fprintf(&#x27;Approximation of the first example: %f %f\n&#x27;, X_rec(1, 1), X_rec(1, 2));</span><br><span class="line">fprintf(&#x27;\n(this value should be about  -1.047419 -1.047419)\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Draw lines connecting the projected points to the original points</span><br><span class="line">hold on;</span><br><span class="line">plot(X_rec(:, 1), X_rec(:, 2), &#x27;ro&#x27;);</span><br><span class="line">for i = 1:size(X_norm, 1)</span><br><span class="line">    drawLine(X_norm(i,:), X_rec(i,:), &#x27;--k&#x27;, &#x27;LineWidth&#x27;, 1);</span><br><span class="line">end</span><br><span class="line">hold off</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =============== Part 4: Loading and Visualizing Face Data =============</span><br><span class="line">%  We start the exercise by first loading and visualizing the dataset.</span><br><span class="line">%  The following code will load the dataset into your environment</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nLoading face dataset.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Load Face dataset</span><br><span class="line">load (&#x27;ex7faces.mat&#x27;)</span><br><span class="line"></span><br><span class="line">%  Display the first 100 faces in the dataset</span><br><span class="line">displayData(X(1:100, :));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =========== Part 5: PCA on Face Data: Eigenfaces  ===================</span><br><span class="line">%  Run PCA and visualize the eigenvectors which are in this case eigenfaces</span><br><span class="line">%  We display the first 36 eigenfaces.</span><br><span class="line">%</span><br><span class="line">fprintf([&#x27;\nRunning PCA on face dataset.\n&#x27; ...</span><br><span class="line">         &#x27;(this might take a minute or two ...)\n\n&#x27;]);</span><br><span class="line"></span><br><span class="line">%  Before running PCA, it is important to first normalize X by subtracting </span><br><span class="line">%  the mean value from each feature</span><br><span class="line">[X_norm, mu, sigma] = featureNormalize(X);</span><br><span class="line"></span><br><span class="line">%  Run PCA</span><br><span class="line">[U, S] = pca(X_norm);</span><br><span class="line"></span><br><span class="line">%  Visualize the top 36 eigenvectors found</span><br><span class="line">displayData(U(:, 1:36)&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============= Part 6: Dimension Reduction for Faces =================</span><br><span class="line">%  Project images to the eigen space using the top k eigenvectors </span><br><span class="line">%  If you are applying a machine learning algorithm </span><br><span class="line">fprintf(&#x27;\nDimension reduction for face dataset.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">K = 100;</span><br><span class="line">Z = projectData(X_norm, U, K);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;The projected data Z has a size of: &#x27;)</span><br><span class="line">fprintf(&#x27;%d &#x27;, size(Z));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\n\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ==== Part 7: Visualization of Faces after PCA Dimension Reduction ====</span><br><span class="line">%  Project images to the eigen space using the top K eigen vectors and </span><br><span class="line">%  visualize only using those K dimensions</span><br><span class="line">%  Compare to the original input, which is also displayed</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nVisualizing the projected (reduced dimension) faces.\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">K = 100;</span><br><span class="line">X_rec  = recoverData(Z, U, K);</span><br><span class="line"></span><br><span class="line">% Display normalized data</span><br><span class="line">subplot(1, 2, 1);</span><br><span class="line">displayData(X_norm(1:100,:));</span><br><span class="line">title(&#x27;Original faces&#x27;);</span><br><span class="line">axis square;</span><br><span class="line"></span><br><span class="line">% Display reconstructed data from only k eigenfaces</span><br><span class="line">subplot(1, 2, 2);</span><br><span class="line">displayData(X_rec(1:100,:));</span><br><span class="line">title(&#x27;Recovered faces&#x27;);</span><br><span class="line">axis square;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% === Part 8(a): Optional (ungraded) Exercise: PCA for Visualization ===</span><br><span class="line">%  One useful application of PCA is to use it to visualize high-dimensional</span><br><span class="line">%  data. In the last K-Means exercise you ran K-Means on 3-dimensional </span><br><span class="line">%  pixel colors of an image. We first visualize this output in 3D, and then</span><br><span class="line">%  apply PCA to obtain a visualization in 2D.</span><br><span class="line"></span><br><span class="line">close all; close all; clc</span><br><span class="line"></span><br><span class="line">% Reload the image from the previous exercise and run K-Means on it</span><br><span class="line">% For this to work, you need to complete the K-Means assignment first</span><br><span class="line">A = double(imread(&#x27;bird_small.png&#x27;));</span><br><span class="line"></span><br><span class="line">% If imread does not work for you, you can try instead</span><br><span class="line">%   load (&#x27;bird_small.mat&#x27;);</span><br><span class="line"></span><br><span class="line">A = A / 255;</span><br><span class="line">img_size = size(A);</span><br><span class="line">X = reshape(A, img_size(1) * img_size(2), 3);</span><br><span class="line">K = 16; </span><br><span class="line">max_iters = 10;</span><br><span class="line">initial_centroids = kMeansInitCentroids(X, K);</span><br><span class="line">[centroids, idx] = runkMeans(X, initial_centroids, max_iters);</span><br><span class="line"></span><br><span class="line">%  Sample 1000 random indexes (since working with all the data is</span><br><span class="line">%  too expensive. If you have a fast computer, you may increase this.</span><br><span class="line">sel = floor(rand(1000, 1) * size(X, 1)) + 1;</span><br><span class="line"></span><br><span class="line">%  Setup Color Palette</span><br><span class="line">palette = hsv(K);</span><br><span class="line">colors = palette(idx(sel), :);</span><br><span class="line"></span><br><span class="line">%  Visualize the data and centroid memberships in 3D</span><br><span class="line">figure;</span><br><span class="line">scatter3(X(sel, 1), X(sel, 2), X(sel, 3), 10, colors);</span><br><span class="line">title(&#x27;Pixel dataset plotted in 3D. Color shows centroid memberships&#x27;);</span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% === Part 8(b): Optional (ungraded) Exercise: PCA for Visualization ===</span><br><span class="line">% Use PCA to project this cloud to 2D for visualization</span><br><span class="line"></span><br><span class="line">% Subtract the mean to use PCA</span><br><span class="line">[X_norm, mu, sigma] = featureNormalize(X);</span><br><span class="line"></span><br><span class="line">% PCA and project the data to 2D</span><br><span class="line">[U, S] = pca(X_norm);</span><br><span class="line">Z = projectData(X_norm, U, 2);</span><br><span class="line"></span><br><span class="line">% Plot in 2D</span><br><span class="line">figure;</span><br><span class="line">plotDataPoints(Z(sel, :), idx(sel), K);</span><br><span class="line">title(&#x27;Pixel dataset plotted in 2D, using PCA for dimensionality reduction&#x27;);</span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br></pre></td></tr></table></figure><h1 id="displayData-m"><a href="#displayData-m" class="headerlink" title="displayData.m"></a>displayData.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">function [h, display_array] = displayData(X, example_width)</span><br><span class="line">%DISPLAYDATA Display 2D data in a nice grid</span><br><span class="line">%   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data</span><br><span class="line">%   stored in X in a nice grid. It returns the figure handle h and the </span><br><span class="line">%   displayed array if requested.</span><br><span class="line"></span><br><span class="line">% Set example_width automatically if not passed in</span><br><span class="line">if ~exist(&#x27;example_width&#x27;, &#x27;var&#x27;) || isempty(example_width) </span><br><span class="line">  example_width = round(sqrt(size(X, 2)));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Gray Image</span><br><span class="line">colormap(gray);</span><br><span class="line"></span><br><span class="line">% Compute rows, cols</span><br><span class="line">[m n] = size(X);</span><br><span class="line">example_height = (n / example_width);</span><br><span class="line"></span><br><span class="line">% Compute number of items to display</span><br><span class="line">display_rows = floor(sqrt(m));</span><br><span class="line">display_cols = ceil(m / display_rows);</span><br><span class="line"></span><br><span class="line">% Between images padding</span><br><span class="line">pad = 1;</span><br><span class="line"></span><br><span class="line">% Setup blank display</span><br><span class="line">display_array = - ones(pad + display_rows * (example_height + pad), ...</span><br><span class="line">                       pad + display_cols * (example_width + pad));</span><br><span class="line"></span><br><span class="line">% Copy each example into a patch on the display array</span><br><span class="line">curr_ex = 1;</span><br><span class="line">for j = 1:display_rows</span><br><span class="line">  for i = 1:display_cols</span><br><span class="line">    if curr_ex &gt; m, </span><br><span class="line">      break; </span><br><span class="line">    end</span><br><span class="line">    % Copy the patch</span><br><span class="line">    </span><br><span class="line">    % Get the max value of the patch</span><br><span class="line">    max_val = max(abs(X(curr_ex, :)));</span><br><span class="line">    display_array(pad + (j - 1) * (example_height + pad) + (1:example_height), ...</span><br><span class="line">                  pad + (i - 1) * (example_width + pad) + (1:example_width)) = ...</span><br><span class="line">            reshape(X(curr_ex, :), example_height, example_width) / max_val;</span><br><span class="line">    curr_ex = curr_ex + 1;</span><br><span class="line">  end</span><br><span class="line">  if curr_ex &gt; m, </span><br><span class="line">    break; </span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Display Image</span><br><span class="line">h = imagesc(display_array, [-1 1]);</span><br><span class="line"></span><br><span class="line">% Do not show axis</span><br><span class="line">axis image off</span><br><span class="line"></span><br><span class="line">drawnow;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="drawLine-m"><a href="#drawLine-m" class="headerlink" title="drawLine.m"></a>drawLine.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">function drawLine(p1, p2, varargin)</span><br><span class="line">%DRAWLINE Draws a line from point p1 to point p2</span><br><span class="line">%   DRAWLINE(p1, p2) Draws a line from point p1 to point p2 and holds the</span><br><span class="line">%   current figure</span><br><span class="line"></span><br><span class="line">plot([p1(1) p2(1)], [p1(2) p2(2)], varargin&#123;:&#125;);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="pca-m"><a href="#pca-m" class="headerlink" title="pca.m"></a>pca.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">function [U, S] = pca(X)</span><br><span class="line">%PCA Run principal component analysis on the dataset X</span><br><span class="line">%   [U, S, X] = pca(X) computes eigenvectors of the covariance matrix of X</span><br><span class="line">%   Returns the eigenvectors U, the eigenvalues (on diagonal) in S</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Useful values</span><br><span class="line">[m, n] = size(X);</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">U = zeros(n);</span><br><span class="line">S = zeros(n);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: You should first compute the covariance matrix. Then, you</span><br><span class="line">%               should use the &quot;svd&quot; function to compute the eigenvectors</span><br><span class="line">%               and eigenvalues of the covariance matrix. </span><br><span class="line">%</span><br><span class="line">% Note: When computing the covariance matrix, remember to divide by m (the</span><br><span class="line">%       number of examples).</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sigma = 1/m*X&#x27;*X;</span><br><span class="line">[U, S, V] = svd(sigma);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="plotDataPoints-m"><a href="#plotDataPoints-m" class="headerlink" title="plotDataPoints.m"></a>plotDataPoints.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">function plotDataPoints(X, idx, K)</span><br><span class="line">%PLOTDATAPOINTS plots data points in X, coloring them so that those with the same</span><br><span class="line">%index assignments in idx have the same color</span><br><span class="line">%   PLOTDATAPOINTS(X, idx, K) plots data points in X, coloring them so that those </span><br><span class="line">%   with the same index assignments in idx have the same color</span><br><span class="line"></span><br><span class="line">% Create palette</span><br><span class="line">palette = hsv(K + 1);</span><br><span class="line">colors = palette(idx, :);</span><br><span class="line"></span><br><span class="line">% Plot the data</span><br><span class="line">scatter(X(:,1), X(:,2), 15, colors);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="plotProgresskMeans-m"><a href="#plotProgresskMeans-m" class="headerlink" title="plotProgresskMeans.m"></a>plotProgresskMeans.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">function plotProgresskMeans(X, centroids, previous, idx, K, i)</span><br><span class="line">%PLOTPROGRESSKMEANS is a helper function that displays the progress of </span><br><span class="line">%k-Means as it is running. It is intended for use only with 2D data.</span><br><span class="line">%   PLOTPROGRESSKMEANS(X, centroids, previous, idx, K, i) plots the data</span><br><span class="line">%   points with colors assigned to each centroid. With the previous</span><br><span class="line">%   centroids, it also plots a line between the previous locations and</span><br><span class="line">%   current locations of the centroids.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Plot the examples</span><br><span class="line">plotDataPoints(X, idx, K);</span><br><span class="line"></span><br><span class="line">% Plot the centroids as black x&#x27;s</span><br><span class="line">plot(centroids(:,1), centroids(:,2), &#x27;x&#x27;, ...</span><br><span class="line">     &#x27;MarkerEdgeColor&#x27;,&#x27;k&#x27;, ...</span><br><span class="line">     &#x27;MarkerSize&#x27;, 10, &#x27;LineWidth&#x27;, 3);</span><br><span class="line"></span><br><span class="line">% Plot the history of the centroids with lines</span><br><span class="line">for j=1:size(centroids,1)</span><br><span class="line">    drawLine(centroids(j, :), previous(j, :));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Title</span><br><span class="line">title(sprintf(&#x27;Iteration number %d&#x27;, i))</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="projectData-m"><a href="#projectData-m" class="headerlink" title="projectData.m"></a>projectData.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">function Z = projectData(X, U, K)</span><br><span class="line">%PROJECTDATA Computes the reduced data representation when projecting only </span><br><span class="line">%on to the top k eigenvectors</span><br><span class="line">%   Z = projectData(X, U, K) computes the projection of </span><br><span class="line">%   the normalized inputs X into the reduced dimensional space spanned by</span><br><span class="line">%   the first K columns of U. It returns the projected examples in Z.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">Z = zeros(size(X, 1), K);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the projection of the data using only the top K </span><br><span class="line">%               eigenvectors in U (first K columns). </span><br><span class="line">%               For the i-th example X(i,:), the projection on to the k-th </span><br><span class="line">%               eigenvector is given as follows:</span><br><span class="line">%                    x = X(i, :)&#x27;;</span><br><span class="line">%                    projection_k = x&#x27; * U(:, k);</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">U_reduce = U(:,1:K);</span><br><span class="line"></span><br><span class="line">Z = X*U_reduce;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="recoverData-m"><a href="#recoverData-m" class="headerlink" title="recoverData.m"></a>recoverData.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">function X_rec = recoverData(Z, U, K)</span><br><span class="line">%RECOVERDATA Recovers an approximation of the original data when using the </span><br><span class="line">%projected data</span><br><span class="line">%   X_rec = RECOVERDATA(Z, U, K) recovers an approximation the </span><br><span class="line">%   original data that has been reduced to K dimensions. It returns the</span><br><span class="line">%   approximate reconstruction in X_rec.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">X_rec = zeros(size(Z, 1), size(U, 1));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the approximation of the data by projecting back</span><br><span class="line">%               onto the original space using the top K eigenvectors in U.</span><br><span class="line">%</span><br><span class="line">%               For the i-th example Z(i,:), the (approximate)</span><br><span class="line">%               recovered data for dimension j is given as follows:</span><br><span class="line">%                    v = Z(i, :)&#x27;;</span><br><span class="line">%                    recovered_j = v&#x27; * U(j, 1:K)&#x27;;</span><br><span class="line">%</span><br><span class="line">%               Notice that U(j, 1:K) is a row vector.</span><br><span class="line">%               </span><br><span class="line"></span><br><span class="line">U_reduce = U(:,1:K);</span><br><span class="line">X_rec = Z*U_reduce&#x27;;</span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 18-exercise 7 summuary。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-17-Unsupervised Learning</title>
    <link href="https://shilei165.github.io/2022/09/05/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-17/"/>
    <id>https://shilei165.github.io/2022/09/05/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-17/</id>
    <published>2022-09-05T19:39:23.000Z</published>
    <updated>2022-09-13T17:26:24.171Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 17-无监督学习(Unsupervised Learning)。</p><span id="more"></span><h1 id="聚类-Clustering"><a href="#聚类-Clustering" class="headerlink" title="聚类(Clustering)"></a>聚类(Clustering)</h1><h2 id="无监督学习-简介"><a href="#无监督学习-简介" class="headerlink" title="无监督学习-简介"></a>无监督学习-简介</h2><p>在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界。与此不同的是，在非监督学习中，我们的数据没有附带任何标签，我们拿到的数据就是这样的：</p><p><img src="/../images/clustering_1.png" alt="clustering_1"></p><p>在这个例子中，我们没有任何标签y。也就是说，在非监督学习中，我们需要将一系列无标签的训练数据，输入到一个算法中，然后我们告诉这个算法，去为我们找找这个数据的内在结构给定数据。</p><p>图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到我圈出的这些点集的算法，就被称为聚类算法。</p><p>那么聚类算法一般用来做什么呢？</p><p><img src="/../images/clustering_2.png" alt="clustering_2"></p><p>在这门课程的早些时候，我曾经列举过一些应用：比如市场分割。也许你在数据库中存储了许多客户的信息，而你希望将他们分成不同的客户群，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。社交网络分析：事实上有许多研究人员正在研究这样一些内容，他们关注一群人，关注社交网络，例如Facebook，Google+，或者是其他的一些信息，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群。另外，聚类算法可以用来更好的组织计算机集群，或者更好的管理数据中心。因为如果你知道数据中心中，哪些计算机经常协作工作。那么，你可以重新分配资源，重新布局网络。由此优化数据中心，优化数据通信。</p><h2 id="k-均值算法"><a href="#k-均值算法" class="headerlink" title="k-均值算法"></a>k-均值算法</h2><p>K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。</p><p>假设我们想要将数据聚类成K个组，其方法为:</p><ul><li>首先，选择K个随机的点，称为聚类中心（cluster centroids）</li><li>对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类</li><li>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置</li><li>重复步骤2-4直至中心点不再变化</li></ul><p>用\(\mu^1,\mu^2,…,\mu^k\)来表示聚类中心，用\(c^{(1)},c^{(2)},c^{(3)},…,c^{(m)}\)来存储与第i个实例数据最接近的聚类中心索引，K-均值的迭代算法代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Repeat &#123;</span><br><span class="line"></span><br><span class="line">for i = 1 to m</span><br><span class="line"></span><br><span class="line">    c(i) := index (form 1 to K) of cluster centroid closest to x(i)</span><br><span class="line"></span><br><span class="line">for k = 1 to K</span><br><span class="line"></span><br><span class="line">    μk := average (mean) of points assigned to cluster k</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>算法分为两个步骤:1)第一个for循环用于分配样例i所属的类；2)第二个for循环用于移动聚类中心，即：对于每一个类K，重新计算该类的质心。</p><p>K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用K-均值算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。</p><p><img src="/../images/clustering_3.png" alt="clustering_3"></p><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的代价函数（又称畸变函数 Distortion function）为：</p><p>$$<br>J(c^{(1)},…,c^{(m)},\mu^1,…,\mu^k) &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}||X^{(i)}-\mu_{c^{(i)}}||^2<br>$$</p><p>其中\(\mu_{c^{(i)}}\)代表与\(x^{(i)}\)最近的聚类中心点。</p><p>回顾刚才给出的: K-均值迭代算法，我们知道，第一个循环是用于减小\(c^{(i)}\)引起的代价，而第二个循环则是用于减小\(\mu_i\)引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。</p><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在运行K-均值算法的之前，我们首先要按照如下步骤随机初始化所有的聚类中心点：</p><ul><li>我们应该选择K &lt; m, 即聚类中心点的个数要小于所有训练集实例的数量</li><li>随机选择K个训练实例，然后令K个聚类中心分别与这K个训练实例相等</li></ul><p>K-均值的一个问题在于，它有可能会停留在一个局部最小值处。</p><p>为了解决这个问题，我们通常需要多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。这种方法在K较小的时候（2–10）还是可行的，但是如果K较大，这么做也可能不会有明显地改善。</p><h2 id="选择聚类数"><a href="#选择聚类数" class="headerlink" title="选择聚类数"></a>选择聚类数</h2><p>首先要明白一个概念，没有所谓最好的选择聚类数的方法。我们通常是根据不同的问题，人工进行选择不同的聚类数。选择的时候思考我们运用K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。</p><p>选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变K值，也就是聚类类别数目的总数。然后，计算成本函数J。</p><p><img src="/../images/clustering_4.png" alt="clustering_4"></p><p>我们可能会得到一条类似于这样一个像人的肘部的曲线。你会发现随着K值增加，它的陈本函数J会迅速下降，并且在3的时候达到一个肘点。在此之后，J值就下降的非常慢，那么我们就选K&#x3D;3。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p><p>当大部分的时候，我们会得到一条如右图所示的曲线，这时候，我们就不能从这条曲线得出合理的聚类数。这时候，我们就应该选择其他的方法来选择聚类数。</p><p>例如，我们的T-恤制造例子中，人们身高和体重数据是一些连续分布的点，我们就无法使用肘部曲线来进行分类。而是要根据我们的需求分类，比如，我们可以分成3个尺寸:S,M,L，也可以分成5个尺寸:XS,S,M,L,XL。这样的选择是建立在回答“聚类后我们制造的T-恤是否能较好地适合我们的客户”这个问题的基础上作出的。</p><h1 id="降维-Dimensionality-Reduction"><a href="#降维-Dimensionality-Reduction" class="headerlink" title="降维(Dimensionality Reduction)"></a>降维(Dimensionality Reduction)</h1><h2 id="动机一：数据压缩"><a href="#动机一：数据压缩" class="headerlink" title="动机一：数据压缩"></a>动机一：数据压缩</h2><p>有几个不同的的原因使你可能想要做降维。一是数据压缩，数据压缩不仅允许我们使用较少的计算机内存或磁盘空间，它也能够使我们的算法计算速度更快。</p><p>例如，我们未知两个的特征：\(x_1\)长度：用厘米表示；\(x_2\)：是用英寸表示同一物体的长度。我们就可以将数据减小到一个维度。</p><p><img src="/../images/dimensionReduction_1.png" alt="dimensionReduction_1"></p><p>类似的，在实际的工业运用中，有时可能有几个不同的工程团队，也许一个工程队给你二百个特征，第二工程队给你另外三百个的特征，第三工程队给你五百个特征，一千多个特征都在一起，它实际上会变得非常困难去追踪这些特征。这时，我们就需要将特征进行降维，从而更加容易的追踪它们。</p><p><strong>将数据从三维降至二维</strong>： 这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，迫使所有的数据都在同一个平面上，从而降至二维的特征向量。</p><p><img src="/../images/dimensionReduction_2.png" alt="dimensionReduction_2"></p><p>我们可以将这样的处理过程应用于任何维度的数据，从而降到任何想要的维度，例如将1000维的特征降至100维。</p><h2 id="动机二：数据可视化"><a href="#动机二：数据可视化" class="headerlink" title="动机二：数据可视化"></a>动机二：数据可视化</h2><p>在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案。这时候，降维便可以帮助我们。</p><p><img src="/../images/dimensionReduction_3.png" alt="dimensionReduction_3"></p><p>假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如GDP，人均GDP，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。</p><p><img src="/../images/dimensionReduction_4.png" alt="dimensionReduction_4"></p><p>这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。</p><h2 id="主成分分析问题-Principal-Component-Analysis-Problem-Formulation"><a href="#主成分分析问题-Principal-Component-Analysis-Problem-Formulation" class="headerlink" title="主成分分析问题(Principal Component Analysis Problem Formulation)"></a>主成分分析问题(Principal Component Analysis Problem Formulation)</h2><p>主成分分析(PCA)是最常见的降维算法。在PCA中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p><p>PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p><h2 id="主成分分析算法-Principal-Component-Analysis-Algorithm"><a href="#主成分分析算法-Principal-Component-Analysis-Algorithm" class="headerlink" title="主成分分析算法(Principal Component Analysis Algorithm)"></a>主成分分析算法(Principal Component Analysis Algorithm)</h2><p>PCA将n维数据降维到k维的步骤：</p><ul><li>均值归一化：我们需要计算出所有特征的均值，然后令\(x_j&#x3D;x_j-\mu_j\)。如果特征是在不同的数量级上，我们还需要将其除以标准差\(\sigma^2\)。</li><li>计算协方差矩阵(covariance matrix)。\(\sum &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{n}(x^{(i)})(x^{(i)})^T\)。</li><li>计算协方差矩阵(covariance matrix)\(\sum\)的特征向量(eigenvectors)：<ul><li>在Octave中可以利用奇异值分解(singular value decomposition)来求解</li><li>其函数为[U,S,V]&#x3D;svd(sigma)</li></ul></li></ul><p>对于一个\(n\times n\)维度的矩阵，上式中的U是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从n维降至k维，我们只需要从U中选取前k个向量，从而获得一个\(n\times k\)维度的矩阵，我们用\(U_{reduce}\)来表示，然后通过如下计算获得新的特征向量\(z^{(i)}\)：<br>$$<br>z^{(i)} &#x3D; U_{reduce}^T * x^{(i)}<br>$$</p><h2 id="选择主成分的数量-Choosing-the-Number-of-Principal-Components"><a href="#选择主成分的数量-Choosing-the-Number-of-Principal-Components" class="headerlink" title="选择主成分的数量(Choosing the Number of Principal Components)"></a>选择主成分的数量(Choosing the Number of Principal Components)</h2><p><strong>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的k值</strong>。</p><p>投射的平均均方误差：\(\frac{1}{m}\sum_{i&#x3D;1}^{m}||x^{(i)}-x_{approx}^{(i)}||^2\)</p><p>训练集的方差为:\(\frac{1}{m}\sum_{i&#x3D;1}^{m}||x^{(i)}||^2\)</p><p>如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了。</p><p>我们可以先令\(k&#x3D;1\)，然后进行主要成分分析，获得\(U_{reduce}\)和\(z\)，然后计算比例是否小于1%。如果不是的话再令\(k&#x3D;2\)，如此类推，直到找到可以使得比例小于1%的最小k值。</p><p>我们还可以使用Octave的svd函数([U, S, V] &#x3D; svd(sigma))中的返回的第二个参数S来快速的得到k值。</p><p>这里S是一个\(n\times n\)的矩阵，只有对角线上有值，而其它单元都是0。</p><p><img src="/../images/dimensionReduction_5.png" alt="dimensionReduction_5"></p><p>计算平均均方误差与训练集方差的比例的方法如下：<br>$$<br>\frac{\frac{1}{m}\sum_{i&#x3D;1}^{m}||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i&#x3D;1}^{m}||x^{(i)}||^2}&#x3D;1-\frac{\sum_{i&#x3D;1}{k}S_{ii}}{\sum_{i&#x3D;1}{m}S_{ii}} \leq 1%<br>$$</p><p>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：\(x_{approax}^{(i)}&#x3D;U_{reduce}z^{(i)}\)</p><h2 id="主成分分析法的应用建议-Advice-for-Applying-PCA"><a href="#主成分分析法的应用建议-Advice-for-Applying-PCA" class="headerlink" title="主成分分析法的应用建议(Advice for Applying PCA)"></a>主成分分析法的应用建议(Advice for Applying PCA)</h2><p>假使我们正在针对一张 100×100像素的图片进行某个计算机视觉的机器学习，即总共有10000 个特征。</p><ol><li>第一步是运用主要成分分析将数据压缩至1000个特征</li><li>然后对训练集运行学习算法</li><li>在预测时，采用之前学习而来的\(U_{reduce}\)将输入的特征\(x\)转换成特征向量\(z\)，然后再进行预测</li></ol><p><strong>错误的主要成分分析情况</strong>：</p><ul><li>用于减少过拟合–原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。</li><li>默认地将主要成分分析作为学习过程中的一部分–这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 17-无监督学习(Unsupervised Learning)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-16-exercise 6 summuary</title>
    <link href="https://shilei165.github.io/2022/09/02/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-16/"/>
    <id>https://shilei165.github.io/2022/09/02/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-16/</id>
    <published>2022-09-02T14:28:23.000Z</published>
    <updated>2022-09-14T15:30:24.884Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 16-exercise 6 summuary。</p><span id="more"></span><p><strong>Programming Exercise 6: Support Vector Machines</strong></p><p>In this exercise, you will be using support vector machines (SVMs) to build<br>a spam classiifier.</p><h1 id="ex6-m"><a href="#ex6-m" class="headerlink" title="ex6.m"></a>ex6.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class</span><br><span class="line">%  Exercise 6 | Support Vector Machines</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  exercise. You will need to complete the following functions:</span><br><span class="line">%</span><br><span class="line">%     gaussianKernel.m</span><br><span class="line">%     dataset3Params.m</span><br><span class="line">%     processEmail.m</span><br><span class="line">%     emailFeatures.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% =============== Part 1: Loading and Visualizing Data ================</span><br><span class="line">%  We start the exercise by first loading and visualizing the dataset. </span><br><span class="line">%  The following code will load the dataset into your environment and plot</span><br><span class="line">%  the data.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Loading and Visualizing Data ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">% Load from ex6data1: </span><br><span class="line">% You will have X, y in your environment</span><br><span class="line">load(&#x27;ex6data1.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% Plot training data</span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ==================== Part 2: Training Linear SVM ====================</span><br><span class="line">%  The following code will train a linear SVM on the dataset and plot the</span><br><span class="line">%  decision boundary learned.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load from ex6data1: </span><br><span class="line">% You will have X, y in your environment</span><br><span class="line">load(&#x27;ex6data1.mat&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nTraining Linear SVM ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">% You should try to change the C value below and see how the decision</span><br><span class="line">% boundary varies (e.g., try C = 1000)</span><br><span class="line">C = 1;</span><br><span class="line">model = svmTrain(X, y, C, @linearKernel, 1e-3, 20);</span><br><span class="line">visualizeBoundaryLinear(X, y, model);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =============== Part 3: Implementing Gaussian Kernel ===============</span><br><span class="line">%  You will now implement the Gaussian kernel to use</span><br><span class="line">%  with the SVM. You should complete the code in gaussianKernel.m</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nEvaluating the Gaussian Kernel ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">x1 = [1 2 1]; x2 = [0 4 -1]; sigma = 2;</span><br><span class="line">sim = gaussianKernel(x1, x2, sigma);</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;Gaussian Kernel between x1 = [1; 2; 1], x2 = [0; 4; -1], sigma = %f :&#x27; ...</span><br><span class="line">         &#x27;\n\t%f\n(for sigma = 2, this value should be about 0.324652)\n&#x27;], sigma, sim);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =============== Part 4: Visualizing Dataset 2 ================</span><br><span class="line">%  The following code will load the next dataset into your environment and </span><br><span class="line">%  plot the data. </span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Loading and Visualizing Data ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">% Load from ex6data2: </span><br><span class="line">% You will have X, y in your environment</span><br><span class="line">load(&#x27;ex6data2.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% Plot training data</span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ========== Part 5: Training SVM with RBF Kernel (Dataset 2) ==========</span><br><span class="line">%  After you have implemented the kernel, we can now use it to train the </span><br><span class="line">%  SVM classifier.</span><br><span class="line">% </span><br><span class="line">fprintf(&#x27;\nTraining SVM with RBF Kernel (this may take 1 to 2 minutes) ...\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Load from ex6data2: </span><br><span class="line">% You will have X, y in your environment</span><br><span class="line">load(&#x27;ex6data2.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% SVM Parameters</span><br><span class="line">C = 1; sigma = 0.1;</span><br><span class="line"></span><br><span class="line">% We set the tolerance and max_passes lower here so that the code will run</span><br><span class="line">% faster. However, in practice, you will want to run the training to</span><br><span class="line">% convergence.</span><br><span class="line">model= svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma)); </span><br><span class="line">visualizeBoundary(X, y, model);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =============== Part 6: Visualizing Dataset 3 ================</span><br><span class="line">%  The following code will load the next dataset into your environment and </span><br><span class="line">%  plot the data. </span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Loading and Visualizing Data ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">% Load from ex6data3: </span><br><span class="line">% You will have X, y in your environment</span><br><span class="line">load(&#x27;ex6data3.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% Plot training data</span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ========== Part 7: Training SVM with RBF Kernel (Dataset 3) ==========</span><br><span class="line"></span><br><span class="line">%  This is a different dataset that you can use to experiment with. Try</span><br><span class="line">%  different values of C and sigma here.</span><br><span class="line">% </span><br><span class="line"></span><br><span class="line">% Load from ex6data3: </span><br><span class="line">% You will have X, y in your environment</span><br><span class="line">load(&#x27;ex6data3.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% Try different SVM Parameters here</span><br><span class="line">[C, sigma] = dataset3Params(X, y, Xval, yval);</span><br><span class="line"></span><br><span class="line">% Train the SVM</span><br><span class="line">model= svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma));</span><br><span class="line">visualizeBoundary(X, y, model);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br></pre></td></tr></table></figure><h1 id="gaussianKernel-m"><a href="#gaussianKernel-m" class="headerlink" title="gaussianKernel.m"></a>gaussianKernel.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">function sim = gaussianKernel(x1, x2, sigma)</span><br><span class="line">%RBFKERNEL returns a radial basis function kernel between x1 and x2</span><br><span class="line">%   sim = gaussianKernel(x1, x2) returns a gaussian kernel between x1 and x2</span><br><span class="line">%   and returns the value in sim</span><br><span class="line"></span><br><span class="line">% Ensure that x1 and x2 are column vectors</span><br><span class="line">x1 = x1(:); x2 = x2(:);</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">sim = 0;</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Fill in this function to return the similarity between x1</span><br><span class="line">%               and x2 computed using a Gaussian kernel with bandwidth</span><br><span class="line">%               sigma</span><br><span class="line">%</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">sim = exp(-(x1-x2)&#x27;*(x1-x2)/(2*sigma^2));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line">    </span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="svmTrain-m"><a href="#svmTrain-m" class="headerlink" title="svmTrain.m"></a>svmTrain.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line">function [model] = svmTrain(X, Y, C, kernelFunction, ...</span><br><span class="line">                            tol, max_passes)</span><br><span class="line">%SVMTRAIN Trains an SVM classifier using a simplified version of the SMO </span><br><span class="line">%algorithm. </span><br><span class="line">%   [model] = SVMTRAIN(X, Y, C, kernelFunction, tol, max_passes) trains an</span><br><span class="line">%   SVM classifier and returns trained model. X is the matrix of training </span><br><span class="line">%   examples.  Each row is a training example, and the jth column holds the </span><br><span class="line">%   jth feature.  Y is a column matrix containing 1 for positive examples </span><br><span class="line">%   and 0 for negative examples.  C is the standard SVM regularization </span><br><span class="line">%   parameter.  tol is a tolerance value used for determining equality of </span><br><span class="line">%   floating point numbers. max_passes controls the number of iterations</span><br><span class="line">%   over the dataset (without changes to alpha) before the algorithm quits.</span><br><span class="line">%</span><br><span class="line">% Note: This is a simplified version of the SMO algorithm for training</span><br><span class="line">%       SVMs. In practice, if you want to train an SVM classifier, we</span><br><span class="line">%       recommend using an optimized package such as:  </span><br><span class="line">%</span><br><span class="line">%           LIBSVM   (http://www.csie.ntu.edu.tw/~cjlin/libsvm/)</span><br><span class="line">%           SVMLight (http://svmlight.joachims.org/)</span><br><span class="line">%</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">if ~exist(&#x27;tol&#x27;, &#x27;var&#x27;) || isempty(tol)</span><br><span class="line">    tol = 1e-3;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">if ~exist(&#x27;max_passes&#x27;, &#x27;var&#x27;) || isempty(max_passes)</span><br><span class="line">    max_passes = 5;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Data parameters</span><br><span class="line">m = size(X, 1);</span><br><span class="line">n = size(X, 2);</span><br><span class="line"></span><br><span class="line">% Map 0 to -1</span><br><span class="line">Y(Y==0) = -1;</span><br><span class="line"></span><br><span class="line">% Variables</span><br><span class="line">alphas = zeros(m, 1);</span><br><span class="line">b = 0;</span><br><span class="line">E = zeros(m, 1);</span><br><span class="line">passes = 0;</span><br><span class="line">eta = 0;</span><br><span class="line">L = 0;</span><br><span class="line">H = 0;</span><br><span class="line"></span><br><span class="line">% Pre-compute the Kernel Matrix since our dataset is small</span><br><span class="line">% (in practice, optimized SVM packages that handle large datasets</span><br><span class="line">%  gracefully will _not_ do this)</span><br><span class="line">% </span><br><span class="line">% We have implemented optimized vectorized version of the Kernels here so</span><br><span class="line">% that the svm training will run faster.</span><br><span class="line">if strcmp(func2str(kernelFunction), &#x27;linearKernel&#x27;)</span><br><span class="line">    % Vectorized computation for the Linear Kernel</span><br><span class="line">    % This is equivalent to computing the kernel on every pair of examples</span><br><span class="line">    K = X*X&#x27;;</span><br><span class="line">elseif strfind(func2str(kernelFunction), &#x27;gaussianKernel&#x27;)</span><br><span class="line">    % Vectorized RBF Kernel</span><br><span class="line">    % This is equivalent to computing the kernel on every pair of examples</span><br><span class="line">    X2 = sum(X.^2, 2);</span><br><span class="line">    K = bsxfun(@plus, X2, bsxfun(@plus, X2&#x27;, - 2 * (X * X&#x27;)));</span><br><span class="line">    K = kernelFunction(1, 0) .^ K;</span><br><span class="line">else</span><br><span class="line">    % Pre-compute the Kernel Matrix</span><br><span class="line">    % The following can be slow due to the lack of vectorization</span><br><span class="line">    K = zeros(m);</span><br><span class="line">    for i = 1:m</span><br><span class="line">        for j = i:m</span><br><span class="line">             K(i,j) = kernelFunction(X(i,:)&#x27;, X(j,:)&#x27;);</span><br><span class="line">             K(j,i) = K(i,j); %the matrix is symmetric</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Train</span><br><span class="line">fprintf(&#x27;\nTraining ...&#x27;);</span><br><span class="line">dots = 12;</span><br><span class="line">while passes &lt; max_passes,</span><br><span class="line">            </span><br><span class="line">    num_changed_alphas = 0;</span><br><span class="line">    for i = 1:m,</span><br><span class="line">        </span><br><span class="line">        % Calculate Ei = f(x(i)) - y(i) using (2). </span><br><span class="line">        % E(i) = b + sum (X(i, :) * (repmat(alphas.*Y,1,n).*X)&#x27;) - Y(i);</span><br><span class="line">        E(i) = b + sum (alphas.*Y.*K(:,i)) - Y(i);</span><br><span class="line">        </span><br><span class="line">        if ((Y(i)*E(i) &lt; -tol &amp;&amp; alphas(i) &lt; C) || (Y(i)*E(i) &gt; tol &amp;&amp; alphas(i) &gt; 0)),</span><br><span class="line">            </span><br><span class="line">            % In practice, there are many heuristics one can use to select</span><br><span class="line">            % the i and j. In this simplified code, we select them randomly.</span><br><span class="line">            j = ceil(m * rand());</span><br><span class="line">            while j == i,  % Make sure i \neq j</span><br><span class="line">                j = ceil(m * rand());</span><br><span class="line">            end</span><br><span class="line"></span><br><span class="line">            % Calculate Ej = f(x(j)) - y(j) using (2).</span><br><span class="line">            E(j) = b + sum (alphas.*Y.*K(:,j)) - Y(j);</span><br><span class="line"></span><br><span class="line">            % Save old alphas</span><br><span class="line">            alpha_i_old = alphas(i);</span><br><span class="line">            alpha_j_old = alphas(j);</span><br><span class="line">            </span><br><span class="line">            % Compute L and H by (10) or (11). </span><br><span class="line">            if (Y(i) == Y(j)),</span><br><span class="line">                L = max(0, alphas(j) + alphas(i) - C);</span><br><span class="line">                H = min(C, alphas(j) + alphas(i));</span><br><span class="line">            else</span><br><span class="line">                L = max(0, alphas(j) - alphas(i));</span><br><span class="line">                H = min(C, C + alphas(j) - alphas(i));</span><br><span class="line">            end</span><br><span class="line">           </span><br><span class="line">            if (L == H),</span><br><span class="line">                % continue to next i. </span><br><span class="line">                continue;</span><br><span class="line">            end</span><br><span class="line"></span><br><span class="line">            % Compute eta by (14).</span><br><span class="line">            eta = 2 * K(i,j) - K(i,i) - K(j,j);</span><br><span class="line">            if (eta &gt;= 0),</span><br><span class="line">                % continue to next i. </span><br><span class="line">                continue;</span><br><span class="line">            end</span><br><span class="line">            </span><br><span class="line">            % Compute and clip new value for alpha j using (12) and (15).</span><br><span class="line">            alphas(j) = alphas(j) - (Y(j) * (E(i) - E(j))) / eta;</span><br><span class="line">            </span><br><span class="line">            % Clip</span><br><span class="line">            alphas(j) = min (H, alphas(j));</span><br><span class="line">            alphas(j) = max (L, alphas(j));</span><br><span class="line">            </span><br><span class="line">            % Check if change in alpha is significant</span><br><span class="line">            if (abs(alphas(j) - alpha_j_old) &lt; tol),</span><br><span class="line">                % continue to next i. </span><br><span class="line">                % replace anyway</span><br><span class="line">                alphas(j) = alpha_j_old;</span><br><span class="line">                continue;</span><br><span class="line">            end</span><br><span class="line">            </span><br><span class="line">            % Determine value for alpha i using (16). </span><br><span class="line">            alphas(i) = alphas(i) + Y(i)*Y(j)*(alpha_j_old - alphas(j));</span><br><span class="line">            </span><br><span class="line">            % Compute b1 and b2 using (17) and (18) respectively. </span><br><span class="line">            b1 = b - E(i) ...</span><br><span class="line">                 - Y(i) * (alphas(i) - alpha_i_old) *  K(i,j)&#x27; ...</span><br><span class="line">                 - Y(j) * (alphas(j) - alpha_j_old) *  K(i,j)&#x27;;</span><br><span class="line">            b2 = b - E(j) ...</span><br><span class="line">                 - Y(i) * (alphas(i) - alpha_i_old) *  K(i,j)&#x27; ...</span><br><span class="line">                 - Y(j) * (alphas(j) - alpha_j_old) *  K(j,j)&#x27;;</span><br><span class="line"></span><br><span class="line">            % Compute b by (19). </span><br><span class="line">            if (0 &lt; alphas(i) &amp;&amp; alphas(i) &lt; C),</span><br><span class="line">                b = b1;</span><br><span class="line">            elseif (0 &lt; alphas(j) &amp;&amp; alphas(j) &lt; C),</span><br><span class="line">                b = b2;</span><br><span class="line">            else</span><br><span class="line">                b = (b1+b2)/2;</span><br><span class="line">            end</span><br><span class="line"></span><br><span class="line">            num_changed_alphas = num_changed_alphas + 1;</span><br><span class="line"></span><br><span class="line">        end</span><br><span class="line">        </span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    if (num_changed_alphas == 0),</span><br><span class="line">        passes = passes + 1;</span><br><span class="line">    else</span><br><span class="line">        passes = 0;</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    fprintf(&#x27;.&#x27;);</span><br><span class="line">    dots = dots + 1;</span><br><span class="line">    if dots &gt; 78</span><br><span class="line">        dots = 0;</span><br><span class="line">        fprintf(&#x27;\n&#x27;);</span><br><span class="line">    end</span><br><span class="line">    if exist(&#x27;OCTAVE_VERSION&#x27;)</span><br><span class="line">        fflush(stdout);</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line">fprintf(&#x27; Done! \n\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Save the model</span><br><span class="line">idx = alphas &gt; 0;</span><br><span class="line">model.X= X(idx,:);</span><br><span class="line">model.y= Y(idx);</span><br><span class="line">model.kernelFunction = kernelFunction;</span><br><span class="line">model.b= b;</span><br><span class="line">model.alphas= alphas(idx);</span><br><span class="line">model.w = ((alphas.*Y)&#x27;*X)&#x27;;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="svmPredict-m"><a href="#svmPredict-m" class="headerlink" title="svmPredict.m"></a>svmPredict.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">function pred = svmPredict(model, X)</span><br><span class="line">%SVMPREDICT returns a vector of predictions using a trained SVM model</span><br><span class="line">%(svmTrain). </span><br><span class="line">%   pred = SVMPREDICT(model, X) returns a vector of predictions using a </span><br><span class="line">%   trained SVM model (svmTrain). X is a mxn matrix where there each </span><br><span class="line">%   example is a row. model is a svm model returned from svmTrain.</span><br><span class="line">%   predictions pred is a m x 1 column of predictions of &#123;0, 1&#125; values.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Check if we are getting a column vector, if so, then assume that we only</span><br><span class="line">% need to do prediction for a single example</span><br><span class="line">if (size(X, 2) == 1)</span><br><span class="line">    % Examples should be in rows</span><br><span class="line">    X = X&#x27;;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Dataset </span><br><span class="line">m = size(X, 1);</span><br><span class="line">p = zeros(m, 1);</span><br><span class="line">pred = zeros(m, 1);</span><br><span class="line"></span><br><span class="line">if strcmp(func2str(model.kernelFunction), &#x27;linearKernel&#x27;)</span><br><span class="line">    % We can use the weights and bias directly if working with the </span><br><span class="line">    % linear kernel</span><br><span class="line">    p = X * model.w + model.b;</span><br><span class="line">elseif strfind(func2str(model.kernelFunction), &#x27;gaussianKernel&#x27;)</span><br><span class="line">    % Vectorized RBF Kernel</span><br><span class="line">    % This is equivalent to computing the kernel on every pair of examples</span><br><span class="line">    X1 = sum(X.^2, 2);</span><br><span class="line">    X2 = sum(model.X.^2, 2)&#x27;;</span><br><span class="line">    K = bsxfun(@plus, X1, bsxfun(@plus, X2, - 2 * X * model.X&#x27;));</span><br><span class="line">    K = model.kernelFunction(1, 0) .^ K;</span><br><span class="line">    K = bsxfun(@times, model.y&#x27;, K);</span><br><span class="line">    K = bsxfun(@times, model.alphas&#x27;, K);</span><br><span class="line">    p = sum(K, 2);</span><br><span class="line">else</span><br><span class="line">    % Other Non-linear kernel</span><br><span class="line">    for i = 1:m</span><br><span class="line">        prediction = 0;</span><br><span class="line">        for j = 1:size(model.X, 1)</span><br><span class="line">            prediction = prediction + ...</span><br><span class="line">                model.alphas(j) * model.y(j) * ...</span><br><span class="line">                model.kernelFunction(X(i,:)&#x27;, model.X(j,:)&#x27;);</span><br><span class="line">        end</span><br><span class="line">        p(i) = prediction + model.b;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Convert predictions into 0 / 1</span><br><span class="line">pred(p &gt;= 0) =  1;</span><br><span class="line">pred(p &lt;  0) =  0;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="visualizeBoundary-m"><a href="#visualizeBoundary-m" class="headerlink" title="visualizeBoundary.m"></a>visualizeBoundary.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">function pred = svmPredict(model, X)</span><br><span class="line">%SVMPREDICT returns a vector of predictions using a trained SVM model</span><br><span class="line">%(svmTrain). </span><br><span class="line">%   pred = SVMPREDICT(model, X) returns a vector of predictions using a </span><br><span class="line">%   trained SVM model (svmTrain). X is a mxn matrix where there each </span><br><span class="line">%   example is a row. model is a svm model returned from svmTrain.</span><br><span class="line">%   predictions pred is a m x 1 column of predictions of &#123;0, 1&#125; values.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Check if we are getting a column vector, if so, then assume that we only</span><br><span class="line">% need to do prediction for a single example</span><br><span class="line">if (size(X, 2) == 1)</span><br><span class="line">    % Examples should be in rows</span><br><span class="line">    X = X&#x27;;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Dataset </span><br><span class="line">m = size(X, 1);</span><br><span class="line">p = zeros(m, 1);</span><br><span class="line">pred = zeros(m, 1);</span><br><span class="line"></span><br><span class="line">if strcmp(func2str(model.kernelFunction), &#x27;linearKernel&#x27;)</span><br><span class="line">    % We can use the weights and bias directly if working with the </span><br><span class="line">    % linear kernel</span><br><span class="line">    p = X * model.w + model.b;</span><br><span class="line">elseif strfind(func2str(model.kernelFunction), &#x27;gaussianKernel&#x27;)</span><br><span class="line">    % Vectorized RBF Kernel</span><br><span class="line">    % This is equivalent to computing the kernel on every pair of examples</span><br><span class="line">    X1 = sum(X.^2, 2);</span><br><span class="line">    X2 = sum(model.X.^2, 2)&#x27;;</span><br><span class="line">    K = bsxfun(@plus, X1, bsxfun(@plus, X2, - 2 * X * model.X&#x27;));</span><br><span class="line">    K = model.kernelFunction(1, 0) .^ K;</span><br><span class="line">    K = bsxfun(@times, model.y&#x27;, K);</span><br><span class="line">    K = bsxfun(@times, model.alphas&#x27;, K);</span><br><span class="line">    p = sum(K, 2);</span><br><span class="line">else</span><br><span class="line">    % Other Non-linear kernel</span><br><span class="line">    for i = 1:m</span><br><span class="line">        prediction = 0;</span><br><span class="line">        for j = 1:size(model.X, 1)</span><br><span class="line">            prediction = prediction + ...</span><br><span class="line">                model.alphas(j) * model.y(j) * ...</span><br><span class="line">                model.kernelFunction(X(i,:)&#x27;, model.X(j,:)&#x27;);</span><br><span class="line">        end</span><br><span class="line">        p(i) = prediction + model.b;</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Convert predictions into 0 / 1</span><br><span class="line">pred(p &gt;= 0) =  1;</span><br><span class="line">pred(p &lt;  0) =  0;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="dataset3Params-m"><a href="#dataset3Params-m" class="headerlink" title="dataset3Params.m"></a>dataset3Params.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">function [C, sigma] = dataset3Params(X, y, Xval, yval)</span><br><span class="line">%DATASET3PARAMS returns your choice of C and sigma for Part 3 of the exercise</span><br><span class="line">%where you select the optimal (C, sigma) learning parameters to use for SVM</span><br><span class="line">%with RBF kernel</span><br><span class="line">%   [C, sigma] = DATASET3PARAMS(X, y, Xval, yval) returns your choice of C and </span><br><span class="line">%   sigma. You should complete this function to return the optimal C and </span><br><span class="line">%   sigma based on a cross-validation set.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">C = 1;</span><br><span class="line">sigma = 0.3;</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Fill in this function to return the optimal C and sigma</span><br><span class="line">%               learning parameters found using the cross validation set.</span><br><span class="line">%               You can use svmPredict to predict the labels on the cross</span><br><span class="line">%               validation set. For example, </span><br><span class="line">%                   predictions = svmPredict(model, Xval);</span><br><span class="line">%               will return the predictions on the cross validation set.</span><br><span class="line">%</span><br><span class="line">%  Note: You can compute the prediction error using </span><br><span class="line">%        mean(double(predictions ~= yval))</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">error_min = inf;</span><br><span class="line"></span><br><span class="line">for C = [0.01 0.03 0.1 0.3 1 3 10 30]</span><br><span class="line">  for sigma = [0.01 0.03 0.1 0.3 1 3 10 30]</span><br><span class="line">    model= svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma));</span><br><span class="line">    predictions = svmPredict(model, Xval);</span><br><span class="line">    error = mean(double(predictions ~= yval));</span><br><span class="line">    if(error &lt;= error_min)</span><br><span class="line">      C_final = C;</span><br><span class="line">      sigma_final = sigma;</span><br><span class="line">      error_min = error;</span><br><span class="line">    end </span><br><span class="line">  end </span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">C = C_final;</span><br><span class="line">sigma = sigma_final;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="ex6-spam-m"><a href="#ex6-spam-m" class="headerlink" title="ex6_spam.m"></a>ex6_spam.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class</span><br><span class="line">%  Exercise 6 | Spam Classification with SVMs</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  exercise. You will need to complete the following functions:</span><br><span class="line">%</span><br><span class="line">%     gaussianKernel.m</span><br><span class="line">%     dataset3Params.m</span><br><span class="line">%     processEmail.m</span><br><span class="line">%     emailFeatures.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% ==================== Part 1: Email Preprocessing ====================</span><br><span class="line">%  To use an SVM to classify emails into Spam v.s. Non-Spam, you first need</span><br><span class="line">%  to convert each email into a vector of features. In this part, you will</span><br><span class="line">%  implement the preprocessing steps for each email. You should</span><br><span class="line">%  complete the code in processEmail.m to produce a word indices vector</span><br><span class="line">%  for a given email.</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nPreprocessing sample email (emailSample1.txt)\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Extract Features</span><br><span class="line">file_contents = readFile(&#x27;emailSample1.txt&#x27;);</span><br><span class="line">word_indices  = processEmail(file_contents);</span><br><span class="line"></span><br><span class="line">% Print Stats</span><br><span class="line">fprintf(&#x27;Word Indices: \n&#x27;);</span><br><span class="line">fprintf(&#x27; %d&#x27;, word_indices);</span><br><span class="line">fprintf(&#x27;\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ==================== Part 2: Feature Extraction ====================</span><br><span class="line">%  Now, you will convert each email into a vector of features in R^n. </span><br><span class="line">%  You should complete the code in emailFeatures.m to produce a feature</span><br><span class="line">%  vector for a given email.</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nExtracting features from sample email (emailSample1.txt)\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Extract Features</span><br><span class="line">file_contents = readFile(&#x27;emailSample1.txt&#x27;);</span><br><span class="line">word_indices  = processEmail(file_contents);</span><br><span class="line">features      = emailFeatures(word_indices);</span><br><span class="line"></span><br><span class="line">% Print Stats</span><br><span class="line">fprintf(&#x27;Length of feature vector: %d\n&#x27;, length(features));</span><br><span class="line">fprintf(&#x27;Number of non-zero entries: %d\n&#x27;, sum(features &gt; 0));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =========== Part 3: Train Linear SVM for Spam Classification ========</span><br><span class="line">%  In this section, you will train a linear classifier to determine if an</span><br><span class="line">%  email is Spam or Not-Spam.</span><br><span class="line"></span><br><span class="line">% Load the Spam Email dataset</span><br><span class="line">% You will have X, y in your environment</span><br><span class="line">load(&#x27;spamTrain.mat&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nTraining Linear SVM (Spam Classification)\n&#x27;)</span><br><span class="line">fprintf(&#x27;(this may take 1 to 2 minutes) ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">C = 0.1;</span><br><span class="line">model = svmTrain(X, y, C, @linearKernel);</span><br><span class="line"></span><br><span class="line">p = svmPredict(model, X);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Training Accuracy: %f\n&#x27;, mean(double(p == y)) * 100);</span><br><span class="line"></span><br><span class="line">%% =================== Part 4: Test Spam Classification ================</span><br><span class="line">%  After training the classifier, we can evaluate it on a test set. We have</span><br><span class="line">%  included a test set in spamTest.mat</span><br><span class="line"></span><br><span class="line">% Load the test dataset</span><br><span class="line">% You will have Xtest, ytest in your environment</span><br><span class="line">load(&#x27;spamTest.mat&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nEvaluating the trained Linear SVM on a test set ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">p = svmPredict(model, Xtest);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Test Accuracy: %f\n&#x27;, mean(double(p == ytest)) * 100);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================= Part 5: Top Predictors of Spam ====================</span><br><span class="line">%  Since the model we are training is a linear SVM, we can inspect the</span><br><span class="line">%  weights learned by the model to understand better how it is determining</span><br><span class="line">%  whether an email is spam or not. The following code finds the words with</span><br><span class="line">%  the highest weights in the classifier. Informally, the classifier</span><br><span class="line">%  &#x27;thinks&#x27; that these words are the most likely indicators of spam.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Sort the weights and obtin the vocabulary list</span><br><span class="line">[weight, idx] = sort(model.w, &#x27;descend&#x27;);</span><br><span class="line">vocabList = getVocabList();</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nTop predictors of spam: \n&#x27;);</span><br><span class="line">for i = 1:15</span><br><span class="line">    fprintf(&#x27; %-15s (%f) \n&#x27;, vocabList&#123;idx(i)&#125;, weight(i));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\n\n&#x27;);</span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =================== Part 6: Try Your Own Emails =====================</span><br><span class="line">%  Now that you&#x27;ve trained the spam classifier, you can use it on your own</span><br><span class="line">%  emails! In the starter code, we have included spamSample1.txt,</span><br><span class="line">%  spamSample2.txt, emailSample1.txt and emailSample2.txt as examples. </span><br><span class="line">%  The following code reads in one of these emails and then uses your </span><br><span class="line">%  learned SVM classifier to determine whether the email is Spam or </span><br><span class="line">%  Not Spam</span><br><span class="line"></span><br><span class="line">% Set the file to be read in (change this to spamSample2.txt,</span><br><span class="line">% emailSample1.txt or emailSample2.txt to see different predictions on</span><br><span class="line">% different emails types). Try your own emails as well!</span><br><span class="line">filename = &#x27;spamSample1.txt&#x27;;</span><br><span class="line"></span><br><span class="line">% Read and predict</span><br><span class="line">file_contents = readFile(filename);</span><br><span class="line">word_indices  = processEmail(file_contents);</span><br><span class="line">x             = emailFeatures(word_indices);</span><br><span class="line">p = svmPredict(model, x);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProcessed %s\n\nSpam Classification: %d\n&#x27;, filename, p);</span><br><span class="line">fprintf(&#x27;(1 indicates spam, 0 indicates not spam)\n\n&#x27;);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="readFile-m"><a href="#readFile-m" class="headerlink" title="readFile.m"></a>readFile.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function file_contents = readFile(filename)</span><br><span class="line">%READFILE reads a file and returns its entire contents </span><br><span class="line">%   file_contents = READFILE(filename) reads a file and returns its entire</span><br><span class="line">%   contents in file_contents</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load File</span><br><span class="line">fid = fopen(filename);</span><br><span class="line">if fid</span><br><span class="line">    file_contents = fscanf(fid, &#x27;%c&#x27;, inf);</span><br><span class="line">    fclose(fid);</span><br><span class="line">else</span><br><span class="line">    file_contents = &#x27;&#x27;;</span><br><span class="line">    fprintf(&#x27;Unable to open %s\n&#x27;, filename);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="processEmail-m"><a href="#processEmail-m" class="headerlink" title="processEmail.m"></a>processEmail.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line">function word_indices = processEmail(email_contents)</span><br><span class="line">%PROCESSEMAIL preprocesses a the body of an email and</span><br><span class="line">%returns a list of word_indices </span><br><span class="line">%   word_indices = PROCESSEMAIL(email_contents) preprocesses </span><br><span class="line">%   the body of an email and returns a list of indices of the </span><br><span class="line">%   words contained in the email. </span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load Vocabulary</span><br><span class="line">vocabList = getVocabList();</span><br><span class="line"></span><br><span class="line">% Init return value</span><br><span class="line">word_indices = [];</span><br><span class="line"></span><br><span class="line">% ========================== Preprocess Email ===========================</span><br><span class="line"></span><br><span class="line">% Find the Headers ( \n\n and remove )</span><br><span class="line">% Uncomment the following lines if you are working with raw emails with the</span><br><span class="line">% full headers</span><br><span class="line"></span><br><span class="line">% hdrstart = strfind(email_contents, ([char(10) char(10)]));</span><br><span class="line">% email_contents = email_contents(hdrstart(1):end);</span><br><span class="line"></span><br><span class="line">% Lower case</span><br><span class="line">email_contents = lower(email_contents);</span><br><span class="line"></span><br><span class="line">% Strip all HTML</span><br><span class="line">% Looks for any expression that starts with &lt; and ends with &gt; and replace</span><br><span class="line">% and does not have any &lt; or &gt; in the tag it with a space</span><br><span class="line">email_contents = regexprep(email_contents, &#x27;&lt;[^&lt;&gt;]+&gt;&#x27;, &#x27; &#x27;);</span><br><span class="line"></span><br><span class="line">% Handle Numbers</span><br><span class="line">% Look for one or more characters between 0-9</span><br><span class="line">email_contents = regexprep(email_contents, &#x27;[0-9]+&#x27;, &#x27;number&#x27;);</span><br><span class="line"></span><br><span class="line">% Handle URLS</span><br><span class="line">% Look for strings starting with http:// or https://</span><br><span class="line">email_contents = regexprep(email_contents, ...</span><br><span class="line">                           &#x27;(http|https)://[^\s]*&#x27;, &#x27;httpaddr&#x27;);</span><br><span class="line"></span><br><span class="line">% Handle Email Addresses</span><br><span class="line">% Look for strings with @ in the middle</span><br><span class="line">email_contents = regexprep(email_contents, &#x27;[^\s]+@[^\s]+&#x27;, &#x27;emailaddr&#x27;);</span><br><span class="line"></span><br><span class="line">% Handle $ sign</span><br><span class="line">email_contents = regexprep(email_contents, &#x27;[$]+&#x27;, &#x27;dollar&#x27;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% ========================== Tokenize Email ===========================</span><br><span class="line"></span><br><span class="line">% Output the email to screen as well</span><br><span class="line">fprintf(&#x27;\n==== Processed Email ====\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Process file</span><br><span class="line">l = 0;</span><br><span class="line"></span><br><span class="line">while ~isempty(email_contents)</span><br><span class="line"></span><br><span class="line">    % Tokenize and also get rid of any punctuation</span><br><span class="line">    [str, email_contents] = ...</span><br><span class="line">       strtok(email_contents, ...</span><br><span class="line">              [&#x27; @$/#.-:&amp;*+=[]?!()&#123;&#125;,&#x27;&#x27;&quot;&gt;_&lt;;%&#x27; char(10) char(13)]);</span><br><span class="line">   </span><br><span class="line">    % Remove any non alphanumeric characters</span><br><span class="line">    str = regexprep(str, &#x27;[^a-zA-Z0-9]&#x27;, &#x27;&#x27;);</span><br><span class="line"></span><br><span class="line">    % Stem the word </span><br><span class="line">    % (the porterStemmer sometimes has issues, so we use a try catch block)</span><br><span class="line">    try str = porterStemmer(strtrim(str)); </span><br><span class="line">    catch str = &#x27;&#x27;; continue;</span><br><span class="line">    end;</span><br><span class="line"></span><br><span class="line">    % Skip the word if it is too short</span><br><span class="line">    if length(str) &lt; 1</span><br><span class="line">       continue;</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    % Look up the word in the dictionary and add to word_indices if</span><br><span class="line">    % found</span><br><span class="line">    % ====================== YOUR CODE HERE ======================</span><br><span class="line">    % Instructions: Fill in this function to add the index of str to</span><br><span class="line">    %               word_indices if it is in the vocabulary. At this point</span><br><span class="line">    %               of the code, you have a stemmed word from the email in</span><br><span class="line">    %               the variable str. You should look up str in the</span><br><span class="line">    %               vocabulary list (vocabList). If a match exists, you</span><br><span class="line">    %               should add the index of the word to the word_indices</span><br><span class="line">    %               vector. Concretely, if str = &#x27;action&#x27;, then you should</span><br><span class="line">    %               look up the vocabulary list to find where in vocabList</span><br><span class="line">    %               &#x27;action&#x27; appears. For example, if vocabList&#123;18&#125; =</span><br><span class="line">    %               &#x27;action&#x27;, then, you should add 18 to the word_indices </span><br><span class="line">    %               vector (e.g., word_indices = [word_indices ; 18]; ).</span><br><span class="line">    % </span><br><span class="line">    % Note: vocabList&#123;idx&#125; returns a the word with index idx in the</span><br><span class="line">    %       vocabulary list.</span><br><span class="line">    % </span><br><span class="line">    % Note: You can use strcmp(str1, str2) to compare two strings (str1 and</span><br><span class="line">    %       str2). It will return 1 only if the two strings are equivalent.</span><br><span class="line">    %</span><br><span class="line">    </span><br><span class="line">    for i = 1:length(vocabList)</span><br><span class="line">        if(strcmp(str, vocabList&#123;i&#125;))</span><br><span class="line">            word_indices = [word_indices; i];</span><br><span class="line">        end</span><br><span class="line">    end    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    % =============================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    % Print to screen, ensuring that the output lines are not too long</span><br><span class="line">    if (l + length(str) + 1) &gt; 78</span><br><span class="line">        fprintf(&#x27;\n&#x27;);</span><br><span class="line">        l = 0;</span><br><span class="line">    end</span><br><span class="line">    fprintf(&#x27;%s &#x27;, str);</span><br><span class="line">    l = l + length(str) + 1;</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Print footer</span><br><span class="line">fprintf(&#x27;\n\n=========================\n&#x27;);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="emailFeatures-m"><a href="#emailFeatures-m" class="headerlink" title="emailFeatures.m"></a>emailFeatures.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">function word_indices = processEmail(email_contents)</span><br><span class="line">%PROCESSEMAIL preprocesses a the body of an email and</span><br><span class="line">%returns a list of word_indices </span><br><span class="line">%   word_indices = PROCESSEMAIL(email_contents) preprocesses </span><br><span class="line">%   the body of an email and returns a list of indices of the </span><br><span class="line">%   words contained in the email. </span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load Vocabulary</span><br><span class="line">vocabList = getVocabList();</span><br><span class="line"></span><br><span class="line">% Init return value</span><br><span class="line">word_indices = [];</span><br><span class="line"></span><br><span class="line">% ========================== Preprocess Email ===========================</span><br><span class="line"></span><br><span class="line">% Find the Headers ( \n\n and remove )</span><br><span class="line">% Uncomment the following lines if you are working with raw emails with the</span><br><span class="line">% full headers</span><br><span class="line"></span><br><span class="line">% hdrstart = strfind(email_contents, ([char(10) char(10)]));</span><br><span class="line">% email_contents = email_contents(hdrstart(1):end);</span><br><span class="line"></span><br><span class="line">% Lower case</span><br><span class="line">email_contents = lower(email_contents);</span><br><span class="line"></span><br><span class="line">% Strip all HTML</span><br><span class="line">% Looks for any expression that starts with &lt; and ends with &gt; and replace</span><br><span class="line">% and does not have any &lt; or &gt; in the tag it with a space</span><br><span class="line">email_contents = regexprep(email_contents, &#x27;&lt;[^&lt;&gt;]+&gt;&#x27;, &#x27; &#x27;);</span><br><span class="line"></span><br><span class="line">% Handle Numbers</span><br><span class="line">% Look for one or more characters between 0-9</span><br><span class="line">email_contents = regexprep(email_contents, &#x27;[0-9]+&#x27;, &#x27;number&#x27;);</span><br><span class="line"></span><br><span class="line">% Handle URLS</span><br><span class="line">% Look for strings starting with http:// or https://</span><br><span class="line">email_contents = regexprep(email_contents, ...</span><br><span class="line">                           &#x27;(http|https)://[^\s]*&#x27;, &#x27;httpaddr&#x27;);</span><br><span class="line"></span><br><span class="line">% Handle Email Addresses</span><br><span class="line">% Look for strings with @ in the middle</span><br><span class="line">email_contents = regexprep(email_contents, &#x27;[^\s]+@[^\s]+&#x27;, &#x27;emailaddr&#x27;);</span><br><span class="line"></span><br><span class="line">% Handle $ sign</span><br><span class="line">email_contents = regexprep(email_contents, &#x27;[$]+&#x27;, &#x27;dollar&#x27;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% ========================== Tokenize Email ===========================</span><br><span class="line"></span><br><span class="line">% Output the email to screen as well</span><br><span class="line">fprintf(&#x27;\n==== Processed Email ====\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Process file</span><br><span class="line">l = 0;</span><br><span class="line"></span><br><span class="line">while ~isempty(email_contents)</span><br><span class="line"></span><br><span class="line">    % Tokenize and also get rid of any punctuation</span><br><span class="line">    [str, email_contents] = ...</span><br><span class="line">       strtok(email_contents, ...</span><br><span class="line">              [&#x27; @$/#.-:&amp;*+=[]?!()&#123;&#125;,&#x27;&#x27;&quot;&gt;_&lt;;%&#x27; char(10) char(13)]);</span><br><span class="line">   </span><br><span class="line">    % Remove any non alphanumeric characters</span><br><span class="line">    str = regexprep(str, &#x27;[^a-zA-Z0-9]&#x27;, &#x27;&#x27;);</span><br><span class="line"></span><br><span class="line">    % Stem the word </span><br><span class="line">    % (the porterStemmer sometimes has issues, so we use a try catch block)</span><br><span class="line">    try str = porterStemmer(strtrim(str)); </span><br><span class="line">    catch str = &#x27;&#x27;; continue;</span><br><span class="line">    end;</span><br><span class="line"></span><br><span class="line">    % Skip the word if it is too short</span><br><span class="line">    if length(str) &lt; 1</span><br><span class="line">       continue;</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    % Look up the word in the dictionary and add to word_indices if</span><br><span class="line">    % found</span><br><span class="line">    % ====================== YOUR CODE HERE ======================</span><br><span class="line">    % Instructions: Fill in this function to add the index of str to</span><br><span class="line">    %               word_indices if it is in the vocabulary. At this point</span><br><span class="line">    %               of the code, you have a stemmed word from the email in</span><br><span class="line">    %               the variable str. You should look up str in the</span><br><span class="line">    %               vocabulary list (vocabList). If a match exists, you</span><br><span class="line">    %               should add the index of the word to the word_indices</span><br><span class="line">    %               vector. Concretely, if str = &#x27;action&#x27;, then you should</span><br><span class="line">    %               look up the vocabulary list to find where in vocabList</span><br><span class="line">    %               &#x27;action&#x27; appears. For example, if vocabList&#123;18&#125; =</span><br><span class="line">    %               &#x27;action&#x27;, then, you should add 18 to the word_indices </span><br><span class="line">    %               vector (e.g., word_indices = [word_indices ; 18]; ).</span><br><span class="line">    % </span><br><span class="line">    % Note: vocabList&#123;idx&#125; returns a the word with index idx in the</span><br><span class="line">    %       vocabulary list.</span><br><span class="line">    % </span><br><span class="line">    % Note: You can use strcmp(str1, str2) to compare two strings (str1 and</span><br><span class="line">    %       str2). It will return 1 only if the two strings are equivalent.</span><br><span class="line">    %</span><br><span class="line">    </span><br><span class="line">    for i = 1:length(vocabList)</span><br><span class="line">        if(strcmp(str, vocabList&#123;i&#125;))</span><br><span class="line">            word_indices = [word_indices; i];</span><br><span class="line">        end</span><br><span class="line">    end    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    % =============================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    % Print to screen, ensuring that the output lines are not too long</span><br><span class="line">    if (l + length(str) + 1) &gt; 78</span><br><span class="line">        fprintf(&#x27;\n&#x27;);</span><br><span class="line">        l = 0;</span><br><span class="line">    end</span><br><span class="line">    fprintf(&#x27;%s &#x27;, str);</span><br><span class="line">    l = l + length(str) + 1;</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Print footer</span><br><span class="line">fprintf(&#x27;\n\n=========================\n&#x27;);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="getVocabList-m"><a href="#getVocabList-m" class="headerlink" title="getVocabList.m"></a>getVocabList.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">function vocabList = getVocabList()</span><br><span class="line">%GETVOCABLIST reads the fixed vocabulary list in vocab.txt and returns a</span><br><span class="line">%cell array of the words</span><br><span class="line">%   vocabList = GETVOCABLIST() reads the fixed vocabulary list in vocab.txt </span><br><span class="line">%   and returns a cell array of the words in vocabList.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% Read the fixed vocabulary list</span><br><span class="line">fid = fopen(&#x27;vocab.txt&#x27;);</span><br><span class="line"></span><br><span class="line">% Store all dictionary words in cell array vocab&#123;&#125;</span><br><span class="line">n = 1899;  % Total number of words in the dictionary</span><br><span class="line"></span><br><span class="line">% For ease of implementation, we use a struct to map the strings =&gt; integers</span><br><span class="line">% In practice, you&#x27;ll want to use some form of hashmap</span><br><span class="line">vocabList = cell(n, 1);</span><br><span class="line">for i = 1:n</span><br><span class="line">    % Word Index (can ignore since it will be = i)</span><br><span class="line">    fscanf(fid, &#x27;%d&#x27;, 1);</span><br><span class="line">    % Actual Word</span><br><span class="line">    vocabList&#123;i&#125; = fscanf(fid, &#x27;%s&#x27;, 1);</span><br><span class="line">end</span><br><span class="line">fclose(fid);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="linearKernel-m"><a href="#linearKernel-m" class="headerlink" title="linearKernel.m"></a>linearKernel.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">function sim = linearKernel(x1, x2)</span><br><span class="line">%LINEARKERNEL returns a linear kernel between x1 and x2</span><br><span class="line">%   sim = linearKernel(x1, x2) returns a linear kernel between x1 and x2</span><br><span class="line">%   and returns the value in sim</span><br><span class="line"></span><br><span class="line">% Ensure that x1 and x2 are column vectors</span><br><span class="line">x1 = x1(:); x2 = x2(:);</span><br><span class="line"></span><br><span class="line">% Compute the kernel</span><br><span class="line">sim = x1&#x27; * x2;  % dot product</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="visulizeBoundaryLinear-m"><a href="#visulizeBoundaryLinear-m" class="headerlink" title="visulizeBoundaryLinear.m"></a>visulizeBoundaryLinear.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">function visualizeBoundaryLinear(X, y, model)</span><br><span class="line">%VISUALIZEBOUNDARYLINEAR plots a linear decision boundary learned by the</span><br><span class="line">%SVM</span><br><span class="line">%   VISUALIZEBOUNDARYLINEAR(X, y, model) plots a linear decision boundary </span><br><span class="line">%   learned by the SVM and overlays the data on it</span><br><span class="line"></span><br><span class="line">w = model.w;</span><br><span class="line">b = model.b;</span><br><span class="line">xp = linspace(min(X(:,1)), max(X(:,1)), 100);</span><br><span class="line">yp = - (w(1)*xp + b)/w(2);</span><br><span class="line">plotData(X, y);</span><br><span class="line">hold on;</span><br><span class="line">plot(xp, yp, &#x27;-b&#x27;); </span><br><span class="line">hold off</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="plotData-m"><a href="#plotData-m" class="headerlink" title="plotData.m"></a>plotData.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function plotData(X, y)</span><br><span class="line">%PLOTDATA Plots the data points X and y into a new figure </span><br><span class="line">%   PLOTDATA(x,y) plots the data points with + for the positive examples</span><br><span class="line">%   and o for the negative examples. X is assumed to be a Mx2 matrix.</span><br><span class="line">%</span><br><span class="line">% Note: This was slightly modified such that it expects y = 1 or y = 0</span><br><span class="line"></span><br><span class="line">% Find Indices of Positive and Negative Examples</span><br><span class="line">pos = find(y == 1); neg = find(y == 0);</span><br><span class="line"></span><br><span class="line">% Plot Examples</span><br><span class="line">plot(X(pos, 1), X(pos, 2), &#x27;k+&#x27;,&#x27;LineWidth&#x27;, 1, &#x27;MarkerSize&#x27;, 7)</span><br><span class="line">hold on;</span><br><span class="line">plot(X(neg, 1), X(neg, 2), &#x27;ko&#x27;, &#x27;MarkerFaceColor&#x27;, &#x27;y&#x27;, &#x27;MarkerSize&#x27;, 7)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="porterStemmer-m"><a href="#porterStemmer-m" class="headerlink" title="porterStemmer.m"></a>porterStemmer.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function plotData(X, y)</span><br><span class="line">%PLOTDATA Plots the data points X and y into a new figure </span><br><span class="line">%   PLOTDATA(x,y) plots the data points with + for the positive examples</span><br><span class="line">%   and o for the negative examples. X is assumed to be a Mx2 matrix.</span><br><span class="line">%</span><br><span class="line">% Note: This was slightly modified such that it expects y = 1 or y = 0</span><br><span class="line"></span><br><span class="line">% Find Indices of Positive and Negative Examples</span><br><span class="line">pos = find(y == 1); neg = find(y == 0);</span><br><span class="line"></span><br><span class="line">% Plot Examples</span><br><span class="line">plot(X(pos, 1), X(pos, 2), &#x27;k+&#x27;,&#x27;LineWidth&#x27;, 1, &#x27;MarkerSize&#x27;, 7)</span><br><span class="line">hold on;</span><br><span class="line">plot(X(neg, 1), X(neg, 2), &#x27;ko&#x27;, &#x27;MarkerFaceColor&#x27;, &#x27;y&#x27;, &#x27;MarkerSize&#x27;, 7)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 16-exercise 6 summuary。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-15-Support Vector Machines</title>
    <link href="https://shilei165.github.io/2022/08/31/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-15/"/>
    <id>https://shilei165.github.io/2022/08/31/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-15/</id>
    <published>2022-08-31T19:35:23.000Z</published>
    <updated>2022-09-13T17:26:26.867Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 15-支持向量(Support Vector Machines)。</p><span id="more"></span><h1 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h1><p>与逻辑回归和神经网络相比，支持向量机(Support Vector Machine)，或者简称SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。它也是我们所介绍的最后一个监督学习算法。</p><p>为了描述支持向量机，事实上，我将会从逻辑回归开始展示我们如何一点一点修改来得到本质上的支持向量机。</p><p><img src="/../images/SVM_1.png" alt="SVM_1"></p><p>在逻辑回归中我们已经熟悉了这里的假设函数和右边的S型激励函数。简单说来，如果我们有一个y&#x3D;1的样本，如果我们想要正确的分类，就希望\(\theta^T x\)远大于0。相反的，如果我们有另一个样本，y&#x3D;0，我们就会希望函数输出值接近于0。对应的，\(\theta^Tx\)应当远小于0。</p><p>下面我们换一种视角来看逻辑回归函数：</p><p><img src="/../images/SVM_2.png" alt="SVM_2"></p><p>在总代价函数中，y&#x3D;1时，只有左半侧函数起作用；y&#x3D;0时，只有右半侧函数起作用。</p><p>近似的，如果我们用一个新的代价函数来代替，即这条从0点开始的水平直线，然后是一条斜线，像上图。那么，现在让我给这两个方程命名，左边的函数，我称之为\(cost_1(z)\)，同时，右边函数我称它为\(cost_0(z)\)。这里的下标是指在代价函数中，对应的y&#x3D;1和y&#x3D;0 的情况，拥有了这些定义后，现在，我们就开始构建支持向量机。</p><p>在逻辑回归中，我们的目标是找到下面这个方程的最小值：<br>$$<br>min_\theta\frac{1}{m}[\sum_{i&#x3D;1}^{m}y^{(i)}(-log h_\theta (x^{(i)}))+(1-y^{(i)})(-log(1-h_\theta (x^{(i)})))]+\frac{\lambda}{2m}\sum_{j&#x3D;1}^{n}\theta^2<br>$$</p><p>对与支持向量机而言，形式稍微有些不同：<br>$$<br>min_\theta C[\sum_{i&#x3D;1}^{m}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j&#x3D;1}^{n}\theta^2<br>$$</p><p>对比而言，两个方程有以下不同：</p><ul><li>去除了1&#x2F;m这一项。当然，这只是在不同函数中的不同习惯导致的，1&#x2F;m项并不会影响取得最优解。</li><li>使用\(C\times A+B\)来优化参数，而不是使用\(A+\lambda \times B\)。当然你也可以把这里的参数\(C\)考虑成\(\frac{1}{\lambda}\)，\(C\)同 \(\frac{1}{\lambda}\)所扮演的角色相同。</li><li>有别于逻辑回归输出的概率。在这里，支持向量机所做的是它来直接预测的值等于1，还是等于0。</li></ul><h1 id="大边界的直观理解"><a href="#大边界的直观理解" class="headerlink" title="大边界的直观理解"></a>大边界的直观理解</h1><p>人们有时将支持向量机看作是大间距分类器。在这一部分，我将介绍其中的含义，这有助于我们直观理解SVM模型的假设是什么样的。</p><p><img src="/../images/SVM_3.png" alt="SVM_3"></p><p>如果你有一个正样本（y&#x3D;1)，我们会希望在\(\theta^Tx \geq 1\)，而不是仅仅\(\theta^Tx \geq 0\)，来使得预测函数为1。反之，如果我们有一个负样本(y&#x3D;0)，我们希望(\theta^Tx \leq -1\)，而不是仅仅\(\theta^Tx \leq 0\)，来使得预测函数为0。那么我会就会用到SVM。</p><p>SVM相比于逻辑回归函数的以0为分界线来说，选取的边界会大很多，对于样品的分离也更加明显。C值的选择将会影响边界的大小。</p><p><img src="/../images/SVM_4.png" alt="SVM_4"></p><p>在SVM中如果我们将C设置的非常大，决策边界会从黑线变到了粉线，但是如果C设置的小一点，则最终会得到这条黑线。也就是说，当C不是非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。</p><h1 id="Kernels-I"><a href="#Kernels-I" class="headerlink" title="Kernels I"></a>Kernels I</h1><p>回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题：</p><p><img src="/../images/Kernels_1.png" alt="Kernels_1"></p><p>为了获得上图所示的判定边界，我们的模型可能是\(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+…\)的形式。</p><p>我们可以用一系列的新的特征来替换模型中的每一项。例如令：<br>$$<br>f_1&#x3D;x_1; f_2&#x3D;x_2; f_3&#x3D;x_1x_2; f_4&#x3D;x_1^2; f_5&#x3D;x_2^2…<br>$$<br>从而得到：<br>$$<br>h_\theta(x)&#x3D;\theta_1f_1+\theta_2f_2+…+\theta_nf_n<br>$$</p><p>然而，除了对原有的特征进行组合以外，有没有更好的方法来构造\(f_1,f_2,f_3…\)？我们可以用核函数(Kernel)来计算出新的特征。</p><p>给定一个训练样本\(x\)，我们利用\(x\)的各个特征与我们预先选定的地标(landmarks)\(l^{(1)},l^{(2)},l^{(3)}\)的近似程度来选取新的特征\(f_1,f_2,f_3\)。</p><p><img src="/../images/Kernels_2.png" alt="Kernels_2"></p><p>例如：\(f_1&#x3D;similarity(x,l^{(1)})&#x3D;e(-\frac{||x-l^{(1)}||}{2\sigma^2})\)。</p><p>其中\(||x-l^{(1)}||&#x3D;\sum_{j&#x3D;1}^n(x_j-l_j^{(1)})^2\)，为实例x中所有特征与地标\(l^{(1)}\)之间的距离的和。其中的\(f_1&#x3D;similarity(x,l^{(1)})\)就是核函数。具体而言，是一个高斯核函数(Gaussian Kernel)。</p><p>这些地标的作用是什么？如果一个训练样本\(x\)与地标\(l\)之间的距离近似于0，则新特征\(f\)近似于\(e^{-0}&#x3D;1\)，如果训练样本\(x\)与地标\(l\)之间距离较远，则\(f\)近似于\(e^{-(一个较大的数)}&#x3D;0\)。</p><p>假设我们的训练样本含有两个特征值\([x_1 x_2]\)，给定地标\(l^{(1)}\)与不同的\(\sigma\)值：</p><p><img src="/../images/Kernels_3.png" alt="Kernels_3"></p><p>图中水平面的坐标为\(x_1,x_2\)，而垂直坐标轴代表\(f\)。可以看出，只有当\(x\)与\(l^{(1)}\)重合时才具有最大值。随着的改变值改变的速率受到\(\sigma^2\)的控制。</p><p>在下图中，当样本处于洋红色的点位置处，因为其离\(l^{(1)}\)更近，但是离\(l^{(2)}\)和\(l^{(3)}\)较远，因此\(f_1\)接近1，而\(f_2\),\(f_3\)接近0。因此\(h_\theta(x)&#x3D;\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3&gt;0\)，因此预测\(y&#x3D;1\)。同理可以求出，对于离\(l^{(2)}\)较近的绿色点，也预测\(y&#x3D;1\)，但是对于蓝绿色的点，因为其离三个地标都较远，预测\(y&#x3D;0\)。</p><p><img src="/../images/Kernels_4.png" alt="Kernels_4"></p><h1 id="Kernels-II"><a href="#Kernels-II" class="headerlink" title="Kernels II"></a>Kernels II</h1><h2 id="如何选择地标？"><a href="#如何选择地标？" class="headerlink" title="如何选择地标？"></a>如何选择地标？</h2><p>我们通常是根据训练集的数量选择地标的数量，即如果训练集中有m个样本，则我们选取m个地标，并且令:\(l^{(1)}&#x3D;x^{(1)}, l^{(2)}&#x3D;x^{(2)}, l^{(3)}&#x3D;x^{(3)}…l^{(m)}&#x3D;x^{(m)}\)。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的。</p><h2 id="向量机中使用核函数"><a href="#向量机中使用核函数" class="headerlink" title="向量机中使用核函数"></a>向量机中使用核函数</h2><p>下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为：<br>给定\(x\)，计算新的特征\(f\)，当\(\theta^T gte 0\)时，预测\(y&#x3D;1\)，否则反之。</p><p>相应的修改代价函数为：<br>$$<br>min_\theta C\sum_{i&#x3D;1}^{m}y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})+\frac{1}{2}\sum_{j&#x3D;1}^{m}\theta_j^2<br>$$</p><p>在具体实施过程中，我们还需要对最后的正则化项进行些微调整。在计算时，我们用\(\theta^TM\theta\)代替\(\theta^T\theta\)，M其中是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。</p><p>理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用M来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。</p><p>另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(linear kernel)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而样本非常少的时候，可以采用这种不带核函数的支持向量机。</p><p>下面是支持向量机的两个参数\(C\)和\(\sigma\)的影响：</p><p>\(C&#x3D;\frac{1}{\lambda}\)</p><p>\(C\)较大时，相当于\(\lambda\)较小，会导致过拟合，高方差；</p><p>\(C\)较小时，相当于\(\lambda\)较大，会导致低拟合，高偏差；</p><p>\(\sigma\)较大时，可能会导致低方差，高偏差；</p><p>\(\sigma\)较小时，可能会导致低偏差，高方差。</p><h1 id="使用支持向量机"><a href="#使用支持向量机" class="headerlink" title="使用支持向量机"></a>使用支持向量机</h1><p>推荐使用SVM软件包来求解参数\(\theta\)，用的比较多的两个包为：liblinear和libsvm。除了高斯核函数之外，我们还有其他一些选择，例如：</p><ul><li>多项式核函数(Polynomial Kernel)</li><li>字符串核函数(String Kernel)</li><li>卡方核函数(chi-square Kernel)</li><li>直方图交集核函数(histogram intersection kernel)</li></ul><p>假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有k个类，则我们需要k个模型，以及k个参数向量。我们同样也可以训练k个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。</p><p>尽管你不去写你自己的SVM的优化软件，但是你也需要做几件事：</p><ul><li>提出参数C的选择。</li><li>选择内核参数或想要使用的相似函数。其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的SVM（支持向量机），这就意味这他使用了不带有核函数的SVM（支持向量机）。</li></ul><h2 id="逻辑回归模型-vs-支持向量机模型"><a href="#逻辑回归模型-vs-支持向量机模型" class="headerlink" title="逻辑回归模型 vs. 支持向量机模型"></a>逻辑回归模型 vs. 支持向量机模型</h2><p>从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？</p><p>下面是一些普遍使用的准则：<br>n为特征数，m为训练样本数。</p><p>(1)如果相较于m而言，n要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。</p><p>(2)如果n较小，而且m大小中等，例如n在 1-1000 之间，而m在10-10000之间，使用高斯核函数的支持向量机。</p><p>(3)如果n较小，而m较大，例如n在1-1000之间，而m大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。</p><p>值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 15-支持向量(Support Vector Machines)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-14-exercise 5 summary</title>
    <link href="https://shilei165.github.io/2022/08/29/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-14/"/>
    <id>https://shilei165.github.io/2022/08/29/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-14/</id>
    <published>2022-08-29T20:26:23.000Z</published>
    <updated>2022-09-14T15:26:49.217Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 14-exercise 5 summary。</p><span id="more"></span><p><strong>Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance</strong></p><p>In this exercise, you will implement regularized linear regression and use it to<br>study models with different bias-variance properties.</p><h1 id="ex5-m"><a href="#ex5-m" class="headerlink" title="ex5.m"></a>ex5.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class</span><br><span class="line">%  Exercise 5 | Regularized Linear Regression and Bias-Variance</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  exercise. You will need to complete the following functions:</span><br><span class="line">%</span><br><span class="line">%     linearRegCostFunction.m</span><br><span class="line">%     learningCurve.m</span><br><span class="line">%     validationCurve.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Loading and Visualizing Data =============</span><br><span class="line">%  We start the exercise by first loading and visualizing the dataset. </span><br><span class="line">%  The following code will load the dataset into your environment and plot</span><br><span class="line">%  the data.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load Training Data</span><br><span class="line">fprintf(&#x27;Loading and Visualizing Data ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">% Load from ex5data1: </span><br><span class="line">% You will have X, y, Xval, yval, Xtest, ytest in your environment</span><br><span class="line">load (&#x27;ex5data1.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% m = Number of examples</span><br><span class="line">m = size(X, 1);</span><br><span class="line"></span><br><span class="line">% Plot training data</span><br><span class="line">plot(X, y, &#x27;rx&#x27;, &#x27;MarkerSize&#x27;, 10, &#x27;LineWidth&#x27;, 1.5);</span><br><span class="line">xlabel(&#x27;Change in water level (x)&#x27;);</span><br><span class="line">ylabel(&#x27;Water flowing out of the dam (y)&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =========== Part 2: Regularized Linear Regression Cost =============</span><br><span class="line">%  You should now implement the cost function for regularized linear </span><br><span class="line">%  regression. </span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">theta = [1 ; 1];</span><br><span class="line">J = linearRegCostFunction([ones(m, 1) X], y, theta, 1);</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;Cost at theta = [1 ; 1]: %f &#x27;...</span><br><span class="line">         &#x27;\n(this value should be about 303.993192)\n&#x27;], J);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =========== Part 3: Regularized Linear Regression Gradient =============</span><br><span class="line">%  You should now implement the gradient for regularized linear </span><br><span class="line">%  regression.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">theta = [1 ; 1];</span><br><span class="line">[J, grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1);</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;Gradient at theta = [1 ; 1]:  [%f; %f] &#x27;...</span><br><span class="line">         &#x27;\n(this value should be about [-15.303016; 598.250744])\n&#x27;], ...</span><br><span class="line">         grad(1), grad(2));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =========== Part 4: Train Linear Regression =============</span><br><span class="line">%  Once you have implemented the cost and gradient correctly, the</span><br><span class="line">%  trainLinearReg function will use your cost function to train </span><br><span class="line">%  regularized linear regression.</span><br><span class="line">% </span><br><span class="line">%  Write Up Note: The data is non-linear, so this will not give a great </span><br><span class="line">%                 fit.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%  Train linear regression with lambda = 0</span><br><span class="line">lambda = 0;</span><br><span class="line">[theta] = trainLinearReg([ones(m, 1) X], y, lambda);</span><br><span class="line"></span><br><span class="line">%  Plot fit over the data</span><br><span class="line">plot(X, y, &#x27;rx&#x27;, &#x27;MarkerSize&#x27;, 10, &#x27;LineWidth&#x27;, 1.5);</span><br><span class="line">xlabel(&#x27;Change in water level (x)&#x27;);</span><br><span class="line">ylabel(&#x27;Water flowing out of the dam (y)&#x27;);</span><br><span class="line">hold on;</span><br><span class="line">plot(X, [ones(m, 1) X]*theta, &#x27;--&#x27;, &#x27;LineWidth&#x27;, 2)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =========== Part 5: Learning Curve for Linear Regression =============</span><br><span class="line">%  Next, you should implement the learningCurve function. </span><br><span class="line">%</span><br><span class="line">%  Write Up Note: Since the model is underfitting the data, we expect to</span><br><span class="line">%                 see a graph with &quot;high bias&quot; -- Figure 3 in ex5.pdf </span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">lambda = 0;</span><br><span class="line">[error_train, error_val] = ...</span><br><span class="line">    learningCurve([ones(m, 1) X], y, ...</span><br><span class="line">                  [ones(size(Xval, 1), 1) Xval], yval, ...</span><br><span class="line">                  lambda);</span><br><span class="line"></span><br><span class="line">plot(1:m, error_train, 1:m, error_val);</span><br><span class="line">title(&#x27;Learning curve for linear regression&#x27;)</span><br><span class="line">legend(&#x27;Train&#x27;, &#x27;Cross Validation&#x27;)</span><br><span class="line">xlabel(&#x27;Number of training examples&#x27;)</span><br><span class="line">ylabel(&#x27;Error&#x27;)</span><br><span class="line">axis([0 13 0 150])</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;# Training Examples\tTrain Error\tCross Validation Error\n&#x27;);</span><br><span class="line">for i = 1:m</span><br><span class="line">    fprintf(&#x27;  \t%d\t\t%f\t%f\n&#x27;, i, error_train(i), error_val(i));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =========== Part 6: Feature Mapping for Polynomial Regression =============</span><br><span class="line">%  One solution to this is to use polynomial regression. You should now</span><br><span class="line">%  complete polyFeatures to map each example into its powers</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">p = 8;</span><br><span class="line"></span><br><span class="line">% Map X onto Polynomial Features and Normalize</span><br><span class="line">X_poly = polyFeatures(X, p);</span><br><span class="line">[X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize</span><br><span class="line">X_poly = [ones(m, 1), X_poly];                   % Add Ones</span><br><span class="line"></span><br><span class="line">% Map X_poly_test and normalize (using mu and sigma)</span><br><span class="line">X_poly_test = polyFeatures(Xtest, p);</span><br><span class="line">X_poly_test = bsxfun(@minus, X_poly_test, mu);</span><br><span class="line">X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);</span><br><span class="line">X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones</span><br><span class="line"></span><br><span class="line">% Map X_poly_val and normalize (using mu and sigma)</span><br><span class="line">X_poly_val = polyFeatures(Xval, p);</span><br><span class="line">X_poly_val = bsxfun(@minus, X_poly_val, mu);</span><br><span class="line">X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);</span><br><span class="line">X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Normalized Training Example 1:\n&#x27;);</span><br><span class="line">fprintf(&#x27;  %f  \n&#x27;, X_poly(1, :));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =========== Part 7: Learning Curve for Polynomial Regression =============</span><br><span class="line">%  Now, you will get to experiment with polynomial regression with multiple</span><br><span class="line">%  values of lambda. The code below runs polynomial regression with </span><br><span class="line">%  lambda = 0. You should try running the code with different values of</span><br><span class="line">%  lambda to see how the fit and learning curve change.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">lambda = 0;</span><br><span class="line">[theta] = trainLinearReg(X_poly, y, lambda);</span><br><span class="line"></span><br><span class="line">% Plot training data and fit</span><br><span class="line">figure(1);</span><br><span class="line">plot(X, y, &#x27;rx&#x27;, &#x27;MarkerSize&#x27;, 10, &#x27;LineWidth&#x27;, 1.5);</span><br><span class="line">plotFit(min(X), max(X), mu, sigma, theta, p);</span><br><span class="line">xlabel(&#x27;Change in water level (x)&#x27;);</span><br><span class="line">ylabel(&#x27;Water flowing out of the dam (y)&#x27;);</span><br><span class="line">title (sprintf(&#x27;Polynomial Regression Fit (lambda = %f)&#x27;, lambda));</span><br><span class="line"></span><br><span class="line">figure(2);</span><br><span class="line">[error_train, error_val] = ...</span><br><span class="line">    learningCurve(X_poly, y, X_poly_val, yval, lambda);</span><br><span class="line">plot(1:m, error_train, 1:m, error_val);</span><br><span class="line"></span><br><span class="line">title(sprintf(&#x27;Polynomial Regression Learning Curve (lambda = %f)&#x27;, lambda));</span><br><span class="line">xlabel(&#x27;Number of training examples&#x27;)</span><br><span class="line">ylabel(&#x27;Error&#x27;)</span><br><span class="line">axis([0 13 0 100])</span><br><span class="line">legend(&#x27;Train&#x27;, &#x27;Cross Validation&#x27;)</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Polynomial Regression (lambda = %f)\n\n&#x27;, lambda);</span><br><span class="line">fprintf(&#x27;# Training Examples\tTrain Error\tCross Validation Error\n&#x27;);</span><br><span class="line">for i = 1:m</span><br><span class="line">    fprintf(&#x27;  \t%d\t\t%f\t%f\n&#x27;, i, error_train(i), error_val(i));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =========== Part 8: Validation for Selecting Lambda =============</span><br><span class="line">%  You will now implement validationCurve to test various values of </span><br><span class="line">%  lambda on a validation set. You will then use this to select the</span><br><span class="line">%  &quot;best&quot; lambda value.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">[lambda_vec, error_train, error_val] = ...</span><br><span class="line">    validationCurve(X_poly, y, X_poly_val, yval);</span><br><span class="line"></span><br><span class="line">close all;</span><br><span class="line">plot(lambda_vec, error_train, lambda_vec, error_val);</span><br><span class="line">legend(&#x27;Train&#x27;, &#x27;Cross Validation&#x27;);</span><br><span class="line">xlabel(&#x27;lambda&#x27;);</span><br><span class="line">ylabel(&#x27;Error&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;lambda\t\tTrain Error\tValidation Error\n&#x27;);</span><br><span class="line">for i = 1:length(lambda_vec)</span><br><span class="line">  fprintf(&#x27; %f\t%f\t%f\n&#x27;, ...</span><br><span class="line">            lambda_vec(i), error_train(i), error_val(i));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br></pre></td></tr></table></figure><h1 id="featureNormalize-m"><a href="#featureNormalize-m" class="headerlink" title="featureNormalize.m"></a>featureNormalize.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function [X_norm, mu, sigma] = featureNormalize(X)</span><br><span class="line">%FEATURENORMALIZE Normalizes the features in X </span><br><span class="line">%   FEATURENORMALIZE(X) returns a normalized version of X where</span><br><span class="line">%   the mean value of each feature is 0 and the standard deviation</span><br><span class="line">%   is 1. This is often a good preprocessing step to do when</span><br><span class="line">%   working with learning algorithms.</span><br><span class="line"></span><br><span class="line">mu = mean(X);</span><br><span class="line">X_norm = bsxfun(@minus, X, mu);</span><br><span class="line"></span><br><span class="line">sigma = std(X_norm);</span><br><span class="line">X_norm = bsxfun(@rdivide, X_norm, sigma);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% ============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="learningCurve-m"><a href="#learningCurve-m" class="headerlink" title="learningCurve.m"></a>learningCurve.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">function [error_train, error_val] = ...</span><br><span class="line">    learningCurve(X, y, Xval, yval, lambda)</span><br><span class="line">%LEARNINGCURVE Generates the train and cross validation set errors needed </span><br><span class="line">%to plot a learning curve</span><br><span class="line">%   [error_train, error_val] = ...</span><br><span class="line">%       LEARNINGCURVE(X, y, Xval, yval, lambda) returns the train and</span><br><span class="line">%       cross validation set errors for a learning curve. In particular, </span><br><span class="line">%       it returns two vectors of the same length - error_train and </span><br><span class="line">%       error_val. Then, error_train(i) contains the training error for</span><br><span class="line">%       i examples (and similarly for error_val(i)).</span><br><span class="line">%</span><br><span class="line">%   In this function, you will compute the train and test errors for</span><br><span class="line">%   dataset sizes from 1 up to m. In practice, when working with larger</span><br><span class="line">%   datasets, you might want to do this in larger intervals.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Number of training examples</span><br><span class="line">m = size(X, 1);</span><br><span class="line"></span><br><span class="line">% You need to return these values correctly</span><br><span class="line">error_train = zeros(m, 1);</span><br><span class="line">error_val   = zeros(m, 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Fill in this function to return training errors in </span><br><span class="line">%               error_train and the cross validation errors in error_val. </span><br><span class="line">%               i.e., error_train(i) and </span><br><span class="line">%               error_val(i) should give you the errors</span><br><span class="line">%               obtained after training on i examples.</span><br><span class="line">%</span><br><span class="line">% Note: You should evaluate the training error on the first i training</span><br><span class="line">%       examples (i.e., X(1:i, :) and y(1:i)).</span><br><span class="line">%</span><br><span class="line">%       For the cross-validation error, you should instead evaluate on</span><br><span class="line">%       the _entire_ cross validation set (Xval and yval).</span><br><span class="line">%</span><br><span class="line">% Note: If you are using your cost function (linearRegCostFunction)</span><br><span class="line">%       to compute the training and cross validation error, you should </span><br><span class="line">%       call the function with the lambda argument set to 0. </span><br><span class="line">%       Do note that you will still need to use lambda when running</span><br><span class="line">%       the training to obtain the theta parameters.</span><br><span class="line">%</span><br><span class="line">% Hint: You can loop over the examples with the following:</span><br><span class="line">%</span><br><span class="line">%       for i = 1:m</span><br><span class="line">%           % Compute train/cross validation errors using training examples </span><br><span class="line">%           % X(1:i, :) and y(1:i), storing the result in </span><br><span class="line">%           % error_train(i) and error_val(i)</span><br><span class="line">%           ....</span><br><span class="line">%           </span><br><span class="line">%       end</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% ---------------------- Sample Solution ----------------------</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line">    X_train = X(1:i, :);</span><br><span class="line">    y_train = y(1:i, :);</span><br><span class="line">    theta = trainLinearReg(X_train, y_train, lambda);</span><br><span class="line"></span><br><span class="line">    error_train(i) = linearRegCostFunction(X_train, y_train, theta, 0);</span><br><span class="line">    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% -------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="linearRegCostFunction-m"><a href="#linearRegCostFunction-m" class="headerlink" title="linearRegCostFunction.m"></a>linearRegCostFunction.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = linearRegCostFunction(X, y, theta, lambda)</span><br><span class="line">%LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear </span><br><span class="line">%regression with multiple variables</span><br><span class="line">%   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the </span><br><span class="line">%   cost of using theta as the parameter for linear regression to fit the </span><br><span class="line">%   data points in X and y. Returns the cost in J and the gradient in grad</span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">J = 0;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost and gradient of regularized linear </span><br><span class="line">%               regression for a particular choice of theta.</span><br><span class="line">%</span><br><span class="line">%               You should set J to the cost and grad to the gradient.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">h = X*theta;</span><br><span class="line"></span><br><span class="line">theta_reg = [0;theta(2:end, :);];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">J = 1/(2*m)*(h-y)&#x27;*(h-y) + lambda/(2*m)*(theta_reg&#x27;*theta_reg);</span><br><span class="line"></span><br><span class="line">grad = X&#x27;*(h -y)/m + lambda/m*(theta_reg);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">grad = grad(:);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="plotFit-m"><a href="#plotFit-m" class="headerlink" title="plotFit.m"></a>plotFit.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">function plotFit(min_x, max_x, mu, sigma, theta, p)</span><br><span class="line">%PLOTFIT Plots a learned polynomial regression fit over an existing figure.</span><br><span class="line">%Also works with linear regression.</span><br><span class="line">%   PLOTFIT(min_x, max_x, mu, sigma, theta, p) plots the learned polynomial</span><br><span class="line">%   fit with power p and feature normalization (mu, sigma).</span><br><span class="line"></span><br><span class="line">% Hold on to the current figure</span><br><span class="line">hold on;</span><br><span class="line"></span><br><span class="line">% We plot a range slightly bigger than the min and max values to get</span><br><span class="line">% an idea of how the fit will vary outside the range of the data points</span><br><span class="line">x = (min_x - 15: 0.05 : max_x + 25)&#x27;;</span><br><span class="line"></span><br><span class="line">% Map the X values </span><br><span class="line">X_poly = polyFeatures(x, p);</span><br><span class="line">X_poly = bsxfun(@minus, X_poly, mu);</span><br><span class="line">X_poly = bsxfun(@rdivide, X_poly, sigma);</span><br><span class="line"></span><br><span class="line">% Add ones</span><br><span class="line">X_poly = [ones(size(x, 1), 1) X_poly];</span><br><span class="line"></span><br><span class="line">% Plot</span><br><span class="line">plot(x, X_poly * theta, &#x27;--&#x27;, &#x27;LineWidth&#x27;, 2)</span><br><span class="line"></span><br><span class="line">% Hold off to the current figure</span><br><span class="line">hold off</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="polyFeatures-m"><a href="#polyFeatures-m" class="headerlink" title="polyFeatures.m"></a>polyFeatures.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">function [X_poly] = polyFeatures(X, p)</span><br><span class="line">%POLYFEATURES Maps X (1D vector) into the p-th power</span><br><span class="line">%   [X_poly] = POLYFEATURES(X, p) takes a data matrix X (size m x 1) and</span><br><span class="line">%   maps each example into its polynomial features where</span><br><span class="line">%   X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p];</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly.</span><br><span class="line">X_poly = zeros(numel(X), p);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Given a vector X, return a matrix X_poly where the p-th </span><br><span class="line">%               column of X contains the values of X to the p-th power.</span><br><span class="line">%</span><br><span class="line">% </span><br><span class="line"></span><br><span class="line">X_poly(:,1) = X;</span><br><span class="line"></span><br><span class="line">for i = 2:p</span><br><span class="line">  X_poly(:, i) = X.*X_poly(:,i-1);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="trainLinearReg-m"><a href="#trainLinearReg-m" class="headerlink" title="trainLinearReg.m"></a>trainLinearReg.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">function [theta] = trainLinearReg(X, y, lambda)</span><br><span class="line">%TRAINLINEARREG Trains linear regression given a dataset (X, y) and a</span><br><span class="line">%regularization parameter lambda</span><br><span class="line">%   [theta] = TRAINLINEARREG (X, y, lambda) trains linear regression using</span><br><span class="line">%   the dataset (X, y) and regularization parameter lambda. Returns the</span><br><span class="line">%   trained parameters theta.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Initialize Theta</span><br><span class="line">initial_theta = zeros(size(X, 2), 1); </span><br><span class="line"></span><br><span class="line">% Create &quot;short hand&quot; for the cost function to be minimized</span><br><span class="line">costFunction = @(t) linearRegCostFunction(X, y, t, lambda);</span><br><span class="line"></span><br><span class="line">% Now, costFunction is a function that takes in only one argument</span><br><span class="line">options = optimset(&#x27;MaxIter&#x27;, 200, &#x27;GradObj&#x27;, &#x27;on&#x27;);</span><br><span class="line"></span><br><span class="line">% Minimize using fmincg</span><br><span class="line">theta = fmincg(costFunction, initial_theta, options);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="validationCurve-m"><a href="#validationCurve-m" class="headerlink" title="validationCurve.m"></a>validationCurve.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">function [lambda_vec, error_train, error_val] = ...</span><br><span class="line">    validationCurve(X, y, Xval, yval)</span><br><span class="line">%VALIDATIONCURVE Generate the train and validation errors needed to</span><br><span class="line">%plot a validation curve that we can use to select lambda</span><br><span class="line">%   [lambda_vec, error_train, error_val] = ...</span><br><span class="line">%       VALIDATIONCURVE(X, y, Xval, yval) returns the train</span><br><span class="line">%       and validation errors (in error_train, error_val)</span><br><span class="line">%       for different values of lambda. You are given the training set (X,</span><br><span class="line">%       y) and validation set (Xval, yval).</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Selected values of lambda (you should not change this)</span><br><span class="line">lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]&#x27;;</span><br><span class="line"></span><br><span class="line">% You need to return these variables correctly.</span><br><span class="line">error_train = zeros(length(lambda_vec), 1);</span><br><span class="line">error_val = zeros(length(lambda_vec), 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Fill in this function to return training errors in </span><br><span class="line">%               error_train and the validation errors in error_val. The </span><br><span class="line">%               vector lambda_vec contains the different lambda parameters </span><br><span class="line">%               to use for each calculation of the errors, i.e, </span><br><span class="line">%               error_train(i), and error_val(i) should give </span><br><span class="line">%               you the errors obtained after training with </span><br><span class="line">%               lambda = lambda_vec(i)</span><br><span class="line">%</span><br><span class="line">% Note: You can loop over lambda_vec with the following:</span><br><span class="line">%</span><br><span class="line">%       for i = 1:length(lambda_vec)</span><br><span class="line">%           lambda = lambda_vec(i);</span><br><span class="line">%           % Compute train / val errors when training linear </span><br><span class="line">%           % regression with regularization parameter lambda</span><br><span class="line">%           % You should store the result in error_train(i)</span><br><span class="line">%           % and error_val(i)</span><br><span class="line">%           ....</span><br><span class="line">%           </span><br><span class="line">%       end</span><br><span class="line">%</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">for i = 1:length(lambda_vec)</span><br><span class="line">    lambda = lambda_vec(i);</span><br><span class="line">    theta = trainLinearReg(X, y, lambda);</span><br><span class="line"></span><br><span class="line">    error_train(i) = linearRegCostFunction(X, y, theta, 0);</span><br><span class="line">    error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 14-exercise 5 summary。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-13-Advice for Applying Machine Learning</title>
    <link href="https://shilei165.github.io/2022/08/29/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-13/"/>
    <id>https://shilei165.github.io/2022/08/29/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-13/</id>
    <published>2022-08-29T16:07:23.000Z</published>
    <updated>2022-09-14T13:09:52.991Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 13-应用机器学习的建议(Advice for Applying Machine Learning)。</p><span id="more"></span><h1 id="决定下一步做什么"><a href="#决定下一步做什么" class="headerlink" title="决定下一步做什么"></a>决定下一步做什么</h1><p>当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？</p><ul><li>获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</li><li>尝试减少特征的数量</li><li>尝试获得更多的特征</li><li>尝试增加多项式特征</li><li>尝试减少正则化程度\(\lambda\)</li><li>尝试增加正则化程度\(\lambda\)</li></ul><p>我这这里将会讨论怎样评估机器学习算法的性能，它们也被称为”机器学习诊断法”。“诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。</p><h2 id="评估一个假设"><a href="#评估一个假设" class="headerlink" title="评估一个假设"></a>评估一个假设</h2><p>你该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假设函数\(h(x)\)进行画图，然后观察图形趋势，但对于特征变量不止一个的这种一般情况，还有像有很多特征变量的问题，想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现。 因此，我们需要另一种方法来评估我们的假设函数过拟合检验。 为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。</p><p><img src="/../images/evaluateHypothesis.png" alt="evaluateHypothesis"></p><p>测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：</p><ol><li>对于线性回归模型，我们利用测试集数据(Test set)计算代价函数J。<br>$$<br>J_{test}(\theta) &#x3D; \frac{1}{2m_{test}}\sum_{i&#x3D;1}^{m_{test}}(h_{\theta}(x_{text}^{(i)})-y_{test}^{(i)})^2<br>$$</li><li>对于逻辑回归模型，我们也同样可以使用Test set来计算Cost function J:<br>$$<br>J_{test}(\theta) &#x3D; -\frac{1}{m_{test}}\sum_{i&#x3D;1}^{m_{test}}(y_{test}^{(i)}log h_\theta(x_{test}^{(i)})+(1-y_{test}^{(i)})log h_\theta(x_{test}^{(i)}))<br>$$</li></ol><h2 id="模型选择和训练、交叉验证、测试集"><a href="#模型选择和训练、交叉验证、测试集" class="headerlink" title="模型选择和训练、交叉验证、测试集"></a>模型选择和训练、交叉验证、测试集</h2><p>假设我们要在10个不同次数的二项式模型之间进行选择：</p><p><img src="/../images/modelSelection.jpg" alt="modelSelection"></p><p>显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。 即：使用60%的数据作为训练集，使用20%的数据作为交叉验证集，使用20%的数据作为测试集。</p><p><img src="/../images/evaluateHypothesis_2.png" alt="evaluateHypothesis_2"></p><p><strong>以上10个不同次数的二项式模型选择的方法为：</strong></p><ol><li>使用训练集训练出10个模型</li><li>用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</li><li>选取代价函数值最小的模型</li><li>用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）</li></ol><p><strong>Training error</strong></p><p>$$<br>J_{train}(\theta) &#x3D; \frac{1}{2m}\sum_{i&#x3D;1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2<br>$$</p><p><strong>Cross Validation error</strong></p><p>$$<br>J_{cv}(\theta) &#x3D; \frac{1}{2m_{cv}}\sum_{i&#x3D;1}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2<br>$$</p><p><strong>Test error</strong></p><p>$$<br>J_{test}(\theta) &#x3D; \frac{1}{2m_{test}}\sum_{i&#x3D;1}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^2<br>$$</p><h2 id="诊断偏差-bias-和方差-variance"><a href="#诊断偏差-bias-和方差-variance" class="headerlink" title="诊断偏差(bias)和方差(variance)"></a>诊断偏差(bias)和方差(variance)</h2><p>当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？</p><p><img src="/../images/biasVariance.jpg" alt="biasVariance"></p><p>我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：</p><p><img src="/../images/biasVariance_2.jpg" alt="biasVariance_2"></p><p>对于训练集，当d较小时，模型拟合程度更低，误差较大；随着d的增长，拟合程度提高，误差减小。 对于交叉验证集，当d较小时，模型拟合程度低，误差较大；但是随着d的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。</p><h2 id="正则化和偏差、方差"><a href="#正则化和偏差、方差" class="headerlink" title="正则化和偏差、方差"></a>正则化和偏差、方差</h2><p>在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。</p><p><img src="/../images/biasVariance_3.jpg" alt="biasVariance_3"></p><p>我们选择一系列的想要测试的\(\lambda\)值，通常是 0-10之间的呈现2倍关系的值（如：0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。</p><p><img src="/../images/regularizationBias.png" alt="regularizationBias"></p><p><strong>选择\(\lambda\)的方法为：</strong></p><ol><li>使用训练集训练出12个不同程度正则化的模型</li><li>用12个模型分别对交叉验证集计算的出交叉验证误差</li><li>选择得出交叉验证误差最小的模型</li><li>运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：</li><li>当\(\lambda\)较小时，训练集误差较小（过拟合）而交叉验证集误差较大; 随着  的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</li></ol><p><img src="/../images/regularizationBias_2.png" alt="regularizationBias_2"></p><h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集样本数量（m）的函数绘制的图表。 即，如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。</p><p><img src="/../images/learningCurve_1.png" alt="learningCurve_1"></p><h3 id="如何利用学习曲线识别高偏差-x2F-欠拟合"><a href="#如何利用学习曲线识别高偏差-x2F-欠拟合" class="headerlink" title="如何利用学习曲线识别高偏差&#x2F;欠拟合"></a>如何利用学习曲线识别<strong>高偏差&#x2F;欠拟合</strong></h3><p>作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观：</p><p><img src="/../images/learningCurve_2.png" alt="learningCurve_2"></p><p>也就是说在高偏差&#x2F;欠拟合的情况下，增加数据到训练集不一定能有帮助。 </p><h3 id="如何利用学习曲线识别高方差-x2F-过拟合"><a href="#如何利用学习曲线识别高方差-x2F-过拟合" class="headerlink" title="如何利用学习曲线识别高方差&#x2F;过拟合"></a>如何利用学习曲线识别<strong>高方差&#x2F;过拟合</strong></h3><p>假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。</p><p><img src="/../images/learningCurve_3.png" alt="learningCurve_3"></p><p>也就是说在高方差&#x2F;过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>回顾前面中提出的六种可选的下一步，让我们来看一看我们在什么情况下应该怎样选择：</p><ul><li>获得更多的训练样本 – 解决高方差（过拟合）</li><li>尝试减少特征的数量 – 解决高方差（过拟合）</li><li>尝试获得更多的特征 – 解决高偏差（欠拟合）</li><li>尝试增加多项式特征 – 解决高偏差（欠拟合）</li><li>尝试减少正则化程度\(\lambda\) – 解决高偏差（欠拟合）</li><li>尝试增加正则化程度\(\lambda\) – 解决高方差（过拟合）</li></ul><h3 id="神经网络和过拟合"><a href="#神经网络和过拟合" class="headerlink" title="神经网络和过拟合"></a>神经网络和过拟合</h3><p>使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。</p><p><strong>通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。</strong> 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。</p><p><img src="/../images/neuralNetworkOverfitting.png" alt="neuralNetworkOverfitting"></p><h1 id="机器学习系统的设计-Machine-Learning-System-Design"><a href="#机器学习系统的设计-Machine-Learning-System-Design" class="headerlink" title="机器学习系统的设计(Machine Learning System Design)"></a>机器学习系统的设计(Machine Learning System Design)</h1><h2 id="首先要做什么：邮件分类的例子"><a href="#首先要做什么：邮件分类的例子" class="headerlink" title="首先要做什么：邮件分类的例子"></a>首先要做什么：邮件分类的例子</h2><p>以一个垃圾邮件分类器算法为例进行讨论。 为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量\(x\)。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。</p><p>为了构建这个分类器算法，我们可以：</p><ul><li>收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本</li><li>基于邮件的路由信息开发一系列复杂的特征</li><li>基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理</li><li>为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法</li></ul><p>在上面这些选项中，非常难决定应该在哪一项上花费时间和精力，作出明智的选择，比随着感觉走要更好。随后讲解的误差分析，会告诉你怎样用一个更加系统性的方法，从一堆不同的方法中，选取合适的那一个。因此，你更有可能选择一个真正的好方法，能让你花上几天几周，甚至是几个月去进行深入的研究。</p><h2 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h2><p>误差分析（Error Analysis）会帮助你更系统地做出决定。如果你准备研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它。 然后通过交叉验证来检验数据。一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。</p><p>这么做的原因是：这在你刚接触机器学习问题时是一个很好的方法，你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。</p><p>除了画出学习曲线之外，一件非常有用的事是误差分析，我的意思是说：当我们在构造垃圾邮件分类器时，我会看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。</p><p>构建一个学习算法的推荐方法为：</p><ol><li>从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法</li><li>绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择</li><li>进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势</li></ol><h2 id="类偏斜的误差度量"><a href="#类偏斜的误差度量" class="headerlink" title="类偏斜的误差度量"></a>类偏斜的误差度量</h2><p>在前面的学习中，我们提到了误差分析，以及设定误差度量值的重要性。那就是，设定某个实数来评估你的学习算法，并衡量它的表现。这里，有一件重要的事情要注意，就是使用一个合适的误差度量值，这有时会对于你的学习算法造成非常微妙的影响，这件重要的事情就是偏斜类（skewed classes）的问题。</p><p>类偏斜情况表现为我们的训练集中有非常多的同一种类的样本，只有很少或没有其他类的样本。 例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。 </p><p>我们将算法预测的结果分成四种情况：</p><p><img src="/../images/precisionRecall.PNG" alt="precisionRecall"></p><p>$$<br>Precision(查准率) &#x3D; \frac{True\ positive}{Predicted\ positive}&#x3D;\frac{True\ positive}{True\ positive+False\ positive}<br>$$</p><p>$$<br>Recall(查全率) &#x3D; \frac{True\ positive}{Actual\ positive}&#x3D;\frac{True\ positive}{True\ positive+False\ negative}<br>$$</p><h2 id="查准率和查全率之间的权衡"><a href="#查准率和查全率之间的权衡" class="headerlink" title="查准率和查全率之间的权衡"></a>查准率和查全率之间的权衡</h2><p>之前我们谈到查准率和查全率，作为遇到偏斜类问题的评估度量值。在很多应用中，我们希望能够保证查准率和查全率的相对平衡。 这里将告诉你应该怎么做，同时也向你展示一些查准率和查全率作为算法评估度量值的更有效的方式。继续沿用刚才预测肿瘤性质的例子。</p><p>查准率(Precision)&#x3D;TP&#x2F;(TP+FP) 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p><p>查全率(Recall)&#x3D;TP&#x2F;(TP+FN)例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p><p>如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。 如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。 我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同：</p><p><img src="/../images/precisionRecall_2.PNG" alt="precisionRecall_2"></p><p>如何使用一个参数来判断是否算法更加合理呢？我们可以使用查准率和查全率的平均值，但很多时候并不能给出合理的预期。另一种更加合理的计算方法是计算他们的F1值，起计算公式为：<br>$$<br>2\frac{PR}{P+R}<br>$$</p><p><img src="/../images/precisionRecall_3.PNG" alt="precisionRecall_3"></p><h2 id="机器学习的数据"><a href="#机器学习的数据" class="headerlink" title="机器学习的数据"></a>机器学习的数据</h2><p>在之前的一些笔记中，曾经提到我们不要盲目地花大量的时间来收集大量的数据。但事实证明，在一定条件下（后面会提到这些条件是什么），得到大量的数据并在某种类型的学习算法中进行训练，可以是一种有效的方法来获得一个具有良好性能的学习算法。</p><p>我们先来看Michele Banko和Eric Brill的研究结果：</p><p><img src="/../images/dataML.jpg" alt="dataML"></p><p>通过对比不同的算法我们会发现，随着训练数据集的增大，所有算法的性能也都会对应地增强。 事实上，如果你选择任意一个算法，可能是选择了一个”劣等的”算法，如果你给这个劣等算法更多的数据，那么从这些例子中看起来的话，它看上去很有可能会其他算法更好，甚至会比”优等算法”更好。</p><p>结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现。真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识：”取得成功的人不是拥有最好算法的人，而是拥有最多数据的人”。</p><p>那么在什么条件下，更多的数据会提高我们模型的准确度呢？</p><ol><li>特征值中有足够的信息来预测y，比如给一个人这些信息，是否可以做出准确预测。</li><li>使用的算法中有足够多的参数（比如线性回归或逻辑回归中有很多特征值，或者神经网络中有足够的hidden layer）。</li><li>训练集中的数据量足够大。</li></ol><p>如果满足以上条件，并且收集到足够多的数据，你大概率会得到一个好性能的算法。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 13-应用机器学习的建议(Advice for Applying Machine Learning)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-12-exercise 4 summary</title>
    <link href="https://shilei165.github.io/2022/08/26/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-12/"/>
    <id>https://shilei165.github.io/2022/08/26/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-12/</id>
    <published>2022-08-26T20:25:23.000Z</published>
    <updated>2022-09-14T15:27:26.959Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 12-exercise 4 summary。</p><span id="more"></span><p><strong>Programming Exercise 4: Neural Networks Learning</strong></p><p>In this exercise, you will implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition.</p><h1 id="ex4-m"><a href="#ex4-m" class="headerlink" title="ex4.m"></a>ex4.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise 4 Neural Network Learning</span><br><span class="line"></span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  linear exercise. You will need to complete the following functions </span><br><span class="line">%  in this exericse:</span><br><span class="line">%</span><br><span class="line">%     sigmoidGradient.m</span><br><span class="line">%     randInitializeWeights.m</span><br><span class="line">%     nnCostFunction.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Setup the parameters you will use for this exercise</span><br><span class="line">input_layer_size  = 400;  % 20x20 Input Images of Digits</span><br><span class="line">hidden_layer_size = 25;   % 25 hidden units</span><br><span class="line">num_labels = 10;          % 10 labels, from 1 to 10   </span><br><span class="line">                          % (note that we have mapped &quot;0&quot; to label 10)</span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Loading and Visualizing Data =============</span><br><span class="line">%  We start the exercise by first loading and visualizing the dataset. </span><br><span class="line">%  You will be working with a dataset that contains handwritten digits.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load Training Data</span><br><span class="line">fprintf(&#x27;Loading and Visualizing Data ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">load(&#x27;ex4data1.mat&#x27;);</span><br><span class="line">m = size(X, 1);</span><br><span class="line"></span><br><span class="line">% Randomly select 100 data points to display</span><br><span class="line">sel = randperm(size(X, 1));</span><br><span class="line">sel = sel(1:100);</span><br><span class="line"></span><br><span class="line">displayData(X(sel, :));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================ Part 2: Loading Parameters ================</span><br><span class="line">% In this part of the exercise, we load some pre-initialized </span><br><span class="line">% neural network parameters.</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nLoading Saved Neural Network Parameters ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">% Load the weights into variables Theta1 and Theta2</span><br><span class="line">load(&#x27;ex4weights.mat&#x27;);</span><br><span class="line"></span><br><span class="line">% Unroll parameters </span><br><span class="line">nn_params = [Theta1(:) ; Theta2(:)];</span><br><span class="line"></span><br><span class="line">%% ================ Part 3: Compute Cost (Feedforward) ================</span><br><span class="line">%  To the neural network, you should first start by implementing the</span><br><span class="line">%  feedforward part of the neural network that returns the cost only. You</span><br><span class="line">%  should complete the code in nnCostFunction.m to return cost. After</span><br><span class="line">%  implementing the feedforward to compute the cost, you can verify that</span><br><span class="line">%  your implementation is correct by verifying that you get the same cost</span><br><span class="line">%  as us for the fixed debugging parameters.</span><br><span class="line">%</span><br><span class="line">%  We suggest implementing the feedforward cost *without* regularization</span><br><span class="line">%  first so that it will be easier for you to debug. Later, in part 4, you</span><br><span class="line">%  will get to implement the regularized cost.</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nFeedforward Using Neural Network ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">% Weight regularization parameter (we set this to 0 here).</span><br><span class="line">lambda = 0;</span><br><span class="line"></span><br><span class="line">J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...</span><br><span class="line">                   num_labels, X, y, lambda);</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;Cost at parameters (loaded from ex4weights): %f &#x27;...</span><br><span class="line">         &#x27;\n(this value should be about 0.287629)\n&#x27;], J);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =============== Part 4: Implement Regularization ===============</span><br><span class="line">%  Once your cost function implementation is correct, you should now</span><br><span class="line">%  continue to implement the regularization with the cost.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nChecking Cost Function (w/ Regularization) ... \n&#x27;)</span><br><span class="line"></span><br><span class="line">% Weight regularization parameter (we set this to 1 here).</span><br><span class="line">lambda = 1;</span><br><span class="line"></span><br><span class="line">J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...</span><br><span class="line">                   num_labels, X, y, lambda);</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;Cost at parameters (loaded from ex4weights): %f &#x27;...</span><br><span class="line">         &#x27;\n(this value should be about 0.383770)\n&#x27;], J);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================ Part 5: Sigmoid Gradient  ================</span><br><span class="line">%  Before you start implementing the neural network, you will first</span><br><span class="line">%  implement the gradient for the sigmoid function. You should complete the</span><br><span class="line">%  code in the sigmoidGradient.m file.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nEvaluating sigmoid gradient...\n&#x27;)</span><br><span class="line"></span><br><span class="line">g = sigmoidGradient([-1 -0.5 0 0.5 1]);</span><br><span class="line">fprintf(&#x27;Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n  &#x27;);</span><br><span class="line">fprintf(&#x27;%f &#x27;, g);</span><br><span class="line">fprintf(&#x27;\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================ Part 6: Initializing Pameters ================</span><br><span class="line">%  In this part of the exercise, you will be starting to implment a two</span><br><span class="line">%  layer neural network that classifies digits. You will start by</span><br><span class="line">%  implementing a function to initialize the weights of the neural network</span><br><span class="line">%  (randInitializeWeights.m)</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nInitializing Neural Network Parameters ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);</span><br><span class="line">initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);</span><br><span class="line"></span><br><span class="line">% Unroll parameters</span><br><span class="line">initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =============== Part 7: Implement Backpropagation ===============</span><br><span class="line">%  Once your cost matches up with ours, you should proceed to implement the</span><br><span class="line">%  backpropagation algorithm for the neural network. You should add to the</span><br><span class="line">%  code you&#x27;ve written in nnCostFunction.m to return the partial</span><br><span class="line">%  derivatives of the parameters.</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nChecking Backpropagation... \n&#x27;);</span><br><span class="line"></span><br><span class="line">%  Check gradients by running checkNNGradients</span><br><span class="line">checkNNGradients;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =============== Part 8: Implement Regularization ===============</span><br><span class="line">%  Once your backpropagation implementation is correct, you should now</span><br><span class="line">%  continue to implement the regularization with the cost and gradient.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nChecking Backpropagation (w/ Regularization) ... \n&#x27;)</span><br><span class="line"></span><br><span class="line">%  Check gradients by running checkNNGradients</span><br><span class="line">lambda = 3;</span><br><span class="line">checkNNGradients(lambda);</span><br><span class="line"></span><br><span class="line">% Also output the costFunction debugging values</span><br><span class="line">debug_J  = nnCostFunction(nn_params, input_layer_size, ...</span><br><span class="line">                          hidden_layer_size, num_labels, X, y, lambda);</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;\n\nCost at (fixed) debugging parameters (w/ lambda = %f): %f &#x27; ...</span><br><span class="line">         &#x27;\n(for lambda = 3, this value should be about 0.576051)\n\n&#x27;], lambda, debug_J);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =================== Part 8: Training NN ===================</span><br><span class="line">%  You have now implemented all the code necessary to train a neural </span><br><span class="line">%  network. To train your neural network, we will now use &quot;fmincg&quot;, which</span><br><span class="line">%  is a function which works similarly to &quot;fminunc&quot;. Recall that these</span><br><span class="line">%  advanced optimizers are able to train our cost functions efficiently as</span><br><span class="line">%  long as we provide them with the gradient computations.</span><br><span class="line">%</span><br><span class="line">fprintf(&#x27;\nTraining Neural Network... \n&#x27;)</span><br><span class="line"></span><br><span class="line">%  After you have completed the assignment, change the MaxIter to a larger</span><br><span class="line">%  value to see how more training helps.</span><br><span class="line">options = optimset(&#x27;MaxIter&#x27;, 50);</span><br><span class="line"></span><br><span class="line">%  You should also try different values of lambda</span><br><span class="line">lambda = 1;</span><br><span class="line"></span><br><span class="line">% Create &quot;short hand&quot; for the cost function to be minimized</span><br><span class="line">costFunction = @(p) nnCostFunction(p, ...</span><br><span class="line">                                   input_layer_size, ...</span><br><span class="line">                                   hidden_layer_size, ...</span><br><span class="line">                                   num_labels, X, y, lambda);</span><br><span class="line"></span><br><span class="line">% Now, costFunction is a function that takes in only one argument (the</span><br><span class="line">% neural network parameters)</span><br><span class="line">[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);</span><br><span class="line"></span><br><span class="line">% Obtain Theta1 and Theta2 back from nn_params</span><br><span class="line">Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...</span><br><span class="line">                 hidden_layer_size, (input_layer_size + 1));</span><br><span class="line"></span><br><span class="line">Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...</span><br><span class="line">                 num_labels, (hidden_layer_size + 1));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================= Part 9: Visualize Weights =================</span><br><span class="line">%  You can now &quot;visualize&quot; what the neural network is learning by </span><br><span class="line">%  displaying the hidden units to see what features they are capturing in </span><br><span class="line">%  the data.</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nVisualizing Neural Network... \n&#x27;)</span><br><span class="line"></span><br><span class="line">displayData(Theta1(:, 2:end));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ================= Part 10: Implement Predict =================</span><br><span class="line">%  After training the neural network, we would like to use it to predict</span><br><span class="line">%  the labels. You will now implement the &quot;predict&quot; function to use the</span><br><span class="line">%  neural network to predict the labels of the training set. This lets</span><br><span class="line">%  you compute the training set accuracy.</span><br><span class="line"></span><br><span class="line">pred = predict(Theta1, Theta2, X);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nTraining Set Accuracy: %f\n&#x27;, mean(double(pred == y)) * 100);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="displayData-m"><a href="#displayData-m" class="headerlink" title="displayData.m"></a>displayData.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">function [h, display_array] = displayData(X, example_width)</span><br><span class="line">%DISPLAYDATA Display 2D data in a nice grid</span><br><span class="line">%   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data</span><br><span class="line">%   stored in X in a nice grid. It returns the figure handle h and the </span><br><span class="line">%   displayed array if requested.</span><br><span class="line"></span><br><span class="line">% Set example_width automatically if not passed in</span><br><span class="line">if ~exist(&#x27;example_width&#x27;, &#x27;var&#x27;) || isempty(example_width) </span><br><span class="line">  example_width = round(sqrt(size(X, 2)));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Gray Image</span><br><span class="line">colormap(gray);</span><br><span class="line"></span><br><span class="line">% Compute rows, cols</span><br><span class="line">[m n] = size(X);</span><br><span class="line">example_height = (n / example_width);</span><br><span class="line"></span><br><span class="line">% Compute number of items to display</span><br><span class="line">display_rows = floor(sqrt(m));</span><br><span class="line">display_cols = ceil(m / display_rows);</span><br><span class="line"></span><br><span class="line">% Between images padding</span><br><span class="line">pad = 1;</span><br><span class="line"></span><br><span class="line">% Setup blank display</span><br><span class="line">display_array = - ones(pad + display_rows * (example_height + pad), ...</span><br><span class="line">                       pad + display_cols * (example_width + pad));</span><br><span class="line"></span><br><span class="line">% Copy each example into a patch on the display array</span><br><span class="line">curr_ex = 1;</span><br><span class="line">for j = 1:display_rows</span><br><span class="line">  for i = 1:display_cols</span><br><span class="line">    if curr_ex &gt; m, </span><br><span class="line">      break; </span><br><span class="line">    end</span><br><span class="line">    % Copy the patch</span><br><span class="line">    </span><br><span class="line">    % Get the max value of the patch</span><br><span class="line">    max_val = max(abs(X(curr_ex, :)));</span><br><span class="line">    display_array(pad + (j - 1) * (example_height + pad) + (1:example_height), ...</span><br><span class="line">                  pad + (i - 1) * (example_width + pad) + (1:example_width)) = ...</span><br><span class="line">            reshape(X(curr_ex, :), example_height, example_width) / max_val;</span><br><span class="line">    curr_ex = curr_ex + 1;</span><br><span class="line">  end</span><br><span class="line">  if curr_ex &gt; m, </span><br><span class="line">    break; </span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Display Image</span><br><span class="line">h = imagesc(display_array, [-1 1]);</span><br><span class="line"></span><br><span class="line">% Do not show axis</span><br><span class="line">axis image off</span><br><span class="line"></span><br><span class="line">drawnow;</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="nnCostFunction-m"><a href="#nnCostFunction-m" class="headerlink" title="nnCostFunction.m"></a>nnCostFunction.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">function [J grad] = nnCostFunction(nn_params, ...</span><br><span class="line">                                   input_layer_size, ...</span><br><span class="line">                                   hidden_layer_size, ...</span><br><span class="line">                                   num_labels, ...</span><br><span class="line">                                   X, y, lambda)</span><br><span class="line">%NNCOSTFUNCTION Implements the neural network cost function for a two layer</span><br><span class="line">%neural network which performs classification</span><br><span class="line">%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...</span><br><span class="line">%   X, y, lambda) computes the cost and gradient of the neural network. The</span><br><span class="line">%   parameters for the neural network are &quot;unrolled&quot; into the vector</span><br><span class="line">%   nn_params and need to be converted back into the weight matrices. </span><br><span class="line">% </span><br><span class="line">%   The returned parameter grad should be a &quot;unrolled&quot; vector of the</span><br><span class="line">%   partial derivatives of the neural network.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices</span><br><span class="line">% for our 2 layer neural network</span><br><span class="line">Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...</span><br><span class="line">                 hidden_layer_size, (input_layer_size + 1));</span><br><span class="line"></span><br><span class="line">Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...</span><br><span class="line">                 num_labels, (hidden_layer_size + 1));</span><br><span class="line"></span><br><span class="line">% Setup some useful variables</span><br><span class="line">m = size(X, 1);</span><br><span class="line">         </span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">J = 0;</span><br><span class="line">Theta1_grad = zeros(size(Theta1));</span><br><span class="line">Theta2_grad = zeros(size(Theta2));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: You should complete the code by working through the</span><br><span class="line">%               following parts.</span><br><span class="line">%</span><br><span class="line">% Part 1: Feedforward the neural network and return the cost in the</span><br><span class="line">%         variable J. After implementing Part 1, you can verify that your</span><br><span class="line">%         cost function computation is correct by verifying the cost</span><br><span class="line">%         computed in ex4.m</span><br><span class="line">%</span><br><span class="line">% Part 2: Implement the backpropagation algorithm to compute the gradients</span><br><span class="line">%         Theta1_grad and Theta2_grad. You should return the partial derivatives of</span><br><span class="line">%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and</span><br><span class="line">%         Theta2_grad, respectively. After implementing Part 2, you can check</span><br><span class="line">%         that your implementation is correct by running checkNNGradients</span><br><span class="line">%</span><br><span class="line">%         Note: The vector y passed into the function is a vector of labels</span><br><span class="line">%               containing values from 1..K. You need to map this vector into a </span><br><span class="line">%               binary vector of 1&#x27;s and 0&#x27;s to be used with the neural network</span><br><span class="line">%               cost function.</span><br><span class="line">%</span><br><span class="line">%         Hint: We recommend implementing backpropagation using a for-loop</span><br><span class="line">%               over the training examples if you are implementing it for the </span><br><span class="line">%               first time.</span><br><span class="line">%</span><br><span class="line">% Part 3: Implement regularization with the cost function and gradients.</span><br><span class="line">%</span><br><span class="line">%         Hint: You can implement this around the code for</span><br><span class="line">%               backpropagation. That is, you can compute the gradients for</span><br><span class="line">%               the regularization separately and then add them to Theta1_grad</span><br><span class="line">%               and Theta2_grad from Part 2.</span><br><span class="line">%</span><br><span class="line">K = num_labels;</span><br><span class="line"></span><br><span class="line">Y = zeros(m,num_labels);</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line">    Y(i,y(i)) = 1;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">a1 = [ones(m,1), X]</span><br><span class="line"></span><br><span class="line">z2 = a1*Theta1&#x27;;</span><br><span class="line">a2 = sigmoid(z2);</span><br><span class="line">a2 = [ones(size(a2,1),1), a2];</span><br><span class="line"></span><br><span class="line">z3 = a2*Theta2&#x27;;</span><br><span class="line">a3 = sigmoid(z3);</span><br><span class="line"></span><br><span class="line">cost = sum((-Y.*log(a3))-((1-Y).*log(1-a3)), 2);</span><br><span class="line">J = 1/m*sum(cost);</span><br><span class="line"></span><br><span class="line">Theta1NoBias = Theta1(:, 2:end);</span><br><span class="line">Theta2NoBias = Theta2(:, 2:end);</span><br><span class="line"></span><br><span class="line">reg = (lambda/(2*m))*(sum(sumsq(Theta1NoBias))+sum(sumsq(Theta2NoBias)));</span><br><span class="line"></span><br><span class="line">J = J + reg;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% -------------------------------------------------------------</span><br><span class="line">% Compute gradients using back propagation</span><br><span class="line"></span><br><span class="line">Delta1 = 0;</span><br><span class="line">Delta2 = 0;</span><br><span class="line"></span><br><span class="line">for t = 1:m</span><br><span class="line">    a_1 = X(t,:)&#x27;;</span><br><span class="line">    a_1 = [1; a_1];</span><br><span class="line"></span><br><span class="line">    z_2 = Theta1* a_1;</span><br><span class="line"></span><br><span class="line">    a_2 = sigmoid(z_2);</span><br><span class="line">    a_2 = [1; a_2];</span><br><span class="line"></span><br><span class="line">    z_3 = Theta2* a_2;</span><br><span class="line"></span><br><span class="line">    a_3 = sigmoid(z_3);</span><br><span class="line"></span><br><span class="line">    % Delta Output layer</span><br><span class="line">    d_3 = a_3 - Y(t,:)&#x27;;</span><br><span class="line"></span><br><span class="line">    % Delta Hidden layer</span><br><span class="line">    d_2 = (Theta2NoBias&#x27;*d_3).*sigmoidGradient(z_2);</span><br><span class="line"></span><br><span class="line">    % Accumulate</span><br><span class="line"></span><br><span class="line">    Delta2 = Delta2+(d_3*a_2&#x27;);</span><br><span class="line">    Delta1 = Delta1+(d_2*a_1&#x27;);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Theta1_grad = (1/m)*Delta1;</span><br><span class="line">Theta2_grad = (1/m)*Delta2;</span><br><span class="line"></span><br><span class="line">Theta1_grad(:, 2:end) = Theta1_grad(:, 2:end) + ((lambda/m)*Theta1NoBias);</span><br><span class="line"></span><br><span class="line">Theta2_grad(:, 2:end) = Theta2_grad(:, 2:end) + ((lambda/m)*Theta2NoBias);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% -------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">% Unroll gradients</span><br><span class="line">grad = [Theta1_grad(:) ; Theta2_grad(:)];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="computeNumericalGradient-m"><a href="#computeNumericalGradient-m" class="headerlink" title="computeNumericalGradient.m"></a>computeNumericalGradient.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">function numgrad = computeNumericalGradient(J, theta)</span><br><span class="line">%COMPUTENUMERICALGRADIENT Computes the gradient using &quot;finite differences&quot;</span><br><span class="line">%and gives us a numerical estimate of the gradient.</span><br><span class="line">%   numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical</span><br><span class="line">%   gradient of the function J around theta. Calling y = J(theta) should</span><br><span class="line">%   return the function value at theta.</span><br><span class="line"></span><br><span class="line">% Notes: The following code implements numerical gradient checking, and </span><br><span class="line">%        returns the numerical gradient.It sets numgrad(i) to (a numerical </span><br><span class="line">%        approximation of) the partial derivative of J with respect to the </span><br><span class="line">%        i-th input argument, evaluated at theta. (i.e., numgrad(i) should </span><br><span class="line">%        be the (approximately) the partial derivative of J with respect </span><br><span class="line">%        to theta(i).)</span><br><span class="line">%                </span><br><span class="line"></span><br><span class="line">numgrad = zeros(size(theta));</span><br><span class="line">perturb = zeros(size(theta));</span><br><span class="line">e = 1e-4;</span><br><span class="line">for p = 1:numel(theta)</span><br><span class="line">    % Set perturbation vector</span><br><span class="line">    perturb(p) = e;</span><br><span class="line">    loss1 = J(theta - perturb);</span><br><span class="line">    loss2 = J(theta + perturb);</span><br><span class="line">    % Compute Numerical Gradient</span><br><span class="line">    numgrad(p) = (loss2 - loss1) / (2*e);</span><br><span class="line">    perturb(p) = 0;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="checkNNGradients-m"><a href="#checkNNGradients-m" class="headerlink" title="checkNNGradients.m"></a>checkNNGradients.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">function checkNNGradients(lambda)</span><br><span class="line">%CHECKNNGRADIENTS Creates a small neural network to check the</span><br><span class="line">%backpropagation gradients</span><br><span class="line">%   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the</span><br><span class="line">%   backpropagation gradients, it will output the analytical gradients</span><br><span class="line">%   produced by your backprop code and the numerical gradients (computed</span><br><span class="line">%   using computeNumericalGradient). These two gradient computations should</span><br><span class="line">%   result in very similar values.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">if ~exist(&#x27;lambda&#x27;, &#x27;var&#x27;) || isempty(lambda)</span><br><span class="line">    lambda = 0;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">input_layer_size = 3;</span><br><span class="line">hidden_layer_size = 5;</span><br><span class="line">num_labels = 3;</span><br><span class="line">m = 5;</span><br><span class="line"></span><br><span class="line">% We generate some &#x27;random&#x27; test data</span><br><span class="line">Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size);</span><br><span class="line">Theta2 = debugInitializeWeights(num_labels, hidden_layer_size);</span><br><span class="line">% Reusing debugInitializeWeights to generate X</span><br><span class="line">X  = debugInitializeWeights(m, input_layer_size - 1);</span><br><span class="line">y  = 1 + mod(1:m, num_labels)&#x27;;</span><br><span class="line"></span><br><span class="line">% Unroll parameters</span><br><span class="line">nn_params = [Theta1(:) ; Theta2(:)];</span><br><span class="line"></span><br><span class="line">% Short hand for cost function</span><br><span class="line">costFunc = @(p) nnCostFunction(p, input_layer_size, hidden_layer_size, ...</span><br><span class="line">                               num_labels, X, y, lambda);</span><br><span class="line"></span><br><span class="line">[cost, grad] = costFunc(nn_params);</span><br><span class="line">numgrad = computeNumericalGradient(costFunc, nn_params);</span><br><span class="line"></span><br><span class="line">% Visually examine the two gradient computations.  The two columns</span><br><span class="line">% you get should be very similar. </span><br><span class="line">disp([numgrad grad]);</span><br><span class="line">fprintf([&#x27;The above two columns you get should be very similar.\n&#x27; ...</span><br><span class="line">         &#x27;(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\n&#x27;]);</span><br><span class="line"></span><br><span class="line">% Evaluate the norm of the difference between two solutions.  </span><br><span class="line">% If you have a correct implementation, and assuming you used EPSILON = 0.0001 </span><br><span class="line">% in computeNumericalGradient.m, then diff below should be less than 1e-9</span><br><span class="line">diff = norm(numgrad-grad)/norm(numgrad+grad);</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;If your backpropagation implementation is correct, then \n&#x27; ...</span><br><span class="line">         &#x27;the relative difference will be small (less than 1e-9). \n&#x27; ...</span><br><span class="line">         &#x27;\nRelative Difference: %g\n&#x27;], diff);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="randInitializaWeights-m"><a href="#randInitializaWeights-m" class="headerlink" title="randInitializaWeights.m"></a>randInitializaWeights.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">function W = randInitializeWeights(L_in, L_out)</span><br><span class="line">%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in</span><br><span class="line">%incoming connections and L_out outgoing connections</span><br><span class="line">%   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights </span><br><span class="line">%   of a layer with L_in incoming connections and L_out outgoing </span><br><span class="line">%   connections. </span><br><span class="line">%</span><br><span class="line">%   Note that W should be set to a matrix of size(L_out, 1 + L_in) as</span><br><span class="line">%   the first column of W handles the &quot;bias&quot; terms</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">W = zeros(L_out, 1 + L_in);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Initialize W randomly so that we break the symmetry while</span><br><span class="line">%               training the neural network.</span><br><span class="line">%</span><br><span class="line">% Note: The first column of W corresponds to the parameters for the bias unit</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">espilon_init = 0.12;</span><br><span class="line">W = rand(L_out, 1 + L_in)*2*espilon_init - espilon_init;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="debugInitializaWeights-m"><a href="#debugInitializaWeights-m" class="headerlink" title="debugInitializaWeights.m"></a>debugInitializaWeights.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">function W = debugInitializeWeights(fan_out, fan_in)</span><br><span class="line">%DEBUGINITIALIZEWEIGHTS Initialize the weights of a layer with fan_in</span><br><span class="line">%incoming connections and fan_out outgoing connections using a fixed</span><br><span class="line">%strategy, this will help you later in debugging</span><br><span class="line">%   W = DEBUGINITIALIZEWEIGHTS(fan_in, fan_out) initializes the weights </span><br><span class="line">%   of a layer with fan_in incoming connections and fan_out outgoing </span><br><span class="line">%   connections using a fix set of values</span><br><span class="line">%</span><br><span class="line">%   Note that W should be set to a matrix of size(1 + fan_in, fan_out) as</span><br><span class="line">%   the first row of W handles the &quot;bias&quot; terms</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Set W to zeros</span><br><span class="line">W = zeros(fan_out, 1 + fan_in);</span><br><span class="line"></span><br><span class="line">% Initialize W using &quot;sin&quot;, this ensures that W is always of the same</span><br><span class="line">% values and will be useful for debugging</span><br><span class="line">W = reshape(sin(1:numel(W)), size(W)) / 10;</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="sigmoidGradient-m"><a href="#sigmoidGradient-m" class="headerlink" title="sigmoidGradient.m"></a>sigmoidGradient.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">function g = sigmoidGradient(z)</span><br><span class="line">%SIGMOIDGRADIENT returns the gradient of the sigmoid function</span><br><span class="line">%evaluated at z</span><br><span class="line">%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function</span><br><span class="line">%   evaluated at z. This should work regardless if z is a matrix or a</span><br><span class="line">%   vector. In particular, if z is a vector or matrix, you should return</span><br><span class="line">%   the gradient for each element.</span><br><span class="line"></span><br><span class="line">g = zeros(size(z));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the gradient of the sigmoid function evaluated at</span><br><span class="line">%               each value of z (z can be a matrix, vector or scalar).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">g = sigmoid(z).*(1-sigmoid(z));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 12-exercise 4 summary。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-11-Neural Networks:Learning</title>
    <link href="https://shilei165.github.io/2022/08/25/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-11/"/>
    <id>https://shilei165.github.io/2022/08/25/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-11/</id>
    <published>2022-08-25T20:56:23.000Z</published>
    <updated>2022-09-13T17:26:35.460Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 11-神经网络的学习(Neural Networks: Learning)。</p><span id="more"></span><h1 id="代价函数-Cost-function"><a href="#代价函数-Cost-function" class="headerlink" title="代价函数(Cost function)"></a>代价函数(Cost function)</h1><p>首先引入一些标记方法，以便于讨论：</p><p>假设神经网络的训练样本有\(m\)个，每个包含一组输入信号\(x\)和一组输出信号\(y\)，\(L\)表示神经网络层数，\(S_l\)表示\(l))层的neuron个数(不包含bias单元)。</p><p>神经网络分为两类：</p><ul><li>二元分类：\(y&#x3D;0\ or\ 1\)；1个输出变量</li><li>多元分类：\(y\in\mathbb{R}^K\)；K个输出变量</li></ul><p><img src="/../images/NeuralNetwork_1.jpg" alt="NeuralNetwork_1"></p><p>逻辑回归中代价函数为：<br>$$<br>J(\theta)&#x3D; -\frac{1}{m}[\sum_{i&#x3D;1}^{m}y^{(i)}log h_\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j&#x3D;1}^{n}\theta_j^2<br>$$<br>与逻辑回归不同的是，神经网络有多个输出变量，或者说\(h_\Theta(x)\)是一个维度为K的向量。因此代价函数也会更加复杂一点：<br>$$<br>J(\Theta)&#x3D; -\frac{1}{m}[\sum_{i&#x3D;1}^{m}\sum_{k&#x3D;1}^{k}y_k^{(i)}log (h_\Theta(x^{(i)}))_k + (1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum_{l&#x3D;1}^{L-1}\sum_{i&#x3D;1}^{s_l}\sum_{j&#x3D;1}^{s_l+1}(\Theta_{ji}^{(l)})^2<br>$$</p><p>其中，\(h_\Theta(x)\in\mathbb{R}^K\)  \((h_\Theta(x))_i&#x3D;i^{th}\ output\)</p><p>这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出K个预测，基本上我们可以利用循环，对每一行特征都预测K个不同结果，然后在利用循环在K个预测中选择可能性最高的一个，将其与y中的实际数据进行比较。</p><p>正则化那一项只是排除了每一层的\(\theta_0\)后，每一层的\(\theta\)矩阵的和。</p><h1 id="反向传播算法-Backpropagation-algorithm"><a href="#反向传播算法-Backpropagation-algorithm" class="headerlink" title="反向传播算法(Backpropagation algorithm)"></a>反向传播算法(Backpropagation algorithm)</h1><p>之前我们在计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的\(h_\theta(x)\)。</p><p>现在为了计算代价函数的偏导数\(\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)\)，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。 </p><p>在不做任何正则化处理时：<br>$$<br>\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)&#x3D;a_j^{(l)}\delta_i^{l+1}<br>$$<br>\(l\)代表目前所计算的是第几层。</p><p>\(j\)代表目前计算层中的激活单元的下标。</p><p>\(i\)代表下一层中误差单元的下标。</p><p>如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量，我们需要计算每一层的误差单元来计算代价函数的偏导数。我们用\(\Delta_{ij}^{(l)}\)来表示这个误差矩阵。</p><p>假设我们有m个训练集，利用反向传播来计算\(\Delta_{ij}^{(l)}\)的算法如下：</p><p><img src="/../images/NeuralNetwork_2.jpg" alt="NeuralNetwork_2"></p><p>在求出了\(\Delta_{ij}^{(l)}\)之后，我们便可以计算代价函数的偏导数了，计算方法如下： </p><p>\(D_{ij}^{(l)}\ :&#x3D;\ \frac{1}{m}\Delta_{ij}^{(l)} + \frac{\lambda}{m}\Theta_{ij}^{(l)}\) \(\ \ \ \ \ \ \ \  if\ \ \ j\neq 0\) </p><p>\(D_{ij}^{(l)}\ :&#x3D;\ \frac{1}{m}\Delta_{ij}^{(l)}\) \(\ \ \ \ \ \ \ \  if\ \ \ j&#x3D; 0\) </p><h1 id="展开参数"><a href="#展开参数" class="headerlink" title="展开参数"></a>展开参数</h1><p>在Octave 中，如果我们要使用fminuc这样的优化算法来求解求出权重矩阵，我们需要将矩阵首先展开成为向量，在利用算法求出最优解后再重新转换回矩阵。</p><p>假设我们有三个权重矩阵，Theta1，Theta2 和 Theta3，尺寸分别为 10*11，10*11 和1*11， 下面的代码可以实现这样的转换：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">thetaVec = [Theta1(:) ; Theta2(:) ; Theta3(:)]</span><br><span class="line"></span><br><span class="line">...optimization using functions like fminuc...</span><br><span class="line"></span><br><span class="line">Theta1 = reshape(thetaVec(1:110, 10, 11);</span><br><span class="line"></span><br><span class="line">Theta2 = reshape(thetaVec(111:220, 10, 11);</span><br><span class="line"></span><br><span class="line">Theta1 = reshape(thetaVec(221:231, 1, 11);</span><br></pre></td></tr></table></figure><h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。</p><p>为了避免这样的问题，我们采取一种叫做梯度的数值检验（Numerical Gradient Checking）方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。</p><p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的\(\theta\)，我们计算出在\(\theta-\epsilon\)处和\(\theta+\epsilon\)的代价值\((\epsilon\)是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在\(\theta\)处的代价值。</p><p><img src="/../images/NeuralNetwork_3.jpg" alt="NeuralNetwork_3"></p><p>Octave 中代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradApprox = (J(theta + eps) – J(theta - eps)) / (2*eps)</span><br></pre></td></tr></table></figure><p>当\(\theta\)是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对\(\theta_1\)进行检验的示例:\(\frac{\partial}{\partial\theta_1}&#x3D;\frac{J(\theta_1+\epsilon_1,\theta_2,\theta_3…\theta_n)-J(\theta_1-\epsilon_1,\theta_2,\theta_3…\theta_n)}{2\epsilon}\)</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i = 1:n</span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(i) = thetaPlus(i) + EPSILON;</span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(i) = thetaMinus(i) - EPSILON;</span><br><span class="line">  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*EPSILON);</span><br><span class="line">end;</span><br></pre></td></tr></table></figure><p>我们需要将通过反向传播计算出的偏导数\(D_{ij}^{(l)}\)转换成向量DVec，然后与gradApprox进行比较。</p><h1 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h1><p>任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的。</p><p>我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Theta1 = rand(10, 11) * (2*eps) - eps</span><br></pre></td></tr></table></figure><h1 id="综合起来"><a href="#综合起来" class="headerlink" title="综合起来"></a>综合起来</h1><p>小结一下使用神经网络时的步骤：</p><ul><li><p>网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。</p></li><li><p>第一层的单元数即我们训练集的特征数量。</p></li><li><p>最后一层的单元数是我们训练集的结果的类的数量。</p></li><li><p>如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。</p></li><li><p>我们真正要决定的是隐藏层的层数和每个中间层的单元数。</p></li></ul><p>训练神经网络：</p><ul><li><p>参数的随机初始化</p></li><li><p>利用正向传播方法计算所有的\(h_\theta(x)\)</p></li><li><p>编写计算代价函数J的代码</p></li><li><p>利用反向传播方法计算所有偏导数</p></li><li><p>利用数值检验方法检验这些偏导数</p></li><li><p>使用优化算法来最小化代价函数</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 11-神经网络的学习(Neural Networks: Learning)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-10-exercise 3 summary</title>
    <link href="https://shilei165.github.io/2022/08/20/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-10/"/>
    <id>https://shilei165.github.io/2022/08/20/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-10/</id>
    <published>2022-08-20T23:50:23.000Z</published>
    <updated>2022-09-14T15:28:25.288Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 10-exercise 3 summary。</p><span id="more"></span><p><strong>Programming Exercise 3: Multi-class Classification and Neural Networks</strong></p><p>In this exercise, you will implement one-vs-all logistic regression and neural networks to recognize hand-written digits.</p><h1 id="ex3-m"><a href="#ex3-m" class="headerlink" title="ex3.m"></a>ex3.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise 3 | Part 1: One-vs-all</span><br><span class="line"></span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  linear exercise. You will need to complete the following functions</span><br><span class="line">%  in this exericse:</span><br><span class="line">%</span><br><span class="line">%     lrCostFunction.m (logistic regression cost function)</span><br><span class="line">%     oneVsAll.m</span><br><span class="line">%     predictOneVsAll.m</span><br><span class="line">%     predict.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Setup the parameters you will use for this part of the exercise</span><br><span class="line">input_layer_size  = 400;  % 20x20 Input Images of Digits</span><br><span class="line">num_labels = 10;          % 10 labels, from 1 to 10</span><br><span class="line">                          % (note that we have mapped &quot;0&quot; to label 10)</span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Loading and Visualizing Data =============</span><br><span class="line">%  We start the exercise by first loading and visualizing the dataset.</span><br><span class="line">%  You will be working with a dataset that contains handwritten digits.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load Training Data</span><br><span class="line">fprintf(&#x27;Loading and Visualizing Data ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">load(&#x27;ex3data1.mat&#x27;); % training data stored in arrays X, y</span><br><span class="line">m = size(X, 1);</span><br><span class="line"></span><br><span class="line">% Randomly select 100 data points to display</span><br><span class="line">rand_indices = randperm(m);</span><br><span class="line">sel = X(rand_indices(1:100), :);</span><br><span class="line"></span><br><span class="line">displayData(sel);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============ Part 2a: Vectorize Logistic Regression ============</span><br><span class="line">%  In this part of the exercise, you will reuse your logistic regression</span><br><span class="line">%  code from the last exercise. You task here is to make sure that your</span><br><span class="line">%  regularized logistic regression implementation is vectorized. After</span><br><span class="line">%  that, you will implement one-vs-all classification for the handwritten</span><br><span class="line">%  digit dataset.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Test case for lrCostFunction</span><br><span class="line">fprintf(&#x27;\nTesting lrCostFunction() with regularization&#x27;);</span><br><span class="line"></span><br><span class="line">theta_t = [-2; -1; 1; 2];</span><br><span class="line">X_t = [ones(5,1) reshape(1:15,5,3)/10];</span><br><span class="line">y_t = ([1;0;1;0;1] &gt;= 0.5);</span><br><span class="line">lambda_t = 3;</span><br><span class="line">[J grad] = lrCostFunction(theta_t, X_t, y_t, lambda_t);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nCost: %f\n&#x27;, J);</span><br><span class="line">fprintf(&#x27;Expected cost: 2.534819\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradients:\n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad);</span><br><span class="line">fprintf(&#x27;Expected gradients:\n&#x27;);</span><br><span class="line">fprintf(&#x27; 0.146561\n -0.548558\n 0.724722\n 1.398003\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line">%% ============ Part 2b: One-vs-All Training ============</span><br><span class="line">fprintf(&#x27;\nTraining One-vs-All Logistic Regression...\n&#x27;)</span><br><span class="line"></span><br><span class="line">lambda = 0.1;</span><br><span class="line">[all_theta] = oneVsAll(X, y, num_labels, lambda);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================ Part 3: Predict for One-Vs-All ================</span><br><span class="line"></span><br><span class="line">pred = predictOneVsAll(all_theta, X);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nTraining Set Accuracy: %f\n&#x27;, mean(double(pred == y)) * 100);</span><br></pre></td></tr></table></figure><h1 id="displayData-m"><a href="#displayData-m" class="headerlink" title="displayData.m"></a>displayData.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">function [h, display_array] = displayData(X, example_width)</span><br><span class="line">%DISPLAYDATA Display 2D data in a nice grid</span><br><span class="line">%   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data</span><br><span class="line">%   stored in X in a nice grid. It returns the figure handle h and the </span><br><span class="line">%   displayed array if requested.</span><br><span class="line"></span><br><span class="line">% Set example_width automatically if not passed in</span><br><span class="line">if ~exist(&#x27;example_width&#x27;, &#x27;var&#x27;) || isempty(example_width) </span><br><span class="line">  example_width = round(sqrt(size(X, 2)));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Gray Image</span><br><span class="line">colormap(gray);</span><br><span class="line"></span><br><span class="line">% Compute rows, cols</span><br><span class="line">[m n] = size(X);</span><br><span class="line">example_height = (n / example_width);</span><br><span class="line"></span><br><span class="line">% Compute number of items to display</span><br><span class="line">display_rows = floor(sqrt(m));</span><br><span class="line">display_cols = ceil(m / display_rows);</span><br><span class="line"></span><br><span class="line">% Between images padding</span><br><span class="line">pad = 1;</span><br><span class="line"></span><br><span class="line">% Setup blank display</span><br><span class="line">display_array = - ones(pad + display_rows * (example_height + pad), ...</span><br><span class="line">                       pad + display_cols * (example_width + pad));</span><br><span class="line"></span><br><span class="line">% Copy each example into a patch on the display array</span><br><span class="line">curr_ex = 1;</span><br><span class="line">for j = 1:display_rows</span><br><span class="line">  for i = 1:display_cols</span><br><span class="line">    if curr_ex &gt; m, </span><br><span class="line">      break; </span><br><span class="line">    end</span><br><span class="line">    % Copy the patch</span><br><span class="line">    </span><br><span class="line">    % Get the max value of the patch</span><br><span class="line">    max_val = max(abs(X(curr_ex, :)));</span><br><span class="line">    display_array(pad + (j - 1) * (example_height + pad) + (1:example_height), ...</span><br><span class="line">                  pad + (i - 1) * (example_width + pad) + (1:example_width)) = ...</span><br><span class="line">            reshape(X(curr_ex, :), example_height, example_width) / max_val;</span><br><span class="line">    curr_ex = curr_ex + 1;</span><br><span class="line">  end</span><br><span class="line">  if curr_ex &gt; m, </span><br><span class="line">    break; </span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Display Image</span><br><span class="line">h = imagesc(display_array, [-1 1]);</span><br><span class="line"></span><br><span class="line">% Do not show axis</span><br><span class="line">axis image off</span><br><span class="line"></span><br><span class="line">drawnow;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="lrCostFunction-m"><a href="#lrCostFunction-m" class="headerlink" title="lrCostFunction.m"></a>lrCostFunction.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = lrCostFunction(theta, X, y, lambda)</span><br><span class="line">%LRCOSTFUNCTION Compute cost and gradient for logistic regression with </span><br><span class="line">%regularization</span><br><span class="line">%   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using</span><br><span class="line">%   theta as the parameter for regularized logistic regression and the</span><br><span class="line">%   gradient of the cost w.r.t. to the parameters. </span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">J = 0;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost of a particular choice of theta.</span><br><span class="line">%               You should set J to the cost.</span><br><span class="line">%               Compute the partial derivatives and set grad to the partial</span><br><span class="line">%               derivatives of the cost w.r.t. each parameter in theta</span><br><span class="line">%</span><br><span class="line">% Hint: The computation of the cost function and gradients can be</span><br><span class="line">%       efficiently vectorized. For example, consider the computation</span><br><span class="line">%</span><br><span class="line">%           sigmoid(X * theta)</span><br><span class="line">%</span><br><span class="line">%       Each row of the resulting matrix will contain the value of the</span><br><span class="line">%       prediction for that example. You can make use of this to vectorize</span><br><span class="line">%       the cost function and gradient computations. </span><br><span class="line">%</span><br><span class="line">% Hint: When computing the gradient of the regularized cost function, </span><br><span class="line">%       there&#x27;re many possible vectorized solutions, but one solution</span><br><span class="line">%       looks like:</span><br><span class="line">%           grad = (unregularized gradient for logistic regression)</span><br><span class="line">%           temp = theta; </span><br><span class="line">%           temp(1) = 0;   % because we don&#x27;t add anything for j = 0  </span><br><span class="line">%           grad = grad + YOUR_CODE_HERE (using the temp variable)</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">h = sigmoid(X*theta);</span><br><span class="line">reg = lambda/(2*m)*(theta&#x27;*theta-theta(1)^2);</span><br><span class="line">J = 1/m*(-y&#x27;*log(h)-(1-y&#x27;)*log(1-h))+reg;</span><br><span class="line"></span><br><span class="line">grad = 1/m*X&#x27;*(h-y);</span><br><span class="line">temp = theta;</span><br><span class="line">temp(1) = 0;</span><br><span class="line">grad = grad + lambda/m*temp;</span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">grad = grad(:);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="oneVsAll-m"><a href="#oneVsAll-m" class="headerlink" title="oneVsAll.m"></a>oneVsAll.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">function [all_theta] = oneVsAll(X, y, num_labels, lambda)</span><br><span class="line">%ONEVSALL trains multiple logistic regression classifiers and returns all</span><br><span class="line">%the classifiers in a matrix all_theta, where the i-th row of all_theta </span><br><span class="line">%corresponds to the classifier for label i</span><br><span class="line">%   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels</span><br><span class="line">%   logistic regression classifiers and returns each of these classifiers</span><br><span class="line">%   in a matrix all_theta, where the i-th row of all_theta corresponds </span><br><span class="line">%   to the classifier for label i</span><br><span class="line"></span><br><span class="line">% Some useful variables</span><br><span class="line">m = size(X, 1);</span><br><span class="line">n = size(X, 2);</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">all_theta = zeros(num_labels, n + 1);</span><br><span class="line"></span><br><span class="line">% Add ones to the X data matrix</span><br><span class="line">X = [ones(m, 1) X];</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: You should complete the following code to train num_labels</span><br><span class="line">%               logistic regression classifiers with regularization</span><br><span class="line">%               parameter lambda. </span><br><span class="line">%</span><br><span class="line">% Hint: theta(:) will return a column vector.</span><br><span class="line">%</span><br><span class="line">% Hint: You can use y == c to obtain a vector of 1&#x27;s and 0&#x27;s that tell you</span><br><span class="line">%       whether the ground truth is true/false for this class.</span><br><span class="line">%</span><br><span class="line">% Note: For this assignment, we recommend using fmincg to optimize the cost</span><br><span class="line">%       function. It is okay to use a for-loop (for c = 1:num_labels) to</span><br><span class="line">%       loop over the different classes.</span><br><span class="line">%</span><br><span class="line">%       fmincg works similarly to fminunc, but is more efficient when we</span><br><span class="line">%       are dealing with large number of parameters.</span><br><span class="line">%</span><br><span class="line">% Example Code for fmincg:</span><br><span class="line">%</span><br><span class="line">%     % Set Initial theta</span><br><span class="line">%     initial_theta = zeros(n + 1, 1);</span><br><span class="line">%     </span><br><span class="line">%     % Set options for fminunc</span><br><span class="line">%     options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, 50);</span><br><span class="line">% </span><br><span class="line">%     % Run fmincg to obtain the optimal theta</span><br><span class="line">%     % This function will return theta and the cost </span><br><span class="line">%     [theta] = ...</span><br><span class="line">%         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...</span><br><span class="line">%                 initial_theta, options);</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">for c = 1:num_labels</span><br><span class="line">  initial_theta = zeros(n + 1, 1);</span><br><span class="line"></span><br><span class="line">  options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, 50);</span><br><span class="line"></span><br><span class="line">  [theta] = ...</span><br><span class="line">    fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...</span><br><span class="line">      initial_theta, options);</span><br><span class="line">  all_theta (c,:) = [theta];</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="predictOneVsAll-m"><a href="#predictOneVsAll-m" class="headerlink" title="predictOneVsAll.m"></a>predictOneVsAll.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">function p = predictOneVsAll(all_theta, X)</span><br><span class="line">%PREDICT Predict the label for a trained one-vs-all classifier. The labels </span><br><span class="line">%are in the range 1..K, where K = size(all_theta, 1). </span><br><span class="line">%  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions</span><br><span class="line">%  for each example in the matrix X. Note that X contains the examples in</span><br><span class="line">%  rows. all_theta is a matrix where the i-th row is a trained logistic</span><br><span class="line">%  regression theta vector for the i-th class. You should set p to a vector</span><br><span class="line">%  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2</span><br><span class="line">%  for 4 examples) </span><br><span class="line"></span><br><span class="line">m = size(X, 1);</span><br><span class="line">num_labels = size(all_theta, 1);</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">p = zeros(size(X, 1), 1);</span><br><span class="line"></span><br><span class="line">% Add ones to the X data matrix</span><br><span class="line">X = [ones(m, 1) X];</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Complete the following code to make predictions using</span><br><span class="line">%               your learned logistic regression parameters (one-vs-all).</span><br><span class="line">%               You should set p to a vector of predictions (from 1 to</span><br><span class="line">%               num_labels).</span><br><span class="line">%</span><br><span class="line">% Hint: This code can be done all vectorized using the max function.</span><br><span class="line">%       In particular, the max function can also return the index of the </span><br><span class="line">%       max element, for more information see &#x27;help max&#x27;. If your examples </span><br><span class="line">%       are in rows, then, you can use max(A, [], 2) to obtain the max </span><br><span class="line">%       for each row.</span><br><span class="line">%       </span><br><span class="line"></span><br><span class="line">[probability, p] = max((sigmoid(X*all_theta&#x27;)), [], 2);</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="ex3-nn-m"><a href="#ex3-nn-m" class="headerlink" title="ex3_nn.m"></a>ex3_nn.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise 3 | Part 2: Neural Networks</span><br><span class="line"></span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you get started on the</span><br><span class="line">%  linear exercise. You will need to complete the following functions </span><br><span class="line">%  in this exericse:</span><br><span class="line">%</span><br><span class="line">%     lrCostFunction.m (logistic regression cost function)</span><br><span class="line">%     oneVsAll.m</span><br><span class="line">%     predictOneVsAll.m</span><br><span class="line">%     predict.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Setup the parameters you will use for this exercise</span><br><span class="line">input_layer_size  = 400;  % 20x20 Input Images of Digits</span><br><span class="line">hidden_layer_size = 25;   % 25 hidden units</span><br><span class="line">num_labels = 10;          % 10 labels, from 1 to 10   </span><br><span class="line">                          % (note that we have mapped &quot;0&quot; to label 10)</span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Loading and Visualizing Data =============</span><br><span class="line">%  We start the exercise by first loading and visualizing the dataset. </span><br><span class="line">%  You will be working with a dataset that contains handwritten digits.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load Training Data</span><br><span class="line">fprintf(&#x27;Loading and Visualizing Data ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">load(&#x27;ex3data1.mat&#x27;);</span><br><span class="line">m = size(X, 1);</span><br><span class="line"></span><br><span class="line">% Randomly select 100 data points to display</span><br><span class="line">sel = randperm(size(X, 1));</span><br><span class="line">sel = sel(1:100);</span><br><span class="line"></span><br><span class="line">displayData(X(sel, :));</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ================ Part 2: Loading Pameters ================</span><br><span class="line">% In this part of the exercise, we load some pre-initialized </span><br><span class="line">% neural network parameters.</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nLoading Saved Neural Network Parameters ...\n&#x27;)</span><br><span class="line"></span><br><span class="line">% Load the weights into variables Theta1 and Theta2</span><br><span class="line">load(&#x27;ex3weights.mat&#x27;);</span><br><span class="line"></span><br><span class="line">%% ================= Part 3: Implement Predict =================</span><br><span class="line">%  After training the neural network, we would like to use it to predict</span><br><span class="line">%  the labels. You will now implement the &quot;predict&quot; function to use the</span><br><span class="line">%  neural network to predict the labels of the training set. This lets</span><br><span class="line">%  you compute the training set accuracy.</span><br><span class="line"></span><br><span class="line">pred = predict(Theta1, Theta2, X);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nTraining Set Accuracy: %f\n&#x27;, mean(double(pred == y)) * 100);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Program paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%  To give you an idea of the network&#x27;s output, you can also run</span><br><span class="line">%  through the examples one at the a time to see what it is predicting.</span><br><span class="line"></span><br><span class="line">%  Randomly permute examples</span><br><span class="line">rp = randperm(m);</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line">    % Display </span><br><span class="line">    fprintf(&#x27;\nDisplaying Example Image\n&#x27;);</span><br><span class="line">    displayData(X(rp(i), :));</span><br><span class="line"></span><br><span class="line">    pred = predict(Theta1, Theta2, X(rp(i),:));</span><br><span class="line">    fprintf(&#x27;\nNeural Network Prediction: %d (digit %d)\n&#x27;, pred, mod(pred, 10));</span><br><span class="line">    </span><br><span class="line">    % Pause with quit option</span><br><span class="line">    s = input(&#x27;Paused - press enter to continue, q to exit:&#x27;,&#x27;s&#x27;);</span><br><span class="line">    if s == &#x27;q&#x27;</span><br><span class="line">      break</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="predict-m"><a href="#predict-m" class="headerlink" title="predict.m"></a>predict.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">function p = predict(Theta1, Theta2, X)</span><br><span class="line">%PREDICT Predict the label of an input given a trained neural network</span><br><span class="line">%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the</span><br><span class="line">%   trained weights of a neural network (Theta1, Theta2)</span><br><span class="line"></span><br><span class="line">% Useful values</span><br><span class="line">m = size(X, 1);</span><br><span class="line">num_labels = size(Theta2, 1);</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">p = zeros(size(X, 1), 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Complete the following code to make predictions using</span><br><span class="line">%               your learned neural network. You should set p to a </span><br><span class="line">%               vector containing labels between 1 to num_labels.</span><br><span class="line">%</span><br><span class="line">% Hint: The max function might come in useful. In particular, the max</span><br><span class="line">%       function can also return the index of the max element, for more</span><br><span class="line">%       information see &#x27;help max&#x27;. If your examples are in rows, then, you</span><br><span class="line">%       can use max(A, [], 2) to obtain the max for each row.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">X = [ones(m,1) X];</span><br><span class="line">a_2 = sigmoid(X*Theta1&#x27;);</span><br><span class="line"></span><br><span class="line">n = size(a_2, 1);</span><br><span class="line">a_2 = [ones(n,1) a_2];</span><br><span class="line"></span><br><span class="line">[probability, p] = max((sigmoid(a_2*Theta2&#x27;)), [], 2);</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 10-exercise 3 summary。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-09-Neural Networks:Representation</title>
    <link href="https://shilei165.github.io/2022/08/20/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-9/"/>
    <id>https://shilei165.github.io/2022/08/20/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-9/</id>
    <published>2022-08-20T16:10:23.000Z</published>
    <updated>2022-09-13T17:26:38.957Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 09-神经网络：表述(Neural Networks: Representation)。</p><span id="more"></span><h1 id="非线性假设"><a href="#非线性假设" class="headerlink" title="非线性假设"></a>非线性假设</h1><p>我们之前学的，无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。</p><p>在特征值相对较少时，使用非线性的多项式项，能够帮助我们建立很好的分类模型。假设我们有非常多的特征（比如大于100个变量），我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。</p><p>同样，在识别图像的训练模型中，由于每一个像素都是一个特征值，即使50x50的像素的小图片都会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约产生至少300万个特征(\(2500^2&#x2F;2\))。很显然，普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。</p><h1 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h1><p>神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（weight）。</p><p><img src="/../images/neuralNetwork_1.png" alt="neuralNetworks_1"></p><p>我们设计出了类似于神经元的神经网络，效果如下：</p><p><img src="/../images/neuralNetwork_2.png" alt="neuralNetworks_2"></p><p>其中\(x_1,x_2,x_3\) 是输入单元（input units），我们将原始数据输入给它们。\(a_1,a_2,a_3\) 是中间单元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算\(h_\theta(x)\)。</p><p>下面引入一些标记法来帮助描述模型：</p><p><strong>\(a_i^{(j)}\) : 代表第j层的第i个激活单元</strong></p><p><strong>\(\theta^{(j)}\) : 代表从第j层映射到第j+1层时的权重的矩阵</strong></p><ul><li>例如\(\theta^{(1)}\)代表从第一层映射到第二层的权重的矩阵</li><li>矩阵维度为：以第j+1层的激活单元数量为行数，以第j层的激活单元数加一为列数</li></ul><p>对于上图所示的模型，激活单元和输出分别表达为：</p><p>$$<br>a_1^{(2)} &#x3D; g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3)<br>$$<br>$$<br>a_2^{(2)} &#x3D; g(\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)<br>$$<br>$$<br>a_3^{(2)} &#x3D; g(\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3)<br>$$<br>$$<br>h_\theta(x)&#x3D;a_1^{(3)}&#x3D;g(\theta_{10}^{(2)}a_0^{(2)}+\theta_{11}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)})<br>$$</p><p>我们可以知道：每一个\(a\)都是由上一层所有的\(x\)和每一个\(x\)所对应的\(\theta\)决定的。我们把这样从左到右的算法称为前向传播算法(FORWARD PROPAGATION)。</p><p>上面的公式可以简化为：<br>$$<br>a^{(2)}&#x3D;g(z^{(2)})<br>$$<br>其中，\(z^{(2)}&#x3D;\theta*X\)。</p><p>我们可以继续使用同样的方法计算下一层的值：<br>$$<br>h_\theta(x)&#x3D;a^{(3)}&#x3D;g(z^{(3)})<br>$$<br>其中，\(z^{(3)}&#x3D;\theta^{(2)}*a^{(2)}\)。</p><h1 id="实例和直观理解"><a href="#实例和直观理解" class="headerlink" title="实例和直观理解"></a>实例和直观理解</h1><p>神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。</p><p>我们可以使用下面的神经网络来表示逻辑与（AND）。</p><p><img src="/../images/neuralNetwork_3.png" alt="neuralNetworks_3"></p><p>我们设置权重为（-30,20,20），输出函数\(h_\theta(x)&#x3D;g(-30+20x_1+20x_2)\)。我们知道g(x)的图像为：</p><p><img src="/..%5Cimages%5CSigmoidFunction.svg" alt="Sigmoid"></p><p>所以当我们将\(x_1\)和\(x_2\)分别设置为0和1时，会得到如下结果：</p><p><img src="/../images/neuralNetwork_4.png" alt="neuralNetworks_4"></p><p>我们发现只有当\(x_1\)和\(x_2\)同时为1时，我们的输出结果才为1，也就是AND的逻辑运算。</p><p>逻辑或（OR）跟上面的AND类似，只是权重设置不同（三个权重分别设置为-10,20,20）。</p><p><img src="/../images/neuralNetwork_5.png" alt="neuralNetworks_5"></p><p>下面的神经元（权重分别设置为10,-20）可以被视为等同于逻辑非（NOT):</p><p><img src="/../images/neuralNetwork_6.png" alt="neuralNetworks_6"></p><p>我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算。例如我们要实现XNOR功能（输入的两个值必须一样，均为1或均为0），即\(XNOR&#x3D;(x_1\ AND\ x_2)OR((NOT\ x_1)AND(NOT\ x_2))\)。</p><p>首先构造一个能表达\((NOT\ x_1)AND(NOT\ x_2)\)部分的神经元：</p><p><img src="/../images/neuralNetwork_7.png" alt="neuralNetworks_7"></p><p>然后将表示\(x_1\ AND\ x_2\)的神经元和表示\((NOT\ x_1)AND(NOT\ x_2)\)的神经元，以及表示\(OR\)的神经元相组合：</p><p><img src="/../images/neuralNetwork_8.png" alt="neuralNetworks_8"></p><p>我们就得到了一个能实现XNOR运算符功能的神经网络。</p><h1 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h1><p>当我们有不止两种分类时（也就是y&#x3D;1,2,3…），比如以下这种情况，该怎么办？</p><p>如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。例如，第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车。</p><p>下面是该神经网络的可能结构示例：</p><p><img src="/../images/neuralNetwork_9.png" alt="neuralNetworks_9"></p><p>神经网络算法的输出结果为四种可能情形之一：</p><p><img src="/../images/neuralNetwork_10.png" alt="neuralNetworks_10"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 09-神经网络：表述(Neural Networks: Representation)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-08-exercise 2 summary</title>
    <link href="https://shilei165.github.io/2022/08/19/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8/"/>
    <id>https://shilei165.github.io/2022/08/19/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8/</id>
    <published>2022-08-20T02:40:23.000Z</published>
    <updated>2022-09-14T15:29:11.325Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 08-exercise 2 summary。</p><span id="more"></span><p><strong>Programming Exercise 2: Logistic Regression</strong></p><p>In this exercise, you will implement logistic regression and apply it to two different datasets.</p><h1 id="ex2-m"><a href="#ex2-m" class="headerlink" title="ex2.m"></a>ex2.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise 2: Logistic Regression</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you get started on the logistic</span><br><span class="line">%  regression exercise. You will need to complete the following functions </span><br><span class="line">%  in this exericse:</span><br><span class="line">%</span><br><span class="line">%     sigmoid.m</span><br><span class="line">%     costFunction.m</span><br><span class="line">%     predict.m</span><br><span class="line">%     costFunctionReg.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Load Data</span><br><span class="line">%  The first two columns contains the exam scores and the third column</span><br><span class="line">%  contains the label.</span><br><span class="line"></span><br><span class="line">data = load(&#x27;ex2data1.txt&#x27;);</span><br><span class="line">X = data(:, [1, 2]); y = data(:, 3);</span><br><span class="line"></span><br><span class="line">%% ==================== Part 1: Plotting ====================</span><br><span class="line">%  We start the exercise by first plotting the data to understand the </span><br><span class="line">%  the problem we are working with.</span><br><span class="line"></span><br><span class="line">fprintf([&#x27;Plotting data with + indicating (y = 1) examples and o &#x27; ...</span><br><span class="line">         &#x27;indicating (y = 0) examples.\n&#x27;]);</span><br><span class="line"></span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels </span><br><span class="line">hold on;</span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel(&#x27;Exam 1 score&#x27;)</span><br><span class="line">ylabel(&#x27;Exam 2 score&#x27;)</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend(&#x27;Admitted&#x27;, &#x27;Not admitted&#x27;)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============ Part 2: Compute Cost and Gradient ============</span><br><span class="line">%  In this part of the exercise, you will implement the cost and gradient</span><br><span class="line">%  for logistic regression. You neeed to complete the code in </span><br><span class="line">%  costFunction.m</span><br><span class="line"></span><br><span class="line">%  Setup the data matrix appropriately, and add ones for the intercept term</span><br><span class="line">[m, n] = size(X);</span><br><span class="line"></span><br><span class="line">% Add intercept term to x and X_test</span><br><span class="line">X = [ones(m, 1) X];</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(n + 1, 1);</span><br><span class="line"></span><br><span class="line">% Compute and display initial cost and gradient</span><br><span class="line">[cost, grad] = costFunction(initial_theta, X, y);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Cost at initial theta (zeros): %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 0.693\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradient at initial theta (zeros): \n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad);</span><br><span class="line">fprintf(&#x27;Expected gradients (approx):\n -0.1000\n -12.0092\n -11.2628\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Compute and display cost and gradient with non-zero theta</span><br><span class="line">test_theta = [-24; 0.2; 0.2];</span><br><span class="line">[cost, grad] = costFunction(test_theta, X, y);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nCost at test theta: %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 0.218\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradient at test theta: \n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad);</span><br><span class="line">fprintf(&#x27;Expected gradients (approx):\n 0.043\n 2.566\n 2.647\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============= Part 3: Optimizing using fminunc  =============</span><br><span class="line">%  In this exercise, you will use a built-in function (fminunc) to find the</span><br><span class="line">%  optimal parameters theta.</span><br><span class="line"></span><br><span class="line">%  Set options for fminunc</span><br><span class="line">options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, 400);</span><br><span class="line"></span><br><span class="line">%  Run fminunc to obtain the optimal theta</span><br><span class="line">%  This function will return theta and the cost </span><br><span class="line">[theta, cost] = ...</span><br><span class="line">  fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);</span><br><span class="line"></span><br><span class="line">% Print theta to screen</span><br><span class="line">fprintf(&#x27;Cost at theta found by fminunc: %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 0.203\n&#x27;);</span><br><span class="line">fprintf(&#x27;theta: \n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, theta);</span><br><span class="line">fprintf(&#x27;Expected theta (approx):\n&#x27;);</span><br><span class="line">fprintf(&#x27; -25.161\n 0.206\n 0.201\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Plot Boundary</span><br><span class="line">plotDecisionBoundary(theta, X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels </span><br><span class="line">hold on;</span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel(&#x27;Exam 1 score&#x27;)</span><br><span class="line">ylabel(&#x27;Exam 2 score&#x27;)</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend(&#x27;Admitted&#x27;, &#x27;Not admitted&#x27;)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============== Part 4: Predict and Accuracies ==============</span><br><span class="line">%  After learning the parameters, you&#x27;ll like to use it to predict the outcomes</span><br><span class="line">%  on unseen data. In this part, you will use the logistic regression model</span><br><span class="line">%  to predict the probability that a student with score 45 on exam 1 and </span><br><span class="line">%  score 85 on exam 2 will be admitted.</span><br><span class="line">%</span><br><span class="line">%  Furthermore, you will compute the training and test set accuracies of </span><br><span class="line">%  our model.</span><br><span class="line">%</span><br><span class="line">%  Your task is to complete the code in predict.m</span><br><span class="line"></span><br><span class="line">%  Predict probability for a student with score 45 on exam 1 </span><br><span class="line">%  and score 85 on exam 2 </span><br><span class="line"></span><br><span class="line">prob = sigmoid([1 45 85] * theta);</span><br><span class="line">fprintf([&#x27;For a student with scores 45 and 85, we predict an admission &#x27; ...</span><br><span class="line">         &#x27;probability of %f\n&#x27;], prob);</span><br><span class="line">fprintf(&#x27;Expected value: 0.775 +/- 0.002\n\n&#x27;);</span><br><span class="line"></span><br><span class="line">% Compute accuracy on our training set</span><br><span class="line">p = predict(theta, X);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Train Accuracy: %f\n&#x27;, mean(double(p == y)) * 100);</span><br><span class="line">fprintf(&#x27;Expected accuracy (approx): 89.0\n&#x27;);</span><br><span class="line">fprintf(&#x27;\n&#x27;);</span><br></pre></td></tr></table></figure><h1 id="plotData-m"><a href="#plotData-m" class="headerlink" title="plotData.m"></a>plotData.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">function plotData(X, y)</span><br><span class="line">%PLOTDATA Plots the data points X and y into a new figure </span><br><span class="line">%   PLOTDATA(x,y) plots the data points with + for the positive examples</span><br><span class="line">%   and o for the negative examples. X is assumed to be a Mx2 matrix.</span><br><span class="line"></span><br><span class="line">% Create New Figure</span><br><span class="line">figure; hold on;</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Plot the positive and negative examples on a</span><br><span class="line">%               2D plot, using the option &#x27;k+&#x27; for the positive</span><br><span class="line">%               examples and &#x27;ko&#x27; for the negative examples.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">pos = find(y==1); neg = find(y==0);</span><br><span class="line">plot(X(pos,1),X(pos,2), &#x27;k+&#x27;,&#x27;LineWidth&#x27;,2,&#x27;MarkerSize&#x27;,5);</span><br><span class="line"></span><br><span class="line">plot(X(neg,1),X(neg,2), &#x27;ko&#x27;,&#x27;LineWidth&#x27;,2,&#x27;MarkerFaceColor&#x27;,&#x27;y&#x27;,&#x27;MarkerSize&#x27;,5);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="costFunction-m"><a href="#costFunction-m" class="headerlink" title="costFunction.m"></a>costFunction.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = costFunction(theta, X, y)</span><br><span class="line">%COSTFUNCTION Compute cost and gradient for logistic regression</span><br><span class="line">%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the</span><br><span class="line">%   parameter for logistic regression and the gradient of the cost</span><br><span class="line">%   w.r.t. to the parameters.</span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">J = 0;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost of a particular choice of theta.</span><br><span class="line">%               You should set J to the cost.</span><br><span class="line">%               Compute the partial derivatives and set grad to the partial</span><br><span class="line">%               derivatives of the cost w.r.t. each parameter in theta</span><br><span class="line">%</span><br><span class="line">% Note: grad should have the same dimensions as theta</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">h = sigmoid(X*theta);</span><br><span class="line">J = (-y&#x27;*log(h)-(1-y&#x27;)*log(1-h))/m;</span><br><span class="line"></span><br><span class="line">grad = ((h-y)&#x27;*X)&#x27;/m;</span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="plotDecisionBoundary-m"><a href="#plotDecisionBoundary-m" class="headerlink" title="plotDecisionBoundary.m"></a>plotDecisionBoundary.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">function plotDecisionBoundary(theta, X, y)</span><br><span class="line">%PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with</span><br><span class="line">%the decision boundary defined by theta</span><br><span class="line">%   PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the </span><br><span class="line">%   positive examples and o for the negative examples. X is assumed to be </span><br><span class="line">%   a either </span><br><span class="line">%   1) Mx3 matrix, where the first column is an all-ones column for the </span><br><span class="line">%      intercept.</span><br><span class="line">%   2) MxN, N&gt;3 matrix, where the first column is all-ones</span><br><span class="line"></span><br><span class="line">% Plot Data</span><br><span class="line">plotData(X(:,2:3), y);</span><br><span class="line">hold on</span><br><span class="line"></span><br><span class="line">if size(X, 2) &lt;= 3</span><br><span class="line">    % Only need 2 points to define a line, so choose two endpoints</span><br><span class="line">    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];</span><br><span class="line"></span><br><span class="line">    % Calculate the decision boundary line</span><br><span class="line">    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));</span><br><span class="line"></span><br><span class="line">    % Plot, and adjust axes for better viewing</span><br><span class="line">    plot(plot_x, plot_y)</span><br><span class="line">    </span><br><span class="line">    % Legend, specific for the exercise</span><br><span class="line">    legend(&#x27;Admitted&#x27;, &#x27;Not admitted&#x27;, &#x27;Decision Boundary&#x27;)</span><br><span class="line">    axis([30, 100, 30, 100])</span><br><span class="line">else</span><br><span class="line">    % Here is the grid range</span><br><span class="line">    u = linspace(-1, 1.5, 50);</span><br><span class="line">    v = linspace(-1, 1.5, 50);</span><br><span class="line"></span><br><span class="line">    z = zeros(length(u), length(v));</span><br><span class="line">    % Evaluate z = theta*x over the grid</span><br><span class="line">    for i = 1:length(u)</span><br><span class="line">        for j = 1:length(v)</span><br><span class="line">            z(i,j) = mapFeature(u(i), v(j))*theta;</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    z = z&#x27;; % important to transpose z before calling contour</span><br><span class="line"></span><br><span class="line">    % Plot z = 0</span><br><span class="line">    % Notice you need to specify the range [0, 0]</span><br><span class="line">    contour(u, v, z, [0, 0], &#x27;LineWidth&#x27;, 2)</span><br><span class="line">end</span><br><span class="line">hold off</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="sigmoid-m"><a href="#sigmoid-m" class="headerlink" title="sigmoid.m"></a>sigmoid.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">function g = sigmoid(z)</span><br><span class="line">%SIGMOID Compute sigmoid function</span><br><span class="line">%   g = SIGMOID(z) computes the sigmoid of z.</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">g = zeros(size(z));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the sigmoid of each value of z (z can be a matrix,</span><br><span class="line">%               vector or scalar).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">g = 1./(1+e.^(-z));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="predict-m"><a href="#predict-m" class="headerlink" title="predict.m"></a>predict.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">function p = predict(theta, X)</span><br><span class="line">%PREDICT Predict whether the label is 0 or 1 using learned logistic </span><br><span class="line">%regression parameters theta</span><br><span class="line">%   p = PREDICT(theta, X) computes the predictions for X using a </span><br><span class="line">%   threshold at 0.5 (i.e., if sigmoid(theta&#x27;*x) &gt;= 0.5, predict 1)</span><br><span class="line"></span><br><span class="line">m = size(X, 1); % Number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly</span><br><span class="line">p = zeros(m, 1);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Complete the following code to make predictions using</span><br><span class="line">%               your learned logistic regression parameters. </span><br><span class="line">%               You should set p to a vector of 0&#x27;s and 1&#x27;s</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p = round(sigmoid(X*theta));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="ex2-reg-m"><a href="#ex2-reg-m" class="headerlink" title="ex2_reg.m"></a>ex2_reg.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise 2: Logistic Regression</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you get started on the second part</span><br><span class="line">%  of the exercise which covers regularization with logistic regression.</span><br><span class="line">%</span><br><span class="line">%  You will need to complete the following functions in this exericse:</span><br><span class="line">%</span><br><span class="line">%     sigmoid.m</span><br><span class="line">%     costFunction.m</span><br><span class="line">%     predict.m</span><br><span class="line">%     costFunctionReg.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Load Data</span><br><span class="line">%  The first two columns contains the X values and the third column</span><br><span class="line">%  contains the label (y).</span><br><span class="line"></span><br><span class="line">data = load(&#x27;ex2data2.txt&#x27;);</span><br><span class="line">X = data(:, [1, 2]); y = data(:, 3);</span><br><span class="line"></span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels</span><br><span class="line">hold on;</span><br><span class="line"></span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel(&#x27;Microchip Test 1&#x27;)</span><br><span class="line">ylabel(&#x27;Microchip Test 2&#x27;)</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend(&#x27;y = 1&#x27;, &#x27;y = 0&#x27;)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Regularized Logistic Regression ============</span><br><span class="line">%  In this part, you are given a dataset with data points that are not</span><br><span class="line">%  linearly separable. However, you would still like to use logistic</span><br><span class="line">%  regression to classify the data points.</span><br><span class="line">%</span><br><span class="line">%  To do so, you introduce more features to use -- in particular, you add</span><br><span class="line">%  polynomial features to our data matrix (similar to polynomial</span><br><span class="line">%  regression).</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Add Polynomial Features</span><br><span class="line"></span><br><span class="line">% Note that mapFeature also adds a column of ones for us, so the intercept</span><br><span class="line">% term is handled</span><br><span class="line">X = mapFeature(X(:,1), X(:,2));</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(size(X, 2), 1);</span><br><span class="line"></span><br><span class="line">% Set regularization parameter lambda to 1</span><br><span class="line">lambda = 1;</span><br><span class="line"></span><br><span class="line">% Compute and display initial cost and gradient for regularized logistic</span><br><span class="line">% regression</span><br><span class="line">[cost, grad] = costFunctionReg(initial_theta, X, y, lambda);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Cost at initial theta (zeros): %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 0.693\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradient at initial theta (zeros) - first five values only:\n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad(1:5));</span><br><span class="line">fprintf(&#x27;Expected gradients (approx) - first five values only:\n&#x27;);</span><br><span class="line">fprintf(&#x27; 0.0085\n 0.0188\n 0.0001\n 0.0503\n 0.0115\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">% Compute and display cost and gradient</span><br><span class="line">% with all-ones theta and lambda = 10</span><br><span class="line">test_theta = ones(size(X,2),1);</span><br><span class="line">[cost, grad] = costFunctionReg(test_theta, X, y, 10);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nCost at test theta (with lambda = 10): %f\n&#x27;, cost);</span><br><span class="line">fprintf(&#x27;Expected cost (approx): 3.16\n&#x27;);</span><br><span class="line">fprintf(&#x27;Gradient at test theta - first five values only:\n&#x27;);</span><br><span class="line">fprintf(&#x27; %f \n&#x27;, grad(1:5));</span><br><span class="line">fprintf(&#x27;Expected gradients (approx) - first five values only:\n&#x27;);</span><br><span class="line">fprintf(&#x27; 0.3460\n 0.1614\n 0.1948\n 0.2269\n 0.0922\n&#x27;);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;\nProgram paused. Press enter to continue.\n&#x27;);</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============= Part 2: Regularization and Accuracies =============</span><br><span class="line">%  Optional Exercise:</span><br><span class="line">%  In this part, you will get to try different values of lambda and</span><br><span class="line">%  see how regularization affects the decision coundart</span><br><span class="line">%</span><br><span class="line">%  Try the following values of lambda (0, 1, 10, 100).</span><br><span class="line">%</span><br><span class="line">%  How does the decision boundary change when you vary lambda? How does</span><br><span class="line">%  the training set accuracy vary?</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(size(X, 2), 1);</span><br><span class="line"></span><br><span class="line">% Set regularization parameter lambda to 1 (you should vary this)</span><br><span class="line">lambda = 1;</span><br><span class="line"></span><br><span class="line">% Set Options</span><br><span class="line">options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, 400);</span><br><span class="line"></span><br><span class="line">% Optimize</span><br><span class="line">[theta, J, exit_flag] = ...</span><br><span class="line">  fminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);</span><br><span class="line"></span><br><span class="line">% Plot Boundary</span><br><span class="line">plotDecisionBoundary(theta, X, y);</span><br><span class="line">hold on;</span><br><span class="line">title(sprintf(&#x27;lambda = %g&#x27;, lambda))</span><br><span class="line"></span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel(&#x27;Microchip Test 1&#x27;)</span><br><span class="line">ylabel(&#x27;Microchip Test 2&#x27;)</span><br><span class="line"></span><br><span class="line">legend(&#x27;y = 1&#x27;, &#x27;y = 0&#x27;, &#x27;Decision boundary&#x27;)</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">% Compute accuracy on our training set</span><br><span class="line">p = predict(theta, X);</span><br><span class="line"></span><br><span class="line">fprintf(&#x27;Train Accuracy: %f\n&#x27;, mean(double(p == y)) * 100);</span><br><span class="line">fprintf(&#x27;Expected accuracy (with lambda = 1): 83.1 (approx)\n&#x27;);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="mapFeature-m"><a href="#mapFeature-m" class="headerlink" title="mapFeature.m"></a>mapFeature.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">function out = mapFeature(X1, X2)</span><br><span class="line">% MAPFEATURE Feature mapping function to polynomial features</span><br><span class="line">%</span><br><span class="line">%   MAPFEATURE(X1, X2) maps the two input features</span><br><span class="line">%   to quadratic features used in the regularization exercise.</span><br><span class="line">%</span><br><span class="line">%   Returns a new feature array with more features, comprising of </span><br><span class="line">%   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..</span><br><span class="line">%</span><br><span class="line">%   Inputs X1, X2 must be the same size</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">degree = 6;</span><br><span class="line">out = ones(size(X1(:,1)));</span><br><span class="line">for i = 1:degree</span><br><span class="line">    for j = 0:i</span><br><span class="line">        out(:, end+1) = (X1.^(i-j)).*(X2.^j);</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h1 id="costFunctionReg-m"><a href="#costFunctionReg-m" class="headerlink" title="costFunctionReg.m"></a>costFunctionReg.m</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = costFunctionReg(theta, X, y, lambda)</span><br><span class="line">%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization</span><br><span class="line">%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using</span><br><span class="line">%   theta as the parameter for regularized logistic regression and the</span><br><span class="line">%   gradient of the cost w.r.t. to the parameters. </span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number of training examples</span><br><span class="line"></span><br><span class="line">% You need to return the following variables correctly </span><br><span class="line">J = 0;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost of a particular choice of theta.</span><br><span class="line">%               You should set J to the cost.</span><br><span class="line">%               Compute the partial derivatives and set grad to the partial</span><br><span class="line">%               derivatives of the cost w.r.t. each parameter in theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">h = sigmoid(X*theta);</span><br><span class="line">reg = lambda/(2*m)*(theta&#x27;*theta-theta(1)^2);</span><br><span class="line">J = 1/m*(-y&#x27;*log(h)-(1-y&#x27;)*log(1-h))+reg;</span><br><span class="line"></span><br><span class="line">mask = ones(size(theta));</span><br><span class="line">mask(1)=0;</span><br><span class="line"></span><br><span class="line">grad = 1/m*X&#x27;*(h-y) + lambda/m*(theta.*mask);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 08-exercise 2 summary。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning-学习笔记-07-Regularization</title>
    <link href="https://shilei165.github.io/2022/08/18/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-7/"/>
    <id>https://shilei165.github.io/2022/08/18/Machine%20Learning-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-7/</id>
    <published>2022-08-18T16:40:23.000Z</published>
    <updated>2022-09-13T17:26:41.907Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章跟大家分享一下Machine Learning的学习笔记: 07-正则化(Regularization)。</p><span id="more"></span><h1 id="过度拟合的问题"><a href="#过度拟合的问题" class="headerlink" title="过度拟合的问题"></a>过度拟合的问题</h1><p>我们已经学习了几种不同的学习算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过度拟合(over-fitting)的问题，可能会导致它们效果很差。</p><p>下面是一个回归问题的例子：</p><p><img src="/../images/overFitting.png" alt="overFitting"></p><p>第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。</p><p>分类问题中也存在这样的问题：</p><p><img src="/../images/overFitting_2.png" alt="overFitting_2"></p><p>应该如何处理过度拟合问题？</p><ul><li><p>减少特征数量。</p><ul><li>可以是手工选择保留哪些特征</li><li>使用一些模型选择的算法来帮忙（例如PCA）</li></ul></li><li><p>正则化。 </p><ul><li>保留所有的特征，但是减少参数的大小（magnitude）。</li><li>适用于有多个特征，但每个特征对预测函数的贡献都比较小。</li></ul></li></ul><h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p>上面的回归问题中如果我们的模型是：  \(h_\theta(x)&#x3D;\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3+\theta_4x_4^4\) 我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。 所以我们要做的就是在一定程度上减小这些参数 的值，这就是<strong>正则化的基本方法</strong>。</p><p>我们决定要减少和的大小，我们要做的便是修改代价函数，在其中和 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的和。 修改后的代价函数如下：</p><p>$$<br>J(\theta)&#x3D; \frac{1}{2m}[\sum_{i&#x3D;1}^m(h_\theta(x^{(i)})-y^{(i)})^2+1000\theta_3^2+1000\theta_4^2]<br>$$</p><p>类似的，假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对除\(\theta_0\)以外的所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：</p><p>$$<br>J(\theta)&#x3D;\frac{1}{2m}[\sum_{i&#x3D;1}^{m}h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum_{j&#x3D;1}^{n}\theta_j^2]<br>$$</p><p>如果我们令\(\lambda\)的值很大的话，为了使Cost Function 尽可能的小，所有的 \(\theta\)的值（不包括\(\theta_0\)）都会在一定程度上减小。 但若\(\lambda\)的值太大了，那么\(\theta\)（不包括\(\theta_0\)）都会趋近于0，这样我们所得到的只能是一条平行于轴的直线。 所以对于正则化，我们要取一个合理的\(\lambda\)的值，这样才能更好的应用正则化。 </p><h1 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h1><p>正则化线性回归的代价函数为：</p><p>$$<br>J(\theta)&#x3D;\frac{1}{2m}[\sum_{i&#x3D;1}^{m}h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum_{j&#x3D;1}^{n}\theta_j^2]<br>$$</p><p>如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对进行正则化，所以梯度下降算法将分两种情形：</p><p>$$<br>\theta_0 :&#x3D; \theta_0 - \alpha\frac{1}{m}\sum_{i&#x3D;1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}<br>$$<br>$$<br>\theta_j :&#x3D; \theta_j - \alpha[\frac{1}{m}\sum_{i&#x3D;1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]\ \ \ \ \ \ for\  j &#x3D; 1,2,3…<br>$$</p><p>将上面的公式稍加整理，即可得到：<br>$$<br>\theta_j :&#x3D; \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}[\sum_{i&#x3D;1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}]<br>$$</p><p>可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令值减少了一个额外的值。</p><p>我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示：</p><p>$$<br>\theta &#x3D; \begin{pmatrix}<br>X^TX+\lambda\begin{bmatrix}<br>0&amp;\ &amp;\ &amp;\ &amp;\ \\<br>\ &amp;1 &amp;\ &amp;\ &amp;\ \\<br>\ &amp;\ &amp;1 &amp;\ &amp;\ \\<br>\ &amp;\ &amp;\ &amp;\ddots &amp;\ \\<br>\ &amp;\ &amp;\ &amp;\ &amp;1 \end{bmatrix}<br>\end{pmatrix}^{-1} X^Ty<br>$$</p><h1 id="正则化的逻辑回归模型"><a href="#正则化的逻辑回归模型" class="headerlink" title="正则化的逻辑回归模型"></a>正则化的逻辑回归模型</h1><p>我们也给代价函数增加一个正则化的表达式，得到代价函数：<br>$$<br>J(\theta)&#x3D; -\frac{1}{m}[\sum_{i&#x3D;1}^{m}y^{(i)}log h_\theta(x^{(i)}) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j&#x3D;1}^{n}\theta_j^2<br>$$</p><p>要最小化该代价函数，通过求导，得出梯度下降算法为：</p><p>$$<br>\theta_0 :&#x3D; \theta_0 - \alpha\frac{1}{m}\sum_{i&#x3D;1}^m(h_\theta(x^{i})-y^{(i)})x_0^{(i)}<br>$$<br>$$<br>\theta_j :&#x3D; \theta_j - \alpha[\frac{1}{m}\sum_{i&#x3D;1}^m(h_\theta(x^{i})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]<br>$$<br>注：看上去同线性回归一样，但是知道\(h_\theta(x)&#x3D;g(\theta^TX)\) ，所以与线性回归不同。 Octave 中，我们依旧可以用 fminuc 函数来求解代价函数最小化的参数，值得注意的是参数\(\theta_0\)的更新规则与其他情况不同。</p><p>下面是 octave 中使用 fminunc 函数的代码示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">function [jVal, gradient] = costFunction(theta)</span><br><span class="line"></span><br><span class="line">    jVal = [...code to compute J(theta)...];</span><br><span class="line"></span><br><span class="line">    gradient(1) = [...code to compute derivative of the partial derivative of cost function at theta_0];</span><br><span class="line"></span><br><span class="line">    gradient(2) = [...code to compute derivative of the partial derivative of cost function at theta_1];</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    gradient(n+1) = [...code to compute derivative of the partial derivative of cost function at theta_n];</span><br><span class="line">    </span><br><span class="line">end</span><br><span class="line">    </span><br><span class="line">options = optimset(&#x27;GradObj&#x27;, &#x27;on&#x27;, &#x27;MaxIter&#x27;, &#x27;100&#x27;);</span><br><span class="line">    </span><br><span class="line">initialTheta = zeros(n+1,1);</span><br><span class="line">    </span><br><span class="line">[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇文章跟大家分享一下Machine Learning的学习笔记: 07-正则化(Regularization)。&lt;/p&gt;</summary>
    
    
    
    <category term="技术杂谈" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Machine-Learning/"/>
    
    
    <category term="计算机" scheme="https://shilei165.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="机器学习" scheme="https://shilei165.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Machine Learning" scheme="https://shilei165.github.io/tags/Machine-Learning/"/>
    
    <category term="人工智能" scheme="https://shilei165.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
</feed>
