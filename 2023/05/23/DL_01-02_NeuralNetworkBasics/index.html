<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-Shi.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-Shi.png">
  <link rel="mask-icon" href="/images/logo-Shi.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Learning Objectives NN vie as Linear Classifiers Computation Graphs Backpropagation Backprop via AutoDiff DAG Logistic Regression (Sigmoid) Simple Layer Jacobians and Vectorization">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning-1-2 Neural Networks">
<meta property="og:url" content="http://example.com/2023/05/23/DL_01-02_NeuralNetworkBasics/index.html">
<meta property="og:site_name" content="Dr. Shi&#39;s Blog">
<meta property="og:description" content="Learning Objectives NN vie as Linear Classifiers Computation Graphs Backpropagation Backprop via AutoDiff DAG Logistic Regression (Sigmoid) Simple Layer Jacobians and Vectorization">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/DL_01-02_01.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_06.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_02.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_03.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_04.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_07.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_08.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_09.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_10.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_11.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_12.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_13.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_14.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_15.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_16.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_17.png">
<meta property="og:image" content="http://example.com/images/DL_01-02_18.png">
<meta property="article:published_time" content="2023-05-23T22:14:22.000Z">
<meta property="article:modified_time" content="2023-05-28T14:50:01.759Z">
<meta property="article:author" content="Dr. Shi">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/DL_01-02_01.png">

<link rel="canonical" href="http://example.com/2023/05/23/DL_01-02_NeuralNetworkBasics/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Deep Learning-1-2 Neural Networks | Dr. Shi's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Dr. Shi's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-top">

    <a href="/top/" rel="section"><i class="signal fa-fw"></i>top</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/23/DL_01-02_NeuralNetworkBasics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar-1.gif">
      <meta itemprop="name" content="Dr. Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dr. Shi's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Deep Learning-1-2 Neural Networks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-05-23 18:14:22" itemprop="dateCreated datePublished" datetime="2023-05-23T18:14:22-04:00">2023-05-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">技术杂谈</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h1><ul>
<li>NN vie as Linear Classifiers</li>
<li>Computation Graphs</li>
<li>Backpropagation</li>
<li>Backprop via AutoDiff</li>
<li>DAG Logistic Regression (Sigmoid)</li>
<li>Simple Layer Jacobians and Vectorization</li>
</ul>
<span id="more"></span>
<hr>
<h1 id="NN-view-as-Linear-Classifiers"><a href="#NN-view-as-Linear-Classifiers" class="headerlink" title="NN view as Linear Classifiers"></a>NN view as Linear Classifiers</h1><p>A linear classifier can be broken down into: 1) input; 2) A function of the inputs; 3) A loss function.</p>
<p><img src="/../images/DL_01-02_01.png" alt="NN as Linear Classifiers"></p>
<p>A simple neural network has similar structure as our linear classifier:</p>
<ol>
<li>A neuron takes input (firings) from other neurons (-&gt; input to linear classifier)</li>
<li>The inputs are summed in a weighted manner (-&gt; weighted sum&#x2F;dot product)</li>
<li>Learning is through a modification of the weights (application of the update rule)</li>
<li>If it receives enough input, it “fires” (threshold or if weighted sum plus bias is high enough)</li>
</ol>
<p>Of course this is an oversimplified model of the neurons in our brain.</p>
<p><img src="/../images/DL_01-02_06.png" alt="NN vs. neurons"></p>
<p>We can also have many neurons connected to the same input. This is what happens in a multi-class classifier. Each output node outputs the score for a class. These are often call “fully connected layers”, or linear projection layers.</p>
<p>$$<br>f(x,W)&#x3D;\sigma(Wx+b) \begin{bmatrix}<br>w_{11} &amp; w_{12} &amp; … &amp; w_{1n}\\<br>w_{21} &amp; w_{22} &amp; … &amp; w_{2n}\\<br>…\\<br>w_{m1} &amp; w_{m2} &amp; … &amp; w_{mn}\\<br>\end{bmatrix}<br>$$</p>
<h2 id="Computation-graph"><a href="#Computation-graph" class="headerlink" title="Computation graph"></a>Computation graph</h2><p>A nice illustration by <a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Backprop/">colah’s blog</a> can help better understand.</p>
<p>Computational graphs are a nice way to think about mathematical expressions. For example, consider the expression e&#x3D;(a+b)∗(b+1). There are three operations: two additions and one multiplication. To help us talk about this, let’s introduce two intermediary variables, c and d so that every function’s output has a variable. We now have:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c=a+b</span><br><span class="line">d=b+1</span><br><span class="line">e=c∗d</span><br></pre></td></tr></table></figure>
<p>To create a computational graph, we make each of these operations, along with the input variables, into nodes. When one node’s value is the input to another node, an arrow goes from one to another.</p>
<p><img src="/../images/DL_01-02_02.png" alt="Computation Graph"></p>
<h2 id="Derivatives-with-Computation-Graph"><a href="#Derivatives-with-Computation-Graph" class="headerlink" title="Derivatives with Computation Graph"></a>Derivatives with Computation Graph</h2><p>If one wants to understand derivatives in a computational graph, the key is to understand derivatives on the edges. If a directly affects c, then we want to know how it affects c. If a changes a little bit, how does c change? We call this the partial derivative of c with respect to a.</p>
<p><img src="/../images/DL_01-02_03.png" alt="Computation Graph"></p>
<h2 id="Logistic-Regression-Gradient-Descent"><a href="#Logistic-Regression-Gradient-Descent" class="headerlink" title="Logistic Regression Gradient Descent"></a>Logistic Regression Gradient Descent</h2><p><img src="/../images/DL_01-02_04.png" alt="Computation Graph"></p>
<h2 id="Logistic-Regression-on-m-Examples"><a href="#Logistic-Regression-on-m-Examples" class="headerlink" title="Logistic Regression on m Examples"></a>Logistic Regression on m Examples</h2><p>The cost function is computed as an average of the $m$ individual loss values, the gradient with respect to each parameter should also be calculated as the mean of the $m$ gradient values on each example.</p>
<p>The calculattion process can be done in a loop through m examples.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">J=0</span><br><span class="line">dw=np.zeros(n)</span><br><span class="line">db=0</span><br><span class="line"></span><br><span class="line">for i in range(m):</span><br><span class="line">  z[i] = w.transpose() * x[i] + b</span><br><span class="line">  a[i] = sigmoid(z[i])</span><br><span class="line">  J = J + (-[y[i]*log(a[i])+(1-y[i])*log(1-a[i])])</span><br><span class="line">  dz[i] = a[i] - y[i]</span><br><span class="line">  </span><br><span class="line">  # inner loop for n features, later will be optimized by vectorization</span><br><span class="line">  for j in range(n):</span><br><span class="line">    dw[j] = dw[j] + x[i][j] * dz[i]</span><br><span class="line">  </span><br><span class="line">  db = db + dz[i]</span><br><span class="line">  </span><br><span class="line">j = j / m</span><br><span class="line">dw = dw / m</span><br><span class="line">db = db / m</span><br></pre></td></tr></table></figure>
<p>After gradient computation, we can update parameters with a learning rate alpha.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vectorization should also applied here</span><br><span class="line">for j in range(n):</span><br><span class="line">  w[j] = w[j] - alpha * dw[j]</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure>
<p>As you can see above, to update parameters one step, we have to go throught all the m examples. </p>
<h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><p>Backpropagation algorithm consists of two main parts:</p>
<ol>
<li>Forward pass - which computes the outputs for a given set of weights</li>
<li>Backward pass - calculates the gradients for each module, can be decomposed into several steps:</li>
</ol>
<ul>
<li>This is a recursive algorithm</li>
<li>Start at the loss function where we know how to compute the gradients</li>
<li>Progress backwards throught the modules (computing the gradients at each stage)</li>
<li>Ends at the input layer where there are no gradients or parameters</li>
</ul>
<h2 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h2><p>We simply compute the output of each component and save. These will be needed to compute the gradients later on.</p>
<h2 id="Backwards-Pass"><a href="#Backwards-Pass" class="headerlink" title="Backwards Pass"></a>Backwards Pass</h2><p>It is much more complex and involved, recursive algorithm. Here we seek to calculate the gradients of the loss with respect to the module’s parameters.</p>
<p>There are three main gradients needed for each module:</p>
<ol>
<li>$\frac{\partial L}{\partial h^{l-1}}$ is the gradient of the input layer (previous)</li>
<li>$\frac{\partial L}{\partial h^{l}}$ is the gradients of the output layer (current)</li>
<li>$\frac{\partial L}{\partial W}$ is given to us by the forward pass</li>
</ol>
<p><img src="/../images/DL_01-02_07.png" alt="Backwards Pass"></p>
<p>We need to compute the local gradients:<br>a. $\frac{\partial h^l}{\partial h^{l-1}}$ – The change in the output with respect to the inputs</p>
<p>b. $\frac{\partial h^l}{\partial W}$ – The change in the putput wrt the weights</p>
<p>c. $\frac{\partial L}{\partial h^{l-1}}$</p>
<p>d. $\frac{\partial L}{\partial W}$</p>
<h3 id="Compute-local-gradients-frac-partial-h-l-partial-h-l-1-and-frac-partial-h-l-partial-W"><a href="#Compute-local-gradients-frac-partial-h-l-partial-h-l-1-and-frac-partial-h-l-partial-W" class="headerlink" title="Compute local gradients $\frac{\partial h^l}{\partial h^{l-1}}$ and $\frac{\partial h^l}{\partial W}$"></a>Compute local gradients $\frac{\partial h^l}{\partial h^{l-1}}$ and $\frac{\partial h^l}{\partial W}$</h3><p>This is simply the derivative of the function wrt it’s parameters and inputs.<br>For example in a single network $h^l&#x3D;Wh^{l−1}$<br>$\frac{\partial h^l}{\partial h^{l-1}} &#x3D; W$ and $\frac{\partial h^l}{\partial W}&#x3D;h^{l-1,T}$</p>
<h3 id="Compute-frac-partial-L-partial-h-l-1-and-frac-partial-L-partial-W"><a href="#Compute-frac-partial-L-partial-h-l-1-and-frac-partial-L-partial-W" class="headerlink" title="Compute $\frac{\partial L}{\partial h^{l-1}}$ and $\frac{\partial L}{\partial W}$"></a>Compute $\frac{\partial L}{\partial h^{l-1}}$ and $\frac{\partial L}{\partial W}$</h3><p><img src="/../images/DL_01-02_08.png" alt="Backwards Pass"><br>We simply apply chain rule<br><img src="/../images/DL_01-02_09.png" alt="Backwards Pass"></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ol>
<li>Forward Pass: Compute the Loss on a Minibatch</li>
<li>Backward Pass: Compute the gradients wrt each parameter</li>
</ol>
<ul>
<li>Starting at the loss function, where we know how to compute the gradient of the loss wrt the last module</li>
<li>This is just the partials of the module wrt the inputs</li>
<li>Now we continue to work backwards propogating the loss in reverse (Upstream)</li>
<li>This is done using the chain rule</li>
</ul>
<ol start="3">
<li>Finally we just update the weights<br>$$<br>w_i&#x3D;w_i-\alpha \frac{\partial L}{\partial w_i}<br>$$</li>
</ol>
<h1 id="Backprop-via-AutoDiff"><a href="#Backprop-via-AutoDiff" class="headerlink" title="Backprop via AutoDiff"></a>Backprop via AutoDiff</h1><p>Backprop only tells you what you need to do, it doesn’t specify how to do it. The backprop idea though can be applied to any directed acyclic graph, aka DAG. Graphs represent an ordering constraining which paths must be calculated first. Given an ordering, we can then iterate from the last module backwards, using the chain rule. As we do so we will store it’s gradient outputs for efficient computation. This is called reverse mode automatic differentiation.</p>
<p>The idea here is that we need to create a framework such that we can just define the computation graph. We can put together a set of functions that use simple primitives like addition and multiplication etc etc. Doing so will allow us to avoid computing the backwards gradients, nor will we write code that computes the gradients of the functions. Because these are all simple primitives.</p>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><ol>
<li>Computation &#x3D; Graph</li>
</ol>
<ul>
<li>Input &#x3D; Data + Parameters</li>
<li>Output &#x3D; Loss</li>
<li>Scheduling &#x3D; Topological ordering</li>
</ul>
<ol start="2">
<li>Auto-Diff</li>
</ol>
<ul>
<li>A family of algorithms for implementing chain rule on computation graphs</li>
</ul>
<h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><p>We want to find the partial derivative of the output $f$, with respect to all the intermediate variables.</p>
<p>For convenience we assign intermediate variable with a simplify notation. Denote bar as:<br>$$<br>\overline{a_3}&#x3D;\frac{\partial f}{\partial a_3}<br>$$</p>
<p>We start at the end and work backwards.</p>
<p><img src="/../images/DL_01-02_10.png" alt="Auto-Diff"></p>
<p>It is interesting to note what certain operations do, and what they tell us about gradient flow.</p>
<ol>
<li>Addition operation distributes gradients along all the paths. e.g. $\overline{a_3}&#x3D;\overline{a_1}$ and also $\overline{a_3}&#x3D;\overline{a_2}$.</li>
<li>Multiplication operation is a gradient switcher (multiplies it by the values of the other term). e.g. $\overline{x_2}&#x3D;\overline{a_2}x_1$, and $\overline{x_1}&#x3D;\overline{a_2}x_2$</li>
<li>In Max operation, gradient flows along the path selected to be the max</li>
</ol>
<p>Key Idea is to store the computation graph in memory and corresponding gradient functions.<br>Nodes broken down to basic primitive computations (addition, multiplication, log, etc.) for which corresponding derivative is known.</p>
<p>Here’s a small example demonstrating graph building using pytorch: (This uses an older version of pytorch, it’s easier today)</p>
<p><img src="/../images/DL_01-02_11.png" alt="pytorch"></p>
<p>The last line computes the backwards gradients. All in one line of code.<br>Computation graphs are not limited to mathematical functions. They can have control flows (if’s, loops, etc etc) and can proprogate through algorithms. Also, they can be done dynamically so that gradients are computed, nodes are added, and repeat. All this falls under the umbrella of differentiable programming.</p>
<h1 id="DAG-Logistic-Regression-Sigmoid"><a href="#DAG-Logistic-Regression-Sigmoid" class="headerlink" title="DAG Logistic Regression (Sigmoid)"></a>DAG Logistic Regression (Sigmoid)</h1><p>In this section we look an example that more closely resemble a NN use case. Recall from our earlier sections the sigmoid function given by the Logistic regression function.<br>$$<br>-log(\frac{1}{1+e^{-w^Tx}})<br>$$<br>where, input $x \in \mathbb{R^D}$, binary label $y \in {-1,1}$, parameters $w \in \mathbb{R^D}$, output prediction $p(y&#x3D;1|x)&#x3D;\frac{1}{1+e^{-w^Tx}}$, loss $L&#x3D;\frac{1}{2}||w||^2-\lambda log(p(y|x))$.</p>
<p>It can be decomposed as follow:</p>
<p><img src="/../images/DL_01-02_12.png" alt="DAG Logistic Regression"></p>
<p>Now, let’s work through the backprop via automatic differentiation.</p>
<p><img src="/../images/DL_01-02_13.png" alt="DAG Logistic Regression"></p>
<h1 id="Vectorization-and-Jacobians-of-Simple-Layers"><a href="#Vectorization-and-Jacobians-of-Simple-Layers" class="headerlink" title="Vectorization and Jacobians of Simple Layers"></a>Vectorization and Jacobians of Simple Layers</h1><h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>Both GPU and CPU have parallelization instructions. They’re sometimes called SIMD instructions, which stands for a single instruction multiple data. The rule of thumb to remember is whenever possible avoid using explicit for loops.</p>
<h2 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h2><p>If we stack all the $m$ examples of $x$ we have a input matrix $X$ with each column representing an example. So by the builtin vectorization of numpy we can simplify the above gradient descent calculation with a few lines of code which can boost the computational efficiency definitely.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T, X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dz = A - Y</span><br><span class="line"></span><br><span class="line"># in constrast to the inner loop above, vectorization is used here to boost computation</span><br><span class="line">dw = 1/m * np.dot(X, dz.T)</span><br><span class="line">db = 1/m * np.sum(dz)  </span><br></pre></td></tr></table></figure>
<p><strong>Update parameters:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = w - alpha * dw</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure>

<h2 id="Simple-Layer-Jacobians-and-Vectorization"><a href="#Simple-Layer-Jacobians-and-Vectorization" class="headerlink" title="Simple Layer Jacobians and Vectorization"></a>Simple Layer Jacobians and Vectorization</h2><p>Let’s reconsider our computation graph using matrix notation:</p>
<p><img src="/../images/DL_01-02_14.png" alt="Simple Layer Jacobians and Vectorization"></p>
<p>For a simple layer network $h^l&#x3D;Wh^{l-1}$, $h^l$ has the dimension of $|h^l|\times 1$, $W$ has the dimention of $|h^l|\times |h^{l-1}|$, and $h^{l-1}$ has the dimension of $|h^{l-1}|\times 1$.</p>
<p><img src="/../images/DL_01-02_15.png" alt="Simple Layer Jacobians and Vectorization"></p>
<p>Now let’s dive into partials (Jacobians).</p>
<p><img src="/../images/DL_01-02_16.png" alt="Simple Layer Jacobians and Vectorization"></p>
<p>Up until now we’ve focused mostly on our sigmoid function, but we can use any differentiable function! This includes piesewise differentiable functions as well. </p>
<p>A popular choice in many applications is the ReLU (<strong>Rectified Linear Unit</strong>)<br>$$h^l&#x3D;max(0,h^{l-1})$$<br>It provides non-linearity but better gradient flow than sigmoid, and is performed element wise.</p>
<p><img src="/../images/DL_01-02_17.png" alt="ReLU"></p>
<p>Full Jacobian of ReLU layer is large (output dim x input dim).</p>
<ul>
<li>But it is sparse</li>
<li>Only diagonal values non-zero, because it is element-wise</li>
<li>An output value affected only by corresponding input value</li>
</ul>
<p>Recall that Max() function funnels gradients through selected path. Gradient will be zero if input &lt;&#x3D; 0.</p>
<p><img src="/../images/DL_01-02_18.png" alt="ReLU"></p>

    </div>

    
    
    
        <div class="reward-container">
  <div>您的支持将鼓励我继续创作</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Dr. Shi 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Dr. Shi 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/05/23/DL_01-01_IntroToDeepLearning/" rel="prev" title="Deep Learning-1-1 Linear Classfiers and Gradient Descent">
      <i class="fa fa-chevron-left"></i> Deep Learning-1-1 Linear Classfiers and Gradient Descent
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/06/04/DL_01-03_NeuralNetworkOptimization%20-%20Copy/" rel="next" title="Deep Learning-1-3 Optimization of Deep Neural Networks Overview">
      Deep Learning-1-3 Optimization of Deep Neural Networks Overview <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Learning-Objectives"><span class="nav-number">1.</span> <span class="nav-text">Learning Objectives</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NN-view-as-Linear-Classifiers"><span class="nav-number">2.</span> <span class="nav-text">NN view as Linear Classifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Computation-graph"><span class="nav-number">2.1.</span> <span class="nav-text">Computation graph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Derivatives-with-Computation-Graph"><span class="nav-number">2.2.</span> <span class="nav-text">Derivatives with Computation Graph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression-Gradient-Descent"><span class="nav-number">2.3.</span> <span class="nav-text">Logistic Regression Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression-on-m-Examples"><span class="nav-number">2.4.</span> <span class="nav-text">Logistic Regression on m Examples</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Backpropagation"><span class="nav-number">3.</span> <span class="nav-text">Backpropagation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Forward-Pass"><span class="nav-number">3.1.</span> <span class="nav-text">Forward Pass</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Backwards-Pass"><span class="nav-number">3.2.</span> <span class="nav-text">Backwards Pass</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Compute-local-gradients-frac-partial-h-l-partial-h-l-1-and-frac-partial-h-l-partial-W"><span class="nav-number">3.2.1.</span> <span class="nav-text">Compute local gradients $\frac{\partial h^l}{\partial h^{l-1}}$ and $\frac{\partial h^l}{\partial W}$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Compute-frac-partial-L-partial-h-l-1-and-frac-partial-L-partial-W"><span class="nav-number">3.2.2.</span> <span class="nav-text">Compute $\frac{\partial L}{\partial h^{l-1}}$ and $\frac{\partial L}{\partial W}$</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">3.3.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Backprop-via-AutoDiff"><span class="nav-number">4.</span> <span class="nav-text">Backprop via AutoDiff</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Terminology"><span class="nav-number">4.1.</span> <span class="nav-text">Terminology</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Examples"><span class="nav-number">4.2.</span> <span class="nav-text">Examples</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DAG-Logistic-Regression-Sigmoid"><span class="nav-number">5.</span> <span class="nav-text">DAG Logistic Regression (Sigmoid)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vectorization-and-Jacobians-of-Simple-Layers"><span class="nav-number">6.</span> <span class="nav-text">Vectorization and Jacobians of Simple Layers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Vectorization"><span class="nav-number">6.1.</span> <span class="nav-text">Vectorization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vectorizing-Logistic-Regression"><span class="nav-number">6.2.</span> <span class="nav-text">Vectorizing Logistic Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Simple-Layer-Jacobians-and-Vectorization"><span class="nav-number">6.3.</span> <span class="nav-text">Simple Layer Jacobians and Vectorization</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Dr. Shi"
      src="/images/avatar-1.gif">
  <p class="site-author-name" itemprop="name">Dr. Shi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">248</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">110</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dr. Shi</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
